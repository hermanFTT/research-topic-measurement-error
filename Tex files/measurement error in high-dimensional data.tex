\documentclass[12pt]{report}
%\usepackage[francais] {babel} %j'ecis en anglais ,pas besoin de ce package
\usepackage[latin1] {inputenc}
\usepackage[T1] {fontenc}
\usepackage{newcent}
\usepackage{pst-xkey}
\usepackage{pstricks}
\usepackage{pstricks-add}
\usepackage{tkz-tab}
\usepackage{color}
\usepackage{colortbl} % colorer un tableau
\usepackage{url} % insertion de lien cliquable
\usepackage{soul} % soulignement
\usepackage{setspace} % interligne
\onehalfspacing
\usepackage{layout} % marge
\usepackage[top=2cm, bottom=2cm, left=2cm, right=2cm]{geometry}
\usepackage{url}
\usepackage{float} % utile pour inserer les figures cote a cote dans une minipage
\usepackage{soul} % soulignement
\usepackage{mathpazo} % police d'ecriture
\usepackage{ulem} % soulignement
\usepackage{graphicx} % package d'insertion et de manipulation d'image
\usepackage{verbatim} % insertion de codes brut
\usepackage{listings} %insertion de codes brut avec options avance
\renewcommand{\emph}{\textit}
% \usepackage{fancyhdr} realiser des entete et pieds de page personnalise
\lstset
{
	language= R,
	basicstyle=\scriptsize, %taille de la police du code
	numbers=left, %placer le numero de chaque ligne a gauche
	numberstyle=\scriptsize, %taille de la police des numeros
	numbersep=0.5pt, %distance entre la ligne et sa numerotation
	backgroundcolor=\color{grisclaire} % couleur de fond
}
\usepackage{array} % choix des separateurs dans un tableau
\usepackage{amsthm}
\usepackage{amsmath} % packages pour ecriture d'expressions scientifiques
\usepackage{ amssymb} % packages pour ecriture d'expressions scientifiques
\usepackage{mathrsfs} % packages pour ecriture d'expressions scientifiques
\usepackage{array} % choix des separateurs dans un tableau
%\usepackage{slashbox} %  n'est pas encore dans la distribution miktex
%\usepackage{multirow} % n'est pas encore dans la distribution miktex
\usepackage{wrapfig} % insertion d'images cdans un paragraphe
\usepackage{eurosym} %pour l'utilisation du symbole Euro via la commande \euro{}
\newtheorem{theoreme}{Theorem}[section] % insere un "Theoreme" par appelation de l'environnement "theoreme" et numeroté par rapport aux sections
\newtheorem {lemme}{Lemma} % La petite étoile enlève la numérotation, mais nécessite le package amsthm
\newtheorem* {preuve}{proof} % preuves dans l'environnement "proof"
\newtheorem {proposition}{Proposition}[section]  % proposition par appelation de l'environnement "proposition"
\newtheorem {definition}{Definition}[section] % definition par appelation de l'environnement "proposition"
\newtheorem* {definition2}{Definition} % definition par appelation de l'environnement "definition"
\newtheorem{remarque}{Remark}[section]
\newtheorem{corollaire}{Corollary}
\definecolor{grisclaire}{gray}{0.8}
\begin{document}

	\addcontentsline{toc}{chapter}{Introduction}
	\tableofcontents
	\listoffigures
	\listoftables
	\chapter*{Introduction}
	This thesis is about measurement error in high-dimensional data.In recent decades, technological progress has led to a great abundance of data in many scientific fields.For example in genetics, a new framework has been developed, in which the number of variables \textbf{p} is larger than the number of observations \textbf{n} (high-dimensional data).High-dimensional data analysis has had a tremendous growth in popularity and a plethora of methods has been proposed for statistical modelling of, and inference in high-dimensional data.Penalized regression methods such as ridge regression \cite{nref8}, Lasso \cite{nref9} methods and Dantzig selector \cite{nref12} are particularly good in this context.
	
	In almost all disciplines, it may not be possible to observe a variable accurately, for some reason, and therefore it is necessary to work with an error-prone version of that variable.Any measurement process can be affected by errors, usually due to the measuring instrument or the sampling process.The consequences of ignoring measurement error, many of which have been known for some time, can range from the non-existent to the rather dramatic.Throughout this work, attention is given to the effects of measurement error on analyses that ignore it. This is mainly because the majority of researchers do not account for measurement error, even if they are aware on its presence  and potential impact.In part this is because the information or extra data needed to correct for measurement error may not be available.Typically , when measurement error creep into the data, there are tree main reason why measurement error cannot be ignored; it can cause bias in parameter estimation \cite{nref1}, interfere with variable selection \cite{nref17} and lead to a loss of power \cite{nref11} leading to trouble in detecting relationships among variables.Results on the bias of naive estimators often provide the added bonus of suggesting a correction method.
	
	Applying high-dimensional regression methods that do not correct for measurement errors result in faulty inference as demonstrated for the Lasso \cite{nref6}. Consequently, correction for measurement error in penalized regression has recently been studied by various authors.Example include ; "Ridge regression approach to measurement error"\cite{nref6} , Non Convex Lasso (NCL) by Loh and Wainwright \cite{nref17}, the Convex Conditional Lasso (CoCoLasso) of Datta and Zou \cite{nref18} and the Matrix Uncertainty Selector proposed by Rosenbaum and Tsybakov (MUS) \cite{nref23}.
	
	The organization of this thesis is as follows; \textbf{Chapter 1} presents high-dimensional data together with potential challenges when analysing the later, along with some statistical methods one may use to handle this kind of datasets. \textbf{Chapter 2} introduces the measurement error in regression theory, provides an overview of the consequences of measurement error in linear regression and introduces some corrections methods. \textbf{Chapter 3} describes behaviour of measurement error in high-dimensional regression and introduces some high-dimensional approaches (methods) to correct for measurement error in high-dimensional context. Both real and simulated data are used for illustrations.
	\chapter{Introduction to High-Dimensional Data}
	\section{Definition and Examples of High-Dimensional Data in bioscience}
	\paragraph*{}
	High-dimensional data are defined as data in which the number of features (\emph{variables observed}) \textbf{p}, are close to or large than the number of observations (or \emph{data points}) \textbf{n}. The opposite is \textbf{low-dimensional data}, in which the number of observations \textbf{n}, far outnumbers the number of feature \textbf{p}.
	
	A related concept is \textbf{Wide data} which refers to data with numerous features irrespective of the number of observations; similarly, \textbf{tall data} is often used to denote data with large number of observations. This concept should not be therefore confuse with notion of \textbf{big data} which is data that contains greater \textit{variety}, arriving in increasing \textit{volumes} and with more \textit{velocity} known as the threes \textbf{Vs} (visit, \url{https://www.oracle.com/big-data/what-is-big-data/}).
	
	High-dimensional datasets are become more common in many scientific fields as new automated data collection techniques have been developed.And example in biological sciences may include \textit{data collected from hospital patients recording symptoms, blood test results, behaviours and general health} resulting in datasets with large number of features.\\
	And example of what high-dimensional data might look like in a biomedical study is shown in figure \ref{tab1} below.
	\begin{figure}[h!]
		\begin{center}
			\includegraphics[scale=1]{high.png}
		\end{center}
		\caption[An high-dimensional dataset.]{an overview of an high-dimensional dataset  with P=20011  features and n=200 observations}
		\label{tab1}
	\end{figure}
	Here are examples of descriptions of research questions whose associate datasets can be considered as high-dimensional data:
	\begin{itemize}
		\item predicting patient blood pressure using: \textit{cholesterol level in blood,age and BMI as well as information on 200000 single nucleotide polymorphisms from 100 patients}
		\item Predicting probability of a patient's cancer progressing using: \textit{gene expression data from 20000 genes as well as data associated with general patient health (age, weight,BMI, blood pressure) and cancer growth (tumour , localised spread,blood test results)}
	\end{itemize}
	Example of application, including in social science are extremely numerous; see \textbf{Plomin (2018)}.
	\section{Challenge when Analysing High-dimensional Data}
	\paragraph*{}
	Analyses of high-dimensional data require consideration of potential problems that come with having more features than observations.Such datasets pose a challenge for data analysis as standard methods of analysis, such as \emph{least squares linear regression}, are no longer appropriate.Many of the issues that arise in the analysis of high-dimensional data are know in classical approaches, since they apply also when $n>p$ :
	these include the role \emph{bias-variance trade-off} and the danger of \textit{over-fitting}.Though these issues are always relevant, they can become particularly important when the number of features is very large relative to the number of observations.
	\subsection{What goes wrong in high dimension setting ?}
	\paragraph*{}
	In other to illustrate the need for extra care and specialized technique for regression when $p>n$, we begin by examining what can go wrong if we apply a statistical technique not intended for high-dimensional setting. For this purpose, we examine \textit{least squares regression}.But the same concepts apply to \emph{logistic regression, linear discriminant analysis} and other classical statistical approaches.
	\begin{center}
		\subsubsection{Setup of Linear Regression Model}
	\end{center}
	
	\paragraph*{}
	The general form of the multiple linear regression model is as follows:
	\begin{equation}
		Y=\mathbb{E}[Y|X] + \epsilon\ =\ \beta_{0}+\beta_{1}X_{1}+...+ \beta_{p}X_{p} + \epsilon
		\label{f1}
	\end{equation}
	Where $y$ is the dependent variable, $\beta_{0}, \beta_{1},...,\beta_{p}$ are regressions coefficients, and $X_{1},...,X_{p}$ are independents variables in the model; $\mathbb{E}[Y]$ the expectation of the response variable. In the classical regression setting, it is usually assumed that the error term $\epsilon$ follows the $normal\ distribution$ with mean $\mathbb{E}[\epsilon]=0$ and constant variance $Var[\epsilon]=\sigma^{2}$.
	
	We consider  a datasets from the following model
	\begin{equation}
		Y_{i}=\beta_{0} + \beta_{1}X_{i1}+...+\beta_{p}X_{ip}+\epsilon_{i},\: i=1,...,n
		\label{f2}
	\end{equation}
	
	Where $X_{ij}$ is the $j^{th}$ variable for individual $i$ and $\epsilon_{i}'s$ are random errors assuming $\mathbb{E}[\epsilon_{i}]=0$ and $Var[\epsilon_{i}|X]=\sigma^{2}\ for\ i=1,2,...,n$. The data from this model can be written in matrix form:
	\begin{equation}
		y=X\beta + \epsilon ,
		\label{lrm}
	\end{equation}
	where: 
	$$ y=
	\begin{pmatrix}
		y_{1} \\
		\vdots \\
		y_{n}
	\end{pmatrix}, \;X= \begin{pmatrix}
		1&x_{11}&x_{12}& \cdots & x_{1p} \\
		1 & x_{21} & x_{22} & \cdots & x_{2p} \\
		\vdots& \vdots&        &         & \vdots \\
		1 & x_{n1} & x_{n2} &   \cdots & x_{np}
	\end{pmatrix} ,\; \beta=
	\begin{pmatrix}
		\beta_{0} \\
		\vdots\\
		\beta_{p}
	\end{pmatrix}, \; and\  \epsilon=
	\begin{pmatrix}
		\epsilon_{1}\\
		\vdots \\
		\epsilon_{n}
	\end{pmatrix}$$
	
	The regression parameter are estimated by minimizing ordinary least squares:
	$$  \sum_{i=1}^{n}[y_{i}-(\beta_{0} + \beta_{1}X_{i1}+...+\beta_{p}X_{ip})]^{2}= (y-X\beta)^{t}(y-X\beta)=\parallel y-X\beta\parallel ^{2},\; (\parallel. \parallel \footnote{$\parallel.\ \parallel$ is the Euclidian norm on $\mathbb{R}^{n}$}).$$
	
	\paragraph* {Ordinary Least Squares Estimates (OLS Estimates)}
	\paragraph*{}
	\begin{proposition}[ \textit{from \cite{ref4}} ]
		The least squares estimation of $\beta$ for linear regression model is given by,
		\begin{equation}
			b=argmin_{\beta} \bigg\{\parallel y-X\beta\parallel ^{2}_{2} \bigg\}= (X^{t}X)^{-1}X^{t}y,
			\label{f3}
		\end{equation}
	\end{proposition}
	
	assuming $(X^{t}X)$ is a non-singular matrix.Note that this is equivalent to assuming that the matrix $X$ is of full rank\footnote{i.e, $rank(X)=p+1<n$, this then implies that $rank(X^{t}X)=p+1$ and therefore that $X^{t}X$ is invertible.}.
	
	\textbf{proof.}
	{\fontfamily{lmr}\selectfont 	we need to minimize the residual sum of squares by solving the following equation: $\frac{\partial}{\partial\beta}(\parallel y-X\beta\parallel ^{2})=0$; set $f(\beta)=\parallel y-X\beta\parallel ^{2}$,
		\begin{align*}
			f(\beta) &=\parallel y-X\beta\parallel ^{2} \\
			&=\parallel y\parallel ^{2} + \parallel X\beta\parallel ^{2} -2<y,X\beta> = y^{t}y + \beta^{t}X^{t}X\beta -2y^{t}X\beta
			= y^{t}y + \beta^{t}X^{t}X\beta -2(X^{t}y)^{t}\beta\\
			\intertext{ (by setting $X^{t}y=(w_{j})_{1\leq j \leq} \in \mathcal{M}_{n\times1}\  ,\quad X^{t}X=(a_{ij})_{1\leq i,j \leq p+1} \in \mathcal{M}_{p+1}$ and recalling that $\beta^{t}X^{t}X\beta$ is a quadratic form\footnote{say, $\sum_{i=1}^{p+1}a_{ii}b_{i}^{2} +2\sum_{1\leq i<j \leq p+1}b_{i}a_{ij}b_{j}$} )}  
			&= \sum_{i=1}^{n}y_{i} ^{2} + \sum_{i=1}^{p+1}a_{ii}\beta_{i}^{2} +2\sum_{1\leq i<j \leq p+1}\beta_{i}a_{ij}\beta_{j} -2\sum_{i=1}^{n}w_{i}y_{i}
		\end{align*}
		by taking partial derivative with respect to each component of $\beta$, we obtain $\frac{\partial}{\partial b}f(b)=-2X^{t}y+2X^{t}Xb$,
		$$\frac{\partial}{\partial b}f(b)=0 \Rightarrow b=(X^{t}X)^{-1}X^{t}y \quad as\  required.$$}
	
	\begin{proposition}
		The estimator $b=(X^{t}X)^{-1}X^{t}y$ is and unbiased estimator of $\beta$. In addition,its covariance matrix is given by $$Cov(b)=(X^{t}X)^{-1}\sigma^{2}.$$
	\end{proposition}
	\label{10}
	\textbf{proof.}
	{\fontfamily{lmr}\selectfont
		\begin{align*}
			\mathbb{E}[b]&=\mathbb{E}[(X^{t}X)^{-1}X^{t}y]=(X^{t}X)^{-1}X^{t}\mathbb{E}[y]=(X^{t}X)^{-1}X^{t}(X\beta) 
			=\beta.
		\end{align*} this complete the proof of unbiasness of $b$.
		\begin{align*}
			Cov(b)&=Cov[(X^{t}X)^{-1}X^{t}y]=[(X^{t}X)^{-1}X^{t}]Cov(y)[(X^{t}X)^{-1}X^{t}]^{t}\\
			&=[(X^{t}X)^{-1}X^{t}]\sigma^{2}\mathbb{I}_{n}[(X^{t}X)^{-1}X^{t}]^{t}
			=(X^{t}X)^{-1}\sigma^{2}.\ as\ required.
	\end{align*}}
	
	In order to estimate $\sigma^{2}$, we consider the residual sum of square (RSS)
	\begin{align*}
		RSS &=\sum_{i=1}^{n}(y_{i}-\hat{y_{i}})^{2}
		=\parallel y-\hat{y}\parallel ^{2}\\
		\intertext{ using \eqref{f3} }
		&=(y-Xb)^{t}(y-Xb)=[y-X(X^{t}X)^{-1}X^{t}y]^{t}[y-X(X^{t}X)^{-1}X^{t}y]\\
		&=y^{t}[\mathbb{I}_{n}-X(X^{t}X)^{-1}X^{t}]y
		=y^{t}Py
	\end{align*}
	which is actually the distance measure between observe $y$ and fitted regression value $\hat{y}=Xb$.
	
	$P=[1-X(X^{t}X)^{-1}X^{t}]$ is and idempotent matrix\footnote{i.e ,$p^{2}=p$}.
	\begin{proposition}[ \textit{ from \cite{ref4}} ]
		The unbiased estimator of the variance $\sigma^{2}$ in the multiple linear regression is given by:
		\begin{equation}
			s^{2}=\frac{RSS}{n-p-1}=\frac{1}{n-p-1}\sum_{i=1}^{n}(y_{i}-\hat{y_{i}})^{2}.
			\label{f4}
		\end{equation}
	\end{proposition}
	Before proving this assertion, let's recall the following lemmas:
	\begin{lemme}
		Let $A_{n\times n}$ be and idempotent matrix of rank $p$ then the eigenvalues of $A$ are either $1$ or $0$.
	\end{lemme}
	\textbf{proof.}
	{\fontfamily{lmr}\selectfont
		Let $\lambda_{i}$ and $v_{i}$ be the eigenvalue and the corresponding normalized eigenvector ($\parallel v_{i}\parallel=1$) of matrix $A$, respectively.
		
		We then have $Av_{i}=\lambda_{i}v_{i}$, and $v_{i}^{t}Av_{i}=v_{i}^{t}(\lambda_{i}v_{i})=\lambda_{i}\parallel v_{i}\parallel^{2}=\lambda_{i}$. on the other hand, since $A^{2}=A$, 
		$v_{i}^{t}Av_{i}=v_{i}^{t}A^{2}v_{i}=(A^{t}v_{i})^{t}(Av_{i})=(\lambda_{i}v_{i})^{t}(\lambda_{i}v_{i})=\lambda_{i}^{2}v_{i}^{t}v_{i}=\lambda_{i}^{2}\parallel v_{i}\parallel^{2}=\lambda_{i}^{2}$  ; (\textit{recall that $A$ and $A^{t}$}} have sames eigenvalues ).This end the proof.
	
	\begin{lemme}
		If $A$ is and idempotent matrix, then $tr(A)=rank(A)=p$.
	\end{lemme}
	\textbf{proof.}
	{\fontfamily{lmr}\selectfont
		If the rank of and $n \times n$ matrix is p, then $A$ has $p$ eigenvalues of $1$ and $n-p$ eigenvalues of $0$, since the eigenvalues of $A$ are either $1$ or $0$.Thus we can write $rank(A)=\sum_{i=1}^{n}\lambda_{i}=p$.
		from matrix theory, there is an orthogonal matrix $V$ such that
		$$
		V^{t}AV = 
		\begin{pmatrix}
			\mathbb{I}_{p} & 0\\
			0       & 0 
		\end{pmatrix} 
		$$ therefore, we have $tr(A)=tr(V^{t}VA)=tr(V^{t}AV)=p=rank(A)$. (\textit{here we use the simple fact that tr(AB)=tr(BA) for any matrix A,B})
	}
	\begin{lemme}
		Let $y^{t}=(y_{1},y_{2},...,y_{n})$ be an $n\times 1$ vector with mean $\mu^{t}=(\mu_{1},...,\mu_{n})$ and variance $\sigma^{2}$  for each component. Further, it is assumed that $y_{1},y_{2},...,y_{n}$ are independent.Let $A$ be and $n\times n$ matrix.
		
		The expectation of the quadratic form of random variables is given by:
		\begin{equation}
			\mathbb{E}[y^{t}Ay]=\sigma^{2}tr(A)+\mu^{t}A\mu ,
			\label{f5}
		\end{equation}
	\end{lemme}
	\textbf{proof.}
	{\fontfamily{lmr}\selectfont
		first we observe that,
		
		$(y-\mu)^{t}A(y-\mu)=y^{t}Ay-y^{t}A\mu-\mu^{t}Ay+\mu^{t}A\mu=y^{t}Ay-2\mu^{t}Ay+\mu^{t}A\mu=y^{t}Ay-2\mu^{t}A(y-\mu)-\mu^{t}A\mu$. Thus , $y^{t}Ay=(y-\mu)^{t}A(y-\mu)+2\mu^{t}A(y-\mu)+\mu^{t}A\mu$. We write:
		\begin{align*}
			\mathbb{E}[y^{t}Ay]&=\mathbb{E}[(y-\mu)^{t}A(y-\mu)] + 2\mathbb{E}[\mu^{t}A(y-\mu)] +\mu^{t}A\mu\\
			&=\mathbb{E}[\sum_{i,j}^{n}a_{ij}(y_{i}-\mu_{i})(y_{j}-\mu_{j})]+2\mathbb{E}[\sum_{i,j}^{n}\mu_{i}a_{ij}(y_{j}-\mu_{j})] + \mu^{t}A\mu\\
			&=\mathbb{E}[\sum_{i}^{n}a_{ii}(y_{i}-\mu_{i})^{2}]+\mathbb{E}[\sum_{i\neq j}a_{ij}(y_{i}-\mu_{i})(y_{j}-\mu_{j})]+2\mathbb{E}[\sum_{i,j}^{n}\mu_{i}a_{ij}(y_{j}-\mu_{j})] + \mu^{t}A\mu\\
			\intertext{using the fact that $y_{1},y_{2},...,y_{n}$ are independent, }
			&=\sum_{i}^{n}a_{ii}\underbrace{\mathbb{E}[(y_{i}-\mu_{i})^{2}]}_{\sigma^{2}}+\sum_{i\neq j}a_{ij}\underbrace{\mathbb{E}[(y_{i}-\mu_{i})]\mathbb{E}[(y_{j}-\mu_{j})]}_{0} + 2\sum_{i,j}^{n}\mu_{i}a_{ij}\underbrace{\mathbb{E}[(y_{j}-\mu_{j})]}_{0}+\mu^{t}A\mu \\
			&=\sigma^{2}\sum_{i}^{n}a_{ii} + \mu^{t}A\mu=\sigma^{2}tr(A) + \mu^{t}A\mu. \ as required.
		\end{align*}
	}
	\textbf{proof.} (\textit{of the Proposition 1.2.2})
	{\fontfamily{lmr}\selectfont
		
		$\mathbb{E}[s^{2}]=\frac{1}{n-p-1}\mathbb{E}[RSS]= \frac{1}{n-p-1}\mathbb{E}[y^{t}Py]=\frac{1}{n-p-1}(\sigma^{2}tr(P) + (\mathbb{E}[y])^{t}P(\mathbb{E}[y]))$ (by   \textit{lemma(3)} ).  Since $P=[\mathbb{I}_{n}-X(X^{t}X)^{-1}X^{t}]$ and $X(X^{t}X)^{-1}X^{t}$ are idempotent matix, using lemma(2) ,we have $rank(X(X^{t}X)^{-1}X^{t})=tr(X(X^{t}X)^{-1}X^{t})=tr(X^{tX}(X^{t}X)^{-1})=tr(\mathbb{I}_{p+1})=p+1$. since $tr(A+B)=tr(A)+tr(B)$ , we have $tr(P)=tr(\mathbb{I}_{n})-tr(X(X^{t}X)^{-1}X^{t})=n-p-1$. Recalling that $(\mathbb{E}[y])=X\beta$, we finally obtain
		\begin{align*}
			\mathbb{E}[s^{2}]&=\frac{1}{n-p-1}[\sigma^{2}(n-p-1) -(X\beta)^{t}[\mathbb{I}_{n}-X(X^{t}X)^{-1}X^{t}](X\beta)]\\
			&= \frac{1}{n-p-1}[\sigma^{2}(n-p-1) -(X\beta)^{t}[(X\beta)-(X\beta)]]=\sigma^{2}
		\end{align*}
		and we have the result.
	}
	\subsubsection{Assessing The Accuracy of The Model ( \textit{ visit \cite{ref1}})} 
	
	Once the parameters of the model have been estimated, It is natural to want to quantify the extent to which the model fits the data.The quality of a linear regression fit is typically assessed using two related quantities: the "\emph{Residual Standard Error" (RSE)} and the $R^{2}$ statistic.
	\begin{description}
		\item[Residual Standard Error (RSE):] Recall from the model \eqref{f2}  that associated with each observation an error term $\epsilon$ .Due to the presence of these error terms, even if we knew the true regression line ( i.e even if $\beta^{t}=(\beta_{0},...,\beta_{p})$ where known) , we would not be able to perfectly predict $Y$ from $X$.
		
		The RSE is an estimate of the standard deviation of $\epsilon$.Roughly speaking, it is the average amount that the response will deviate from the true regression line.It is computed using the formula
		\begin{equation}
			RSE=\sqrt{s^{2}}=\sqrt{\frac{1}{n-p-1}\sum_{i=1}^{n}(y_{i}-\hat{y_{i}})^{2}.
			} \label{f6}
		\end{equation}
		The RSE is considered a measure of the  lack of fit of the model \eqref{f2} to the data. If the model are very close to the outcomes values, that is, if $\hat{y_{i}}\simeq y_{i}$ for $i=1,...,n$ , then \eqref{f6} will be small, and we can conclude that the model fits the data very well.On the other hand, if $\hat{y_{i}}$  is very far from $y_{i}$  for one or more observations, then the RSE may be quite large, indicating that the model doesn't fit the data very well.
		\item[$R^{2}$ Statistic:] The RSE provides an absolute measure of lack of fit of the model to the data.But since it is measured in the unit of $Y$, it is not always clear what constitutes a good RSE. The $R^{2}$ Statistic provides an alternative measure of fit.It take a form of a proportion: The proportion of variance explained and so it always takes on a value between $0$ and $1$; an is independent of the scale Y.\\
		To calculate $R^{2}$ , we use the formula
		\begin{equation}
			R^{2}=\frac{TSS-RSS}{TSS}=1-\frac{RSS}{TSS}
			\label{f7}
		\end{equation}
		where, $TSS=\sum_{i=1}^{n}(y_{i}-\bar{y})^{2}$ is the total sum of squares, and RSS is defined in \eqref{f4}.\\
		$TSS$ measures the total variance in the response Y, and can be thought as the amount of variability inherent in response before the regression is performed. In contrast, $RSS$ measures the amount of variability that is left unexplained after performing the regression. Hence,$TSS-RSS$ measures the amount of variability in the response that is explained by performing the regression; and $R^{2}$ measure the proportion of variability in $Y$  that can be explained using $X$.
		
		An $R^{2}$ Statistic that is close to $1$ indicates that a large proportion of the variability in the response is explained by the regression. A number near $0$ indicate that the regression does not explain much of the variability in the response; this might occur because the linear model is wrong or the error variance $\sigma^{2}$ is high.
	\end{description}
	Now that a brief presentation of the linear model has been made, come back to the main question to know the problems encountered in high-dimension setting.
	\begin{description}
		\item[Theoretically:] When $\ p>n,\ X^{t}X\ $ is not invertible (or near singular ) and $\ s^{2}\ $ in \eqref{f4} is not defined.
		\item[i) visualisation problem:] Datasets with large number of features are difficult to visualise.When exploring low-dimensional datasets, it is possible to plot the response variable against each of the limited number of explanatory variables to get an idea which of these are important predictors of the response.With high-dimensional data,the large number of explanatory variables makes doing this difficult.
		\item[ ii) Risk of Overfitting:] When the number of features $p$ is as large as or large than, the number of observations ($p\geq n$), least squares as described previously should not be performed.The reason is simple; regardless of whether or note there truly is a relationship between the features and the response, least square will yield a set of coefficient estimates that result in a perfect fit to the data, such that the residuals are zero.
		In other words, though it is possible to perfectly fit the training data in high-dimensional setting, the resulting linear model will perform extremely poorly on an independent text set and therefore does not constitute a useful model.
		
		This indicates the importance of applying extra care methods when analysing data sets with a large number of variables, and of always evaluating model performance on an independent test set.
		\item[iii) Multi-Collinearity problem:] Another problem in carrying out regression on high-dimensional data is dealing with multi-collinearity. \label{mcp}
		\begin{definition}
			In multilinear regression, collinearity refers to the situation in which two or more variables are highly correlated.
		\end{definition}
		The large numbers of features in these datasets makes high correlations between variables more likely. Consider the regression model \eqref{f1} , the collinearity occurs if the independent variable $X_{i}$ is highly linearly correlated to another one or more independent variable $(X_{j})_{1\leq j\neq i \leq p}$ ; in other words,the $i^{th}$ column of $X$ can be \emph{almost} linearly express by one or more other  column vectors in $X$.
		
		If there is a perfect collinearity among column vectors of $X$ then  the matrix $X^{t}X$ is not invertible.Therefore , it is problematic to solve for the unique least squares estimators of the regression coefficients from the normal equation \eqref{f3} $b=(X^{t}X)^{-1}X^{t}y$.
		
		when the column vectors of the design matrix $X$ are highly correlated, then the matrix $X^{t}X$ becomes ill-conditioned\footnote{ its condition number is too large, see " \url{https://arxiv.org} " for more detail} or near singular and the least squares estimator become less reliable even though we can find a unique solution of the normal equation.To see this let's look at the following example of two simple data sets:
		
		\begin{minipage}{0.49\textwidth}
			\begin{table}[H]
				\begin{center}
					\begin{tabular}{|c|c|}
						\hline
						$x_{1}$ & $x_{2}$ \\
						\hline
						10&10\\
						\hline
						10&10\\
						\hline
						10&15\\
						\hline
						10&15\\
						\hline
						15&10\\
						\hline
						15&10\\
						\hline
						15&15\\
						\hline
						15&15\\
						\hline
					\end{tabular}
				\end{center}
				\caption{Two independent vectors}
				\label{fig2}
			\end{table}
		\end{minipage}
		\hspace{4ex}
		\begin{minipage}{0.49\textwidth}
			\begin{table}[H]
				\begin{center}
					\begin{tabular}{|c|c|}
						\hline
						$x_{1}$ & $x_{2}$ \\
						\hline
						10&10\\
						\hline
						11&11.4\\
						\hline
						11.9&12.2\\
						\hline
						12.7&12.5\\
						\hline
						13.3&13.2\\
						\hline
						14.2&13.9\\
						\hline
						14.7&14.4\\
						\hline
						15&15\\
						\hline
					\end{tabular}
				\end{center}
				\caption{Two highly correlated vectors}
				\label{fig3}
			\end{table}
		\end{minipage} \newline
		
		The correlation matrix of vectors in the first example datasets table \ref{fig2} is $2\times2$ identity matrix 
		$$ X^{t}X = 
		\begin{pmatrix}
			1 & 0 \\
			0 & 1
		\end{pmatrix}, \quad (X^{t}X)^{-1}= \begin{pmatrix}
			1 & 0 \\
			0 & 1
		\end{pmatrix}
		$$
		The correlation matrix of the two vectors in the second example datasets table \ref{fig3} is ,
		$$  X^{t}X = 
		\begin{pmatrix}
			1 & 0.99215 \\
			0.99215& 1.0000
		\end{pmatrix}, \quad (X^{t}X)^{-1}= \begin{pmatrix}
			63.94 & -64.44 \\
			-64.44 & 63.94
		\end{pmatrix}
		$$
		recall that for linear regression, $Cov(b)=(X^{t}X)^{-1}\sigma^{2}$ ( see proposition \ref{10})) ; for the vector in the second example dataset we have
		$$ Cov(b)=
		\begin{pmatrix}
			\sigma^{2} & 0 \\
			0 & \sigma^{2}
		\end{pmatrix},\ b=(b_{1},b_{2})^{t}\quad say,\ Var(b_{1})=Var(b_{2})=\sigma^{2}, $$
		for the example in the second example dataset, we have
		$$ Var(b_{1})=Var(b_{2})=63.94\times \sigma^{2}, $$
		The variances of the regression coefficients are \textit{inflated} in the second datasets; this is because of the collinearity of the two vectors in the dataset.
		\begin{lemme}
			An $n\times n$ ill-conditioned or near singular matrix has at least one of its eigenvalues close to zero, and then the eigenvalue of the inverse tend to be very large.
		\end{lemme}	 
		\begin{proposition}[ \textit{ from \cite{ref4}} ]
			The  average Euclidean distance measure $\mathbb{E}[\parallel b-\beta \parallel^{2}]$ between the least squares estimate $b$ and the true parameter $\beta$ is given by:
			\begin{equation}
				\mathbb{E}[\parallel b-\beta \parallel^{2}]=\sigma^{2}tr[(X^{t}X)^{-1}]
				\label{f8}
			\end{equation}
		\end{proposition}
		\textbf{proof.}
		{\fontfamily{lmr}\selectfont
			$$\mathbb{E}[\parallel b-\beta \parallel^{2}]=\mathbb{E}[\parallel b \parallel^{2} +\parallel \beta \parallel^{2} -2<b,\beta> ]=\mathbb{E}[\parallel b \parallel^{2}] +\parallel \beta \parallel^{2} -2 \beta^{t}\underbrace{\mathbb{E}[b]}_{\beta}=\mathbb{E}[\parallel b \parallel^{2}] -\parallel \beta \parallel^{2},$$ 
			\begin{align*}
				\mathbb{E}[\parallel b \parallel^{2}] & =\mathbb{E}[ b^{t}b] = \mathbb{E}[(X^{t}X)^{-1}X^{t}y)^{t}(X^{t}X)^{-1}X^{t}y] \\
				& =\mathbb{E}[ y^{t}(X(X^{t}X)^{-1}(X^{t}X)^{-1}X^{t})y] \\
				\intertext{using lemma(3), we get}
				& = \sigma^{2}tr[X(X^{t}X)^{-1}(X^{t}X)^{-1}X^{t}] + \mathbb{E}[y]^{t}(X(X^{t}X)^{-1}(X^{t}X)^{-1}X^{t})\mathbb{E}[y] \\
				& =  \sigma^{2}tr[X(X^{t}X)^{-1}(X^{t}X)^{-1}X^{t}] + (X\beta)^{t}(X(X^{t}X)^{-1}(X^{t}X)^{-1}X^{t})(X\beta) \\
				&=\sigma^{2}tr[(X^{t}X)^{-1}] + \beta^{t}\beta = \sigma^{2}tr[(X^{t}X)^{-1}] + \parallel \beta \parallel^{2}
			\end{align*}
		}
		\begin{remarque}
			Assuming that $(X^{t}X)$ has $k$ distinct eigenvalues $\lambda_{1},...,\lambda_{k}$ , then the eigenvalues of $(X^{t}X)^{-1}$ are $\frac{1}{\lambda_{1}},...,\frac{1}{\lambda_{k}}$, denoting by $ V=(v_{1},...,v_{k})^{t}$ the corresponding normalized eigenvectors, we can write $V^{t}(X^{t}X)^{-1}V=D=diag(\frac{1}{\lambda_{1}},...,\frac{1}{\lambda_{k}})$.\\ Moreover, $tr(X^{t}X)^{-1})=tr(V^{t}V(X^{t}X)^{-1})=tr(V^{t}(X^{t}X)^{-1}V)=tr(D)=\sum_{i=1}^{k}\frac{1}{\lambda_{i}}$ ; we then have:
			\begin{equation}
				\mathbb{E}[\parallel b-\beta \parallel^{2}]=\sigma^{2}\sum_{i=1}^{k}\frac{1}{\lambda_{i}}\  \Leftrightarrow\ \mathbb{E}[\parallel b \parallel^{2}]=\parallel \beta \parallel^{2} + \sigma^{2}\sum_{i=1}^{k}\frac{1}{\lambda_{i}}.
				\label{f9}
			\end{equation}	
		\end{remarque}
		
		Now it is easy to see that if one of $\lambda_{i} ,\ i=1,...,k$ is very small, say for instance $\lambda_{i}=0.00001$ then roughly, $\parallel b \parallel^{2}=\sum_{i=1}^{k}b_{i}^{2}$ may \textbf{over estimate} $\parallel \beta \parallel^{2}=\sum_{i=1}^{k}\beta_{i}^{2}$ by $10000\sigma^{2}$ times.\newline
		The above discussions indicate that if some columns in $X$ are highly correlated with other column in $X$ then, from \textit{lemma(4)} , the covariance matrix $Cov(b)=(X^{t}X)^{-1}\sigma^{2}$ will have one or more large eigenvalues so that the mean Euclidean distance of $\mathbb{E}[\parallel b-\beta \parallel^{2}]$ will be inflated.Consequently, this makes the estimation of regression parameter $\beta$ less reliable.Thus the high levels correlation between variable in high-dimensional datasets will have negative impact on least square estimates of regression parameter.
		
		Clearly, alternative approaches that are better-suited to the high-dimensional setting are required.
	\end{description}
	\section{Statistical Suitable Methods for Analysing High-Dimensional Data}
	As we found out in the above challenges, carrying out linear regression on datasets with large numbers of features is difficult due to: 
	
	High levels of correlation between variables ,difficulty to identifying a clear response by visualizing and risk of over-fitting.This problems are common to the analysis of many high-dimensional datasets, for example, those using genomics data with multiples genes. While straightforward linear regression cannot be used in datasets with many features, high-dimensional regression methods are available with methods to deal with multicollinearity , over-fitting and fitting models including many explanatory variables.
	\subsection{Ridge Regression}
	\label{rre}
	Ridge regression is one of the remedial measures for handling severe multicollinearity in least squares estimation.Multicollinearity occurs when the predictors included in the linear model are highly correlate with each other.When this is the case, the matrix $X^{t}X$ tends to be singular or ill-conditioned and hence identifying the least squares estimates will encounter numerical problems.
	
	To motivate the Ridge estimator, we first take a look at the \textbf{Mean Square Error}\footnote{MSE is a commonly-used measured for assessing quality of estimation} (MSE) ,$MSE(b)=\mathbb{E}[\parallel b-\beta \parallel^{2}]$ of least squares estimator of $\beta$, which can be break into two parts: $bias^{2} + variance$.
	\begin{proposition}
		$	\mathbb{E}[\parallel b-\beta \parallel^{2}]	=\sum_{j=1}^{n}	(\mathbb{E}[b_{j}]-\beta_{j})^{2} + \sum_{j=1}^{n}	Var[b_{j}]$
	\end{proposition}
	\textbf{proof.}
	{\fontfamily{lmr}\selectfont
		\begin{align*}
			\mathbb{E}[\parallel b-\beta \parallel^{2}]&=\sum_{j=1}^{n}	\mathbb{E}[(b_{j}-\beta_{j})^{2}]
			=\sum_{j=1}^{n}	\mathbb{E}[b_{j}^{2}]+\beta_{j}^{2} -2\beta_{j}\mathbb{E}[b_{j}]\\
			&=\sum_{j=1}^{n}	Var[b_{j}]+\mathbb{E}[b_{j}]^{2}+\beta_{j}^{2} -2\beta_{j}\mathbb{E}[b_{j}]
			=\sum_{j=1}^{n}	(\mathbb{E}[b_{j}-\beta_{j})^{2} + \sum_{j=1}^{n}	Var[b_{j}]
	\end{align*} }
	According to "Gauss-Markov" theorem, the least squares approach achieves the smallest variance among all unbiased linear estimates.This however does not necessarily guarantee the minimum \textbf{MSE}.
	
	To better distinguish different type of estimators, let $\hat{\beta}^{LS}$ denote the ordinary least square estimator of $\beta$. We shown that $MSE(\hat{\beta}^{LS})=\mathbb{E}[\parallel \hat{\beta}^{LS}-\beta \parallel^{2}]=\sigma^{2}tr[(X^{t}X)^{-1}]$ \eqref{f8} thus,$\mathbb{E}[\parallel \hat{\beta}^{LS} \parallel^{2}]=\parallel \beta \parallel^{2} + \sigma^{2}tr[(X^{t}X)^{-1}$ \eqref{f9}; it can be seen that, with ill-conditioned $X^{t}X$, the resultant LSE $\hat{\beta}^{LS}$ would be large in length $\parallel\hat{\beta}^{LS}\parallel$ and associated with inflated standard error ( see \eqref{f9}). This inflated variation would lead to poor model prediction as well.
	
	The Ridge regression is a constrained version of least squares.It tackles the estimation problem by providing biased estimator yet with small variance.
	\begin{center}
		\subsubsection{Ridge Shrinkage Estimator}
	\end{center}
	\begin{theoreme}
		For any estimator $b$, the least squares criterion $	\mathcal{Q}(b)= \parallel y-Xb \parallel^{2}$ can be rewritten as its minimum, reached at $\hat{\beta}^{LS}$ plus a quadratic form in $b$.
	\end{theoreme}
	\textbf{proof.}
	{\fontfamily{lmr}\selectfont \begin{align*}
			\mathcal{Q}(b)&= \parallel y-Xb \parallel^{2} = \parallel y-X\hat{\beta}^{LS} + X\hat{\beta}^{LS}-Xb\parallel^{2}\\
			&=\parallel y-X\hat{\beta}^{LS}  \parallel^{2} + \parallel X\hat{\beta}^{LS}-Xb  \parallel^{2} +2<X\hat{\beta}^{LS}-Xb, y-X\hat{\beta}^{LS}>\\
			&=\mathcal{Q}_{min} + ( X\hat{\beta}^{LS}-Xb)^{t}( X\hat{\beta}^{LS}-Xb)+2(<X\hat{\beta}^{LS}-Xb)^{t}(y-X\hat{\beta}^{LS}) \\
			&=\mathcal{Q}_{min}+\underbrace{( \hat{\beta}^{LS}-b)^{t}X^{t}X( \hat{\beta}^{LS}-b)}_{\phi(b)}  +\underbrace{2(\hat{\beta}^{LS}-b)^{t}X^{t}(y-X\hat{\beta}^{LS})}_{A} \\
			& A=2(\hat{\beta}^{LS}-b)^{t}X^{t}(y-X(XX^{t})^{-1}X^{t}y)= 2(\hat{\beta}^{LS}-b)^{t}[X^{t}y-(XX^{t})((XX^{t})^{-1})X^{t}y]\\
			&= 2(\hat{\beta}^{LS}-b)^{t}[X^{t}y-X^{t}y]=0
		\end{align*}
		\begin{equation}
			thus,\quad  \mathcal{Q}(b)=\mathcal{Q}_{min} + \phi(b)
			\label{f10}
	\end{equation} }
	
	
	contour for each constant of the quadratic form $\phi(b)$ are hyper-ellipsoids centred at ordinary LSE $\hat{\beta}^{LS}$. In view of \eqref{f9}, it is reasonable to expect that, if one move away from $\mathcal{Q}_{min}$, the movement is in a direction which shortens the length of $b$.
	
	The optimization problem in Ridge regression can be state as:
	$$ minimize \parallel \beta \parallel^{2}\ subject\ to\ ( \hat{\beta}^{LS}-b)^{t}X^{t}X( \hat{\beta}^{LS}-b)=\phi_{0}\ for\ some\ constant\ \phi_{0}.$$
	The enforced constrain guarantees a relatively small residual sum of squares $\mathcal{Q}(\beta)$ when compared to its minimum $\mathcal{Q}_{min}$.As a Lagrangian problem, it is equivalent to 
	$$ minimizing\ f(\beta) = \parallel \beta \parallel^{2} + \frac{1}{k}[( \hat{\beta}^{LS}-b)^{t}X^{t}X( \hat{\beta}^{LS}-b)-\phi_{0}],\qquad k>0$$
	Where $\frac{1}{k}$ is the multiplier chosen to satisfy the constraint.
	\begin{proposition}[Hoerl and Kennard (1970)]
		The numerical solution of this problem corresponding to the Ridge regression estimator of $\beta$ is,
		\begin{equation}
			\hat{\beta}^{R}=(X^{t}X+k\mathbb{I}_{p})^{-1}X^{t}y
			\label{f11}
		\end{equation}
	\end{proposition}
	\textbf{proof.}
	{\fontfamily{lmr}\selectfont $ f(\beta)=\sum_{i}\beta_{i}^{2}+\frac{1}{k}\bigg[\sum_{i}(\beta_{i}-\hat{\beta}^{LS}_{i})^{2}a_{ii}+2\sum_{1\leq i < j \leq n}(\beta_{i}-\hat{\beta}^{LS}_{i})a_{ij}(\beta_{j}-\hat{\beta}^{LS}_{j})-\phi_{0}\bigg] $\\
		by taking partial derivative with respect to each component of $\beta$, we obtain $\frac{\partial f(\beta)}{\partial \beta}=2\beta + \frac{1}{k}(2X^{t}X(\beta-\hat{\beta}^{LS}))$,}
	
	\begin{align*}
		\frac{\partial f(\beta)}{\partial \beta}=0& \Rightarrow 2\beta + \frac{1}{k}(2X^{t}X\beta)=\frac{2}{k}X^{t}X\hat{\beta}^{LS} \\
		& \Rightarrow \beta=(X^{t}X+k\mathbb{I}_{p})^{-1}X^{t}X\hat{\beta}^{LS}=(X^{t}X+k\mathbb{I}_{p})^{-1}X^{t}X(X^{t}X)^{-1}X^{t}y=(X^{t}X+k\mathbb{I}_{p})^{-1}X^{t}y
	\end{align*}
	
	An equivalent way is to write the Ridge problem in the penalized or constrained least squares form by :
	\begin{equation}
		minimize\  \parallel y- X\beta \parallel^{2}\quad subject\ to\ \parallel \beta \parallel^{2}\leq s\ for\ some\ constant\ s
		\label{u1}
	\end{equation}
	the Lagrangian problem become
	\begin{equation}
		minimizing\ \parallel y- X\beta \parallel^{2}+\lambda\parallel \beta \parallel^{2}
		\label{f12}
	\end{equation} 
	which yield the same estimator given in \eqref{f11}.The penality parameter $\lambda \geq 0$ controls the amount of shrinkage in $\parallel \beta \parallel^{2}$.The large value of $\lambda$, the greater amount of shrinkage.For this reason, the Ridge estimator is also called the shrinkage estimator. There is one-to-one correspondence among $\lambda,\ s,\ k\ and\ \phi_{0}$.
	
	\begin{figure}[h!]
		\begin{center}
			\includegraphics[scale=1]{ridge figure.png}
		\end{center}
		\caption[Contour of the error and constraint function for the Ridge regression]{contour of the error and constraint function for the Ridge regression in two dimensional case.
			The solid blue area is the constraint region $\beta_{1}^{2}+ \beta_{2}^{2}\leq s$ , while the red ellipse is the contour of RSS. \textbf{  ($\ figure\ from.\ \cite{ref1},\ P.244) $} } 
		\label{tab2}
	\end{figure}
	\subsubsection{Why does Ridge regression improve over Least Square ?}
	
	We shall compute the expectation and variance of $	\hat{\beta}^{R}$.Eventually, we want to compare $	\hat{\beta}^{R}$ with $	\hat{\beta}^{LS}$ to see whether a smaller MSE can be achieve by $	\hat{\beta}^{R}$ for certain values of $k$.
	\begin{remarque}
		$	\hat{\beta}^{R}=(X^{t}X+k\mathbb{I}_{p})^{-1}X^{t}y=(\mathbb{I}_{p}+k(X^{t}X)^{-1})^{-1}(X^{t}X)^{-1}X^{t}y=(\mathbb{I}_{p}+k(X^{t}X)^{-1})^{-1}\hat{\beta}^{LS}.$
		denote $Z=(\mathbb{I}_{p}+k(X^{t}X)^{-1})^{-1}$.
		\begin{equation}
			\mathbb{E}[\hat{\beta}^{R}]=\mathbb{E}[Z\hat{\beta}^{LS}]=Z\mathbb{E}[\hat{\beta}^{LS}]=Z\beta,\quad \hat{\beta}^{R}\ is\ a\ biased\ estimator 
			\label{f13}
		\end{equation}
		\begin{equation}
			Cov(\hat{\beta}^{R})=ZCov(\hat{\beta}^{LS})Z^{t}=Z(X^{t}X)^{-1}\sigma^{2}Z^{t}=\sigma^{2}[Z(X^{t}X)^{-1}Z^{t}]
			\label{f14}
		\end{equation}
	\end{remarque}
	
	
	Let $\lambda_{max} = \lambda_{1} \geq \lambda_{2} \geq...\geq \lambda_{p}=\lambda_{min}$ denote the eigenvalues of $X^{t}X$, then the corresponding eigenvalues of $Z$ are $\frac{\lambda_{j}}{\lambda_{j} + k},\ j=1,...,p$. From \eqref{f9}, $MSE(\hat{\beta}^{LS})=\sigma^{2}\sum_{j=1}\frac{1}{\lambda_{j}}$.
	\begin{proposition}[ \textit{from \cite{text}}]
		If $MSE(\hat{\beta}^{R},k)$ denote the mean square error of ridge regression estimator, then 
		\begin{equation}
			MSE(\hat{\beta}^{R},k)=k^{2}\beta^{t}(X^{t}X+k\mathbb{I})^{-2}\beta +\sigma^{2}\sum_{j}\frac{1}{\lambda_{j}} .	\frac{\lambda_{j}^{2}}{(\lambda_{j}+k)^{2}}=\lambda_{1}(k)+\lambda_{2}(k).
			\label{f17}
		\end{equation}
	\end{proposition}
	\textbf{proof.}
	{\fontfamily{lmr}\selectfont  For the component in Ridge estimator, it can be found from \eqref{f13} and \eqref{f14} that the sum of their square biases is:\begin{align*}
			\sum_{j}	(\mathbb{E}[\hat{\beta}^{R}_{j}]-\beta_{j})^{2} &= (\mathbb{E}[\hat{\beta}^{R}]-\beta)^{t}(\mathbb{E}[\hat{\beta}^{R}]-\beta)=(Z\beta-\beta)^{t}((Z\beta-\beta)\\
			&= \beta^{t}(\mathbb{I}-Z)^{t}(\mathbb{I}-Z)\beta \\
			\intertext{we have $Z^{t}=Z$, thus }
			&(\mathbb{I}-Z)^{t}=\mathbb{I}-Z=\mathbb{I}-(\mathbb{I}+k(X^{t}X)^{-1})^{-1}\\
			& = [\mathbb{I}+k(X^{t}X)^{-1}]^{-1}[\mathbb{I}+k(X^{t}X)^{-1}]-[\mathbb{I}+k(X^{t}X)^{-1}]\\
			&=[\mathbb{I}+k(X^{t}X)^{-1}](\mathbb{I} +k(X^{t}X)^{-1}-\mathbb{I})=[(X^{t}X)^{-1}(X^{t}X+k\mathbb{I})]^{-1}(k(X^{t}X)^{-1})\\
			&=k(X^{t}X+k\mathbb{I})^{-1}(X^{t}X)(X^{t}X)^{-1}=k(X^{t}X+k\mathbb{I})^{-1}
		\end{align*}
		\begin{equation}
			thus,\quad 	\sum_{j}	(\mathbb{E}[\hat{\beta}^{R}_{j}]-\beta_{j})^{2}=\beta^{t}(\mathbb{I}-Z)^{t}(\mathbb{I}-Z)\beta=k^{2}\beta^{t}(X^{t}X+k\mathbb{I})^{-2}\beta
			\label{f15}
		\end{equation}
		an the sum of their variance,
		\begin{align*}
			\sum_{j} Var[\hat{\beta}^{R}_{j}]&= tr(Cov(\hat{\beta}^{R}))=\sigma^{2}tr(Z(X^{t}X)^{-1}Z^{t})\\
			&=\sigma^{2}tr((X^{t}X)^{-1}Z^{t}Z)=\sigma^{2}tr((X^{t}X)^{-1}Z^{2})
		\end{align*}
		\begin{equation}
			thus,\quad 	\sum_{j}Var[\hat{\beta}^{R}_{j}]=\sigma^{2}\sum_{j}\frac{1}{\lambda_{j}} \times 	\frac{\lambda_{j}^{2}}{(\lambda_{j}+k)^{2}}
			\label{f16}
		\end{equation}
		hence,
		$$MSE(\hat{\beta}^{R},k)=k^{2}\beta^{t}(X^{t}X+k\mathbb{I})^{-2}\beta +\sigma^{2}\sum_{j}\frac{1}{\lambda_{j}} .	\frac{\lambda_{j}^{2}}{(\lambda_{j}+k)^{2}}$$}
	
	\begin{remarque}
		The function $\lambda_{1}(k)$ is a monotonic increasing function of $k$ while $\lambda_{2}(k)$ is monotonically decreasing. 
		
		The constant $k$ reflects the amount of bias increased and the variance reduced. When $k=0$, it becomes LSE.
	\end{remarque}
	\begin{theoreme}[ \emph{Hoerl and Kennard (1970)}]
		There always exists a $k > 0$ such that, 
		$$MSE(\hat{\beta}^{R},k)<MSE(\hat{\beta}^{R},0)=MSE(\hat{\beta}^{LS})$$
	\end{theoreme}
	\textbf{proof.} \textit{later in chapter 3, similar the proof of theorem 3.1.2}
	
	in other words, the Ridge estimator can out perform the \emph{LSE } in terms of providing a smaller \emph{MSE}.Nevertheless, in practice, the choice of $k$ is yet to be determined and hence there is no guarantee that a smaller MSE always be attained by Ridge regression.
	
	Before we take and example, it is important to not that ,the Ridge solution is not invariant under scaling of inputs.Thus, one should standardize both the inputs and the response 
	$$ x_{ij}^{'}=\frac{x_{ij}-\bar{x_{j}}}{s_{x_{j}}}\quad and\quad  y_{i}^{'}=\frac{y_{i}-\bar{y}}{s_{y}}\quad such\ that\quad \frac{1}{n} \sum_{i} x_{ij}^{'}=0\    \frac{1}{n} \sum_{i} y_{i}^{'}=0\ and\quad \frac{1}{n} \sum_{i} x_{ij}^{'2}=1$$
	before computing the shrinkage estimator in \eqref{f11}.Besides, the intercept $\beta_{0}$ is automatically suppressed  when working with standardized data.
	\subsection{Lasso Regression}
	The Lasso (Least Absolute Shrinkage and Selection Operator) is another shrinkage method like Ridge regression, yet with an important and attractive feature in variable selection.
	
	Ridge regression does have one obvious disadvantage; unlike \emph{best subset, forward step-wise,backward step-wise}\footnote{methods used in low-dimension regression to select the most appropriate variables for a best model}, which will generally select models that involve just a subset of variables, Ridge regression will include all $p$ predictors in the final model.The penality $\lambda\parallel \beta \parallel^{2}$  in \eqref{f12} will shrink all of the coefficients towards zero, but it will not set any of them exactly to zero (unless $\lambda=\infty$).This may not be a problem for prediction accuracy, but it can create a challenge in model interpretation in setting in which the number of variables $p$ is quite large.Increasing the value of $\lambda$ will tend to reduce the magnitudes of the coefficients, but will not result in exclusion of any of the variables.
	
	The Lasso is a relatively recent alternative to Ridge regression that overcomes this disadvantage .The Lasso estimator of $\beta$ is obtained by 
	\begin{equation}
		minimizing\  \bigg \{\parallel y- X\beta \parallel^{2}_{2}\bigg\}\quad subject\ to\ \sum_{j=1}^{p}|\beta_{j}|\leq s\ for\ some\ constant\ s
		\label{u2}
	\end{equation}
	Namely, the $L_{2}$ penality $\parallel \beta \parallel^{2}=\sum_{j=1}^{p}\beta_{j}^{2}$ in Ridge regression is replaced by the $L_{1}$ penality $\parallel \beta \parallel_{1}=\sum_{j=1}^{p}|\beta_{j}|$ in Lasso. The Lagrangian problem become:
	\begin{equation}
		minimize_{\beta \in \mathbb{R}^{p} } \{ \parallel y- X\beta \parallel^{2}+ \lambda \parallel \beta \parallel_{1}\}.
		\label{f18}
	\end{equation}
	\begin{figure}[h!]
		\begin{center}
			\includegraphics[scale=1]{lasso figure.png}
		\end{center}
		\caption[Contour of the error and constraint function for the Lasso regression]{contour of the error and constraint function for the Lasso regression in two dimensional case.
			The solid blue area is the constraint region $|\beta_{1}|+ |\beta_{2}|\leq s$ , while the red ellipse is the contour of RSS. \textsl{ ($\ figure\ from.\ \cite{ref1},\ P.244 ) $ } } 
		\label{tab3}
	\end{figure}
	
	Figure \ref{tab3} portrays the Lasso estimation problem in two dimensional case.The constraint region in Ridge regression has a disk shape while the constraint region in Lasso is a diamond. Both methods find the first point at which the elliptical contours hit the constraint region.However, unlike disk, the diamond has corners. If the solution occurs at a corner, then it has one coefficient $\hat{\beta_{j}}$ equal to zero.$L_{1}$ penality has the effect of forcing some of the coefficient estimates to zero when the turning parameter $\lambda$ is sufficiently large.Hence, much like \emph{best subset selection}, the Lasso performs variable selection.
	
	As a result, model generated from Lasso are generally much easier to interpret than those produced by Ridge regression.we can say that the Lasso yield \emph{sparse models}, that is models that involve only a subset of the variable.
	\begin{center}
		\subsubsection{Computation of Lasso Solution}
	\end{center}
	The Lasso problem is a convex program, specifically a quadratic program (\textbf{QP}) ( \textsl{visit \cite{ref2} for more detail.}) with a convex constraint.As such, there are many sophisticated \textbf{QP} methods for solving the Lasso. However, there is a particularly simple an effective computational algorithm, that gives insight into how the Lasso works. The Lagrangian form \eqref{f18}is especially convenient for numerical computation of the solution.
	$$minimize_{\beta \in \mathbb{R}^{p} } \{ \parallel y- X\beta \parallel^{2}+ \lambda \parallel \beta \parallel_{1}\}\ \Leftrightarrow\ minimize_{\beta \in \mathbb{R}^{p}} \{ \frac{1}{2n}\sum_{i=1}^{n}(y_{i}-\sum_{j=1}^{p}x_{ij}\beta_{j})^{2}+\lambda \sum_{j=1}^{p}|\beta_{j}| \}$$
	we will assume that both $y_{i}$ and the features $x_{ij}$ have been standardized (the intercept term $\beta_{0}$ can be omitted). 
	
	Let first consider a single predictor setting, based on samples $\{(x_{i},y_{i})\}_{i=1}^{n}$ the problem then is to solve $mimizing_{\beta \in \mathbb{R}^{p}} \{ \frac{1}{2n}\sum_{i=1}^{n}(y_{i}-x_{i}\beta)^{2}+\lambda|\beta| \}$. The standard approach to this univariate minimization problem would be to take gradient ( first derivative) with respect to $\beta$, and set it to zero.There is a complication however, because the absolute value function $|\beta|$ does not have a derivative at $\beta=0$.However, we can proceed by inspection of the function 
	\begin{align*}
		f(\beta)&=\frac{1}{2n}\sum_{i=1}^{n}(y_{i}-x_{i}\beta)^{2}+\lambda|\beta|=\frac{1}{2n}(\sum_{i=1}^{n}y_{i}^{2}+\sum_{i=1}^{n}x_{i}^{2}\beta^{2}-2\beta \sum_{i=1}^{n}x_{i}y_{i})+\lambda|\beta| \\
		\intertext{(\textit{recall we assumed that both $y_{i}$ and the features $x_{ij}$ have been standardized})}
		&=\frac{1}{2n}(n+n\beta^{2}-2\beta<x,y>)+\lambda|\beta|=\frac{1}{2}+\frac{1}{2}\beta^{2}-\beta\frac{<x,y>}{n}+\lambda|\beta|
	\end{align*}
	\begin{proposition}[ \textit{from \cite{nref9}} ]
		if $\beta$ and $\hat{\beta}^{Lasso} $ are the true and Lasso estimator in a single predictor setting based on samples  $\{(x_{i},y_{i})\}_{i=1}^{n}$ ,then we have:
		\begin{equation}
			\hat{\beta}^{Lasso}=sign(\hat{\beta}^{LS})( |\hat{\beta}^{LS}|-\lambda)_{+},\quad (x_{+}\footnote{$x_{+}=max(x,0)$}).
			\label{f19}
		\end{equation}
	\end{proposition}
	\textbf{proof.}
	{\fontfamily{lmr}\selectfont 
		\[f(\beta)=
		\left \{
		\begin{array}{r  l}
			&\frac{1}{2}+\frac{1}{2}\beta^{2} +\beta(\lambda-\frac{<x,y>}{n})\; si\ \beta \geq 0 \\
			&\frac{1}{2}+\frac{1}{2}\beta^{2} -\beta(\lambda+\frac{<x,y>}{n})\; si\ \beta<0
		\end{array}
		\right. ,\quad
		f^{'}(\beta)=
		\left \{
		\begin{array}{r  l}
			&\beta +(\lambda-\frac{<x,y>}{n})\; si\ \beta \geq 0 \\
			&\beta -(\lambda+\frac{<x,y>}{n})\; si\ \beta<0
		\end{array}
		\right.
		\]
		\begin{minipage}{0.49\textwidth}
			\textbf{	for $\beta >0$,}
			\begin{itemize}
				\item if $\frac{<x,y>}{n}>\lambda $, $f^{'}(\beta_{1})=0 \Rightarrow \beta_{1}= \frac{<x,y>}{n}-\lambda >0$
				\[
				\begin{tikzpicture}
					\tkzTabInit[espcl=1.5,lgt=1]{$\beta$/1,$f^{'}(\beta)$/1,$f(\beta)$/2}{$0$,$\beta_{1}$,$+\infty$}
					\tkzTabLine{d , - , z , +}
					\tkzTabVar{+/$f(0)$/,-/$f^(\beta_{1})$/,+/$+\infty$/}
				\end{tikzpicture}
				\]
				$argmin_{\beta}f(\beta)=\beta_{1}$
				\item if not, say $\frac{<x,y>}{n} \leq \lambda $, $f^{'}(\beta_{1})=0 \Rightarrow \beta_{1}= \frac{<x,y>}{n}-\lambda \leq 0$
				\[
				\begin{tikzpicture}
					\tkzTabInit[espcl=1.5,lgt=1]{$\beta$/1,$f^{'}(\beta)$/1,$f(\beta)$/2}{$0$,$+\infty$}
					\tkzTabLine{d ,  +}
					\tkzTabVar{-/$f(0)$/,+/$+\infty$/}
				\end{tikzpicture}
				\]
				$argmin_{\beta}f(\beta)=0$
				
			\end{itemize}
		\end{minipage}
		\hspace{4ex}
		\begin{minipage}{0.49\textwidth}
			\textbf{	for $\beta <0$,}
			\begin{itemize}
				\item if $\frac{<x,y>}{n}<-\lambda $, $f^{'}(\beta_{2})=0 \Rightarrow \beta_{2}= \frac{<x,y>}{n}+\lambda <0$
				\[
				\begin{tikzpicture}
					\tkzTabInit[espcl=1.5,lgt=1]{$\beta$/1,$f^{'}(\beta)$/1,$f(\beta)$/2}{$-\infty$,$\beta_{2}$,$0$}
					\tkzTabLine{ , - , z , + ,d}
					\tkzTabVar{+/$+\infty$/,-/$f^(\beta_{2})$/,+/$f(0)$/}
				\end{tikzpicture}
				\]
				$argmin_{\beta}f(\beta)=\beta_{2}$
				\item if not, say $\frac{<x,y>}{n} \geq -\lambda $, $f^{'}(\beta_{2})=0 \Rightarrow \beta_{2}= \frac{<x,y>}{n}-\lambda \geq 0$
				\[
				\begin{tikzpicture}
					\tkzTabInit[espcl=1.5,lgt=1]{$\beta$/1,$f^{'}(\beta)$/1,$f(\beta)$/2}{$-\infty$,$0$}
					\tkzTabLine{,  - ,d}
					\tkzTabVar{+/$+\infty$/,-/$f(0)$/}
				\end{tikzpicture}
				\]
				$argmin_{\beta}f(\beta)=0$
			\end{itemize}
		\end{minipage}
		we find that, 
		\begin{equation}
			\hat{\beta}^{Lasso}=
			\left \{
			\begin{array}{r l}
				\frac{<x,y>}{n}-\lambda & if\  \frac{<x,y>}{n}>\lambda\\
				\frac{<x,y>}{n}+\lambda& if\ \frac{<x,y>}{n}<-\lambda \\
				0& if\ |\frac{<x,y>}{n}|\leq \lambda
			\end{array}
			\right.
		\end{equation}
		remark that, $\hat{\beta}^{LS}=(x^{t}x)^{-1}x^{t}y=(\sum_{i=1}^{n}x_{i}^{2})^{-1}<x,y>=\frac{<x,y>}{n}$ , thus we can succinctly rewrite $\hat{\beta}^{Lasso}$ as :
		$$	\hat{\beta}^{Lasso}=sign(\frac{<x,y>}{n})( |\frac{<x,y>}{n}|-\lambda)_{+}=sign(\hat{\beta}^{LS})( |\hat{\beta}^{LS}|-\lambda)_{+}.$$}
	
	
	We see that the Lasso shrinks the least squares coefficient toward zero by a constant amount $\lambda$; least square coefficient that is less than $\lambda$ in absolute value is shrunken entirely to zero.The fact that some Lasso coefficients are shrunken entirely to zero explains why the Lasso performs feature selection.
	\begin{remarque}
		Using this intuition from univariate case, we can develop a simple \textit{coordinate wise} scheme for solving the full Lasso problem \eqref{f18} .More precisely, we repeatedly cycle  through the predictors in some fixed (but arbitrary) order (say, $j=1,...,p$), where at the $j^{th}$ step, we update the coefficient $\beta_{j}$ by minimizing the objective function  in this coordinate while holding fixed all other coefficients $\{\hat{\beta_{k}}, \ k \neq j\}$ at their current values.
	\end{remarque}
	Writing the objective function in \eqref{f18} as 
	$$f(\beta)=\frac{1}{2n}\sum_{i=1}^{n}(y_{i}-\sum_{1\leq k\neq j \leq p}x_{ik}\beta_{k}-x_{ij}\beta_{j})^{2}+\lambda \sum_{j\neq k}|\beta_{k}|+\lambda|\beta_{j}|$$
	we see that the solution for each $\beta_{j}$ can be expressed in terms of the partial residual $r_{i}^{(j)}=y_{i}-\sum_{1\leq k\neq j \leq p}x_{ik}\hat{\beta_{k}}$ as
	$$ \hat{\beta_{j}}^{Lasso}=sign(\frac{<x_{j},r^{(j)}>}{n})( |\frac{<x_{j},r^{(j)}}{n}|-\lambda)_{+}\ ,\eqref{f19}\quad where\  x_{j}=(x_{1j},...,x_{nj})^{t},\; r^{(j)}=(r_{1}^{(j)},...,r_{n}^{(j)})^{t}$$
	equivalently, since the full residual $r_{i}=\sum_{k=1}^{p}x_{ik}\hat{\beta_{k}}=r_{i}^{(j)}-x_{ij}\hat{\beta_{j}}$, we have $r^{(j)}=r +\hat{\beta_{j}}x_{j}$ where $ r=(r_{1},...,r_{n})^{t}$ then,
	\begin{align*}
		\frac{<x_{j},r^{(j)}>}{n}=\frac{1}{n}<x_{j},r +\hat{\beta_{j}}x_{j}>&=\frac{1}{n}<x_{j},r>+\frac{1}{n}\hat{\beta_{j}}\underbrace{<x_{j},x_{j}>}_{\frac{1}{n}\sum_{i}^{n}x_{ij}^{2}=n}\\
		&=\frac{1}{n}<x_{j},r>+\hat{\beta_{j}}
	\end{align*}
	therefore, the update can be written as:
	\begin{equation}
		\hat{\beta}_{j}^{Lasso}=sign(\frac{1}{n}<x_{j},r>+\hat{\beta_{j}})( |\frac{1}{n}<x_{j},r>+\hat{\beta_{j}}|-\lambda)_{+}\quad for\ all\ j=1,...,p.
	\end{equation}
	\subsubsection{Theoretical properties of Lasso penalty}
	\label{tpl}
A common assumption of Lasso model is \textbf{sparsity}, i.e only a small number of covariates influence the outcome.

Let $S=\big\{j:\beta_{j}\neq 0\big \}$ the index set of non-zero components of the true coefficient vector $\beta \in \mathbb{R}$ and denote the number of relevant covariate by $s=card\{S\}$.Under sparsity assumption, most components of $\beta$ are zero such that $s\ll p$.For any $\lambda\geqslant 0$ define the active set of the Lasso, $\hat{S}(\lambda)=\big\{j:\hat{\beta}_{j}(\lambda)\neq 0\}$.Given $\beta$ ,we order the covariates such that $S=\big\{1,\dots,s\},\ S^{c} =\big\{s+1,\dots,p\big \}$ and considering the partitioning $X=\big(X_{S},X_{S^{c}}\big)$ where $X_{S}\in \mathbb{R}^{n\times s}$ contains the $n$ measurements of the $s$ relevant covariates, and  $X_{S^{c}}\in \mathbb{R}^{n\times(p-s) }$ contains the $n$  measurements of the $(p-s)$ irrelevant covariates.Sample covariance matrix are denote by $\Sigma_{X}$ and the empirical covariance is given by $S_{XX}=\frac{X^{T}X}{n}$.

State the following basic inequality,
\begin{lemme}
	we have, 
	\begin{equation}
	\frac{1}{n}\parallel X\hat{\beta}^{Lasso}-X\beta \parallel^{2}_{2} +\lambda\parallel\hat{\beta}^{Lasso} \parallel_{1} \leq \frac{2 \epsilon^{t}X(\hat{\beta}^{Lasso}-\beta)}{n}+\lambda \parallel \beta \parallel_{1}.
	\label{f29}
	\end{equation}
\end{lemme}
\textbf{Proof.} refer to $\big[\ \cite{nref14},P.103 \big]$ \\
The random part part $2 \epsilon^{t}X(\hat{\beta}^{Lasso}-\beta)$ can be bounded in term of the $L_{1}$ norm of the parameters involved:
\begin{equation}
2| \epsilon^{t}X(\hat{\beta}^{Lasso}-\beta)|\leq \big( \underset{1\leq j \geq p}{max}2|\epsilon_{t}X_{(j)}|\big)\parallel \hat{\beta}^{Lasso}-\beta \parallel_{1}=2\parallel \epsilon^{t}X\parallel_{\infty}\parallel \hat{\beta}^{Lasso}-\beta \parallel_{1}.
\label{f30}
\end{equation}
Now let us introduce the set, $$ \mathcal{A}=\big\{ \frac{2}{n}\parallel \epsilon^{t}X\parallel_{\infty} \leq \lambda_{o}\big\},$$
for a suitable value of $\lambda_{o}$, the set $\mathcal{A}$ has large probability. Indeed, with Gaussian errors this follow from the following lemma:
\begin{lemme}
	 Suppose that the diagonal elements of the Gram matrix $\frac{X^{T}X}{n}$ equal $1$ for all $j$.Then we have for all $t>0$ and for $\lambda_{0}=2\sigma\sqrt{\frac{t^{2}+2\log(p)}{n}}$,
	 \begin{equation}
	 	\mathbb{P}\big( \mathcal{A}\big)\geq 1-2\exp(-\frac{t^{2}}{2})
	 	\label{f31}
	 \end{equation}
\end{lemme}
\textbf{Proof.} see [ \cite{nref14}, P.104 ]
\begin{corollaire}[Lasso estimation consistency]
	Let the assumption of lemma \textbf{6} hold. For some $t>0$, let the regularization parameter be $\lambda=2 \hat{\sigma}\sqrt{\frac{t^{2}+2\log(p)}{n}}$ , where $\hat{\sigma}$ is some estimator of $\sigma$. Then with probability at least $1-\alpha$, where $\alpha=2\exp(-\frac{t^{2}}{2})+\mathbb{P}\big(\hat{\sigma}\leq \sigma \big)$.We have:
	\begin{equation}
	\frac{2}{n}\parallel X(\hat{\beta}^{Lasso}-\beta )\parallel^{2}_{2} \leq 3\lambda \parallel \beta \parallel_{1}
	\label{f32}
	\end{equation}
\end{corollaire}
we thus conclude that, taking the regularisation parameter $\lambda$ of order $\sqrt{\frac{\log(p)}{n}}$ and assume that $\parallel \beta \parallel_{1}=o\big(\sqrt{\frac{n}{\log(p)}}\big)$, result in consistency of the Lasso.

This means that , up to the $\log(p)-term$ and compatibility constant $\Phi_{o}^{2}$, the mean squared prediction error is of the same order as if one knew a priori which of the covariates are relevant and using ordinary least squares estimation based on the true relevant $s$ only. 
 See also [\textit{Theorem 14.6, Chap 14 from Guedon et al. (2007)}] for the corresponding result for the random design.

Let us define  the vectors $\beta_{S}$ and $\beta_{S^{c}}$ by:
\begin{equation}
	\beta_{j,S}=\beta_{j}\mathbb{1}_{\{j\in S\}},\quad \beta_{j,S^{c}}=\beta_{j}\mathbb{1}_{\{j\notin S\}}. \label{f33}
\end{equation}
Clearly, $\beta=\beta_{S}+\beta_{S^{c}}$ ; $\beta_{S}$ has zeroes outside the index set $S$ and the elements of $\beta_{S^{c}}$ can only be non-zero in the complement $S^{c}$ of $S$.
\begin{definition}[Compatibility condition]
	We say the the compatibility conditionis met for the set $S$ if for some $\Phi_{0}>0$ and for all $\beta \in \mathbb{R}^{p}$ such that $\parallel\beta_{S^{c}}\parallel_{1}\leq 3 \parallel \beta_{S} \parallel_{1}$, it holds that 
	\begin{equation}
		\parallel \beta_{S} \parallel_{1}^{2}\leq \frac{1}{n}\frac{s\parallel X\beta \parallel_{2}^{2}}{\Phi_{o}2^{2}}=\frac{s(\beta^{t} S_{XX}\beta)}{\Phi_{o}^{2}}.
		\label{f34}
	\end{equation}
	
\end{definition}
 \begin{theoreme}
 	Suppose the compatibility condition holds for $S$.Then on $\mathcal{A}$, we have for $\lambda\geq 2\lambda_{0}$
 	\begin{equation}
 			\frac{1}{n}\parallel X(\hat{\beta}^{Lasso}-\beta )\parallel^{2}_{2}+  \lambda\parallel \hat{\beta}^{Lasso}-\beta \parallel_{1}\leq \frac{4 \lambda^{2}s}{\Phi_{o}^{2}}.
 			\label{f35}
 	\end{equation}
 	
 \end{theoreme}
\textbf{Proof.} the proof for this result is clearly detailed in [\textit{\cite{nref14}, Theorem 6.1,P.107}] using the following lemma:
\begin{lemme}
	On $\mathcal{A}$, with  $\lambda\geq 2\lambda_{0}$ we have:
	\begin{equation}
			\frac{2}{n}\parallel X(\hat{\beta}^{Lasso}-\beta )\parallel^{2}_{2}+ \lambda \parallel \hat{\beta}^{Lasso}_{S^{c}}\parallel_{1}\leq 3\lambda \parallel \hat{\beta}^{Lasso}_{S}-\beta_{S} \parallel_{1}.
			\label{f36}
	\end{equation}
\end{lemme}
\textbf{Proof.} on $\mathcal{A}$, by basic inequality \eqref{f29}  and using  $\lambda\geq 2\lambda_{0}$ , \\

$\frac{2}{n}\parallel X(\hat{\beta}^{Lasso}-\beta )\parallel^{2}_{2}+ 2\lambda \parallel \hat{\beta}^{Lasso}\parallel_{1}\leq \lambda \parallel \hat{\beta}^{Lasso}-\beta \parallel_{1}+2 \lambda \parallel \beta \parallel_{1}. (\ast)$. Using triangular inequality (second form), $\parallel \hat{\beta}^{Lasso}_{S}-\beta_{S} \parallel_{1}\geq \parallel \beta_{S} \parallel_{1} - \parallel\hat{\beta}^{Lasso}_{S} \parallel_{1}.$ Thus ,\\

$\parallel\hat{\beta}^{Lasso} \parallel_{1}=\parallel\hat{\beta}^{Lasso}_{S} \parallel_{1}+\parallel\hat{\beta}^{Lasso}_{S^{c}} \parallel_{1}\geq \parallel \beta_{S} \parallel_{1} -\parallel \hat{\beta}^{Lasso}_{S}-\beta_{S} \parallel_{1}+ \parallel\hat{\beta}^{Lasso}_{S^{c}} \parallel_{1}\, (\ast \ast)$.in the other hand,\\
$\parallel \hat{\beta}^{Lasso}-\beta \parallel_{1}=\parallel \hat{\beta}^{Lasso}_{S}-\beta_{S} \parallel_{1}+\parallel\hat{\beta}^{Lasso}_{S^{c}} \parallel_{1},\ (\ast \ast \ast).$ Using both inequalities $(\ast \ast)$ and $(\ast \ast \ast)$ in $(\ast)$ yield the result.

\begin{remarque}
 The theorem combines two results: 
 \begin{equation}
 	\frac{2}{n}\parallel X(\hat{\beta}^{Lasso}-\beta )\parallel^{2}_{2}\leq \frac{4 \lambda^{2}s}{\Phi_{o}^{2}}, \quad ( the\ bound\ for\ predictions\ error)
 	\label{f37}
 \end{equation}
\begin{equation}
	\parallel \hat{\beta}^{Lasso}-\beta \parallel_{1}\leq \frac{4 \lambda^{2}s}{\Phi_{o}^{2}}, \quad ( the\ bound\ for\ L_{1}-error\ of\ coefficients\ estimates.)
	\label{f38}
\end{equation}
\end{remarque}
\begin{corollaire}[estimation accuracy of $\beta$]
	Under compatibility assumptions on design matrix $X$ and on the sparsity $s=card\{S\}$ , for lambda in the suitable range of order $\lambda \thickapprox \sqrt{\frac{log(p)}{n}}$,
	\begin{equation}
			\parallel \hat{\beta}^{Lasso}-\beta \parallel_{1} \overset{\mathbb{P}}{\underset{n\rightarrow +\infty}{\longrightarrow}}0\ ;\quad \parallel \hat{\beta}^{Lasso}-\beta \parallel_{2} \overset{\mathbb{P}}{\underset{n\rightarrow +\infty}{\longrightarrow}}0
			\label{f39}
	\end{equation}
\end{corollaire}
\textbf{Proof.} see \textit{Knight and Fu (2000)}.

Knowing that Lasso is widely use for model selection, it is necessary to assess how well the sparse model given by Lasso relates to the true model.We make this assessment by investigating Lasso's model consistency (under linear model); That is, for $S=\big\{ j, \beta_{j} \neq 0\big\}$ being the true active set, we look for a Lasso procedure delivering an estimator $\hat{S}=\big\{ j, \hat{\beta}^{Lasso}_{j} \neq 0\big\}$ of $S$ such that $\hat{S}=S$ with large probability.

Since using Lasso estimate involves choosing the appropriate amount of regularization, to study the model selection consistency of the Lasso, we consider two problems: whether there exists a deterministic amount of regularization that gives consistent selection, or for each random realization whether there exists a correct amount of regularization that selects the true model.The so-called \textbf{"irrepresentable condition"} thoroughly interpreted by \textit{Zhao and Yu (2006) \cite {nref15}} is almost necessary and sufficient for both types of consistency.

An estimate which is consistent in term of parameter estimation does not necessarily consistently select the correct model (or even attempt to do so) where the reverse is also true.The former requires $ \hat{\beta}^{Lasso}-\beta \overset{\mathbb{P}}{\underset{n\rightarrow +\infty}{\longrightarrow}}0\ $ while the latter requires $\mathbb{P}\big(\{\hat{S}=S\}\big)\overset{\mathbb{P}}{\underset{n\rightarrow +\infty}{\longrightarrow}}1$. We desire our estimate to have both consistencies. However, to separate the selection aspect of consistency from the parameter estimation aspect.We make the following definitions about \textit{"sign\footnote{$Sign(.)$ maps positive entry to 1 ,negative to -1 and 0 to 0 } consistency"} that does not assume the estimates to be estimation consistent.
\begin{definition}
	An estimate $\hat{\beta}_{n}$ is equal in sign with the true model $\beta$ if and only if,
	$$Sign(\hat{\beta}_{n})=Sign(\beta)$$
\end{definition}
\begin{definition}
	Lasso is strongly sign consistent if there exists $\lambda_{n}=f(n)$, that is , a function  independent of $Y$ and $X$ such that: 
	$$\lim_{n \rightarrow \infty}\mathbb{P}\big(\{ Sign(\hat{\beta}^{Lasso})=Sign(\beta)\}\big)=1,\ (\ast )$$
\end{definition}
\begin{definition}
	Lasso is general sign consistentcy if 
	$$\mathbb{P}\big(\{\exists\lambda \geq 0,Sign(\hat{\beta}^{Lasso})=Sign(\beta) \}\big)=1,\ (\ast \ast)$$
\end{definition}

\begin{remarque}
	\begin{itemize}
		\item Strong sign consistency implies one can use a preselected $\lambda$ to achieve consistent model selection via Lasso.
		\item General sign consistency means for a random realization there exists a correct amount of regularization that select true model.
		\item $(\ast) \Rightarrow (\ast \ast)$
	\end{itemize}
\end{remarque}
\begin{definition}[Irrepresentable Condition]
	We say that , Irrepresentable condition is met for the set $S$ if there exists a constant $\theta \in [0,1[$ such that ,
	\begin{equation}
		\parallel S_{XX}(S^{c},S)S_{XX}(S,S)^{-1}sign\big(\beta_{S}\big)\parallel_{\infty}\leq \theta.
		\label{f40}.
	\end{equation}
\end{definition}

\begin{theoreme}[Variables selection consistency]
	The irrepresentable condition \eqref{f40} for the active set $S$ is a sufficient and essentially necessary condition for Lasso to select only variables in active set $S$; that is to achieve sign consistency.
\end{theoreme}
\textbf{Proof.} refer to \textit{Zao and Yu (2006) \cite{nref15}} or \textit{Meinshausen and Buhlmann (2010)} for more details.

\begin{remarque}
The irrepresentable condition , as given in \eqref{f40} depends on the Gram matrix $\frac{X^{t}X}{n}$ but also on the signs of the true unknown parameter $\beta$, whereas the compatibility condition \eqref{f34} only depends on $\Sigma_{X}$.
\end{remarque}
\begin{proposition}
	The irrepresentable condition implies the compatibility condition.
\end{proposition}
\textbf{Proof.} see [\textit{\cite{nref14}, chap7, P.195}].

Sign consistency is stronger than the usual selection consistency  which only requires the zero to be matched , but not signs. It is needed for proving the necessity of the \textit{irrepresentable condition} to avoid dealing with situations where a model is estimated with matching zeros but reversed sign.
	
	\subsection{Dantzig Selector (DS)}
	The Lasso is not the only $L_{1}-penalization$ possible. from the score equation ,the Dantzig Selector by \textit{Candes and Tao \cite{nref12}} also belongs to the class of regularisation methods in regression.It can be formulated as the Lasso but instead of controlling the squared error loss, it controls the correlation of residuals with $X$.Specifically, the Dantzig selector estimator is defined to be the solution of the minimization problem:
	\begin{equation}
		\underset{\beta\in \mathbb{R}^{p}}{min} \big\{\parallel \beta\parallel_{1}\big\}\ subject\ to\ \parallel X^{t}\big(y-X\beta\big) \parallel_{\infty}:=\underset{1\leq i \geq p}{\sup}|\big(X^{t}r\big)_{i}|\leq \lambda_{p}.\sigma,
		\label{f41}
	\end{equation}
 for some $\lambda_{p}>0,\ where\ r=y-X\beta\ $ is the residual vector.
 
 The intuition behind the program \eqref{f41} is that, we seek and estimator $ \hat{\beta}$ with minimum complexity (as measured by the $L_{1}-norm$ ) among all objects that are consistent with data.
 \begin{remarque}
 	The constraint on the residual vector imposes that for each $j\in \{1,\dots, p\}$ , $|\big(X^{t}r\big)_{j}|\leq \lambda_{p}.\sigma$, which guarantees that the residuals are within the noise level.
 \end{remarque}

The Dantzig selector and Lasso are closely related.Connections between the Dantzig Selector and the Lasso have been discussed in \textit{Jame et al. (2008)} where it is shown that under some general conditions, the Dantzig Selector and the Lasso produce the same solution path.

Both models share the feature of setting some of parameters to zero i.e they perform variable selection.
 
 \begin{remarque}
 	Though under some general conditions , the Lasso and Dantzig may produce the same solution path, they differ conceptually in that the Dantzig stems directly from an estimating equation, whereas the Lasso stems from a likelihood or an objective function.
 \end{remarque}
The theoretical results (estimation accuracy and model selection consistency) for the Dantzig selector estimator are provide with detailed supporting proof in [\textit{\cite{nref12}, theorem 1.1; theorem 1.2}]

	\subsection{Elastic-Net Regression}
	We ended the section on Lasso regression by saying that it works best when your model contains a lot of useless variables. We also said that Ridge regression works best when most of the variables in your model are useful.
	
	\begin{remarque}
		When we know about all of the parameters in our model, it's easy to choose if we want to use Lasso or Ridge regression; but what do we do when we are in high dimension setting where the model include tons more variables, far too many to know everything about ?.
	\end{remarque}
	
	When you have million of parameters, then you will almost certainly need to use some sort of regularization to estimate them.However, the variables in those models might be useful or useless; we don't not in advance.So how do we choose if we should use Lasso or Ridge regression?.
	
	The good news is that we don't have to choose, instead, we use \emph{Elastic-Net} regression. Just like Lasso and Ridge regression, Elastic-Net regression starts with least squares, then it combines the Lasso regression penalty $\lambda_{1}\parallel \beta \parallel_{1}$ with the Ridge regression penalty $\lambda_{2}\parallel \beta \parallel^{2}_{2}$. The Lagrangian problem become 
	$$minimize_{\beta}\{\parallel y-X\beta \parallel^{2} +\lambda_{1}\parallel \beta \parallel_{1}+\lambda_{2}\parallel \beta \parallel^{2}\}.$$
	Altogether, Elastic-Net regression combines the strengths of Lasso and Ridge regression. Note that the Lasso and Ridge regression penalty get their own $\lambda's$ ; $\lambda_{1}$ for Lasso and $\lambda_{2}$ for Ridge.But more often, the problem is writing as
	$$minimize_{\beta}\{\parallel y-X\beta \parallel^{2} +\lambda(\alpha \parallel \beta \parallel_{1}+(1-\alpha)\parallel \beta \parallel^{2})\},\quad for\ \alpha \in [0,1]\; and\ \lambda \geq 0$$, say
	\begin{equation}
		\hat{\beta}^{E}(\lambda,\alpha)=argmin_{\beta}\{\parallel y-X\beta \parallel^{2} +\lambda(\alpha \parallel \beta \parallel_{1}+(1-\alpha)\parallel \beta \parallel^{2})\}.
	\end{equation}
	We still have the regularization parameter $\lambda$ , but we only have one regularization parameter common to both terms, we also have a parameter $\alpha$  which will control the mix between  $L_{1}$ and $L_{2}$ regularization. 
	\begin{remarque}
		We notice that:
		\begin{itemize}
			\item $\hat{\beta}^{E}(\lambda,1)=\hat{\beta}^{Lasso}(\lambda)$
			\item $\hat{\beta}^{E}(\lambda,0)=\hat{\beta}^{R}(\lambda)$
			\item $\hat{\beta}^{E}(0,\alpha)=\hat{\beta}^{LS}$
			\item and when $\alpha \notin\{0,1\} $ and $\lambda \neq 0$ , then we get the hybrid of Ridge and Lasso estimation.
		\end{itemize}
	\end{remarque}
	
	\begin{center}
		\subsubsection{Why Elastic-Net ?}
	\end{center}
	
	The hybrid Elastic-Net regression is especially good at dealing with situations when there are high correlations between parameters.This is because on it's own, Lasso regression tend to pick just one of the correlated terms and eliminates the others, whereas Ridge regression tends to shrink all of the parameters for the correlated variables together; By combining Lasso and Ridge regression, Elastic-Net regression groups and shrinks the parameters associated with the correlated variables and leaves them in equation or removes them all at once.
	\subsection{Selecting The Appropriate Tuning Parameter}
	Implementing Ridge and Lasso regression requires a method for selecting a value for the tuning parameter $\lambda$ in \eqref{f12} and \eqref{f18} or equivalently, the value of the constraint $s$ in \eqref{u1} and \eqref{u2}.
	
	One way to find a good value of $\lambda$ is to calculate the $MSE$ of \emph{prediction} ($MSE_{p}$) by some sort of \emph{Cross-Validation} for many different values of $\lambda$.
	
	\begin{description}
		\item[The principle is the folloing:] we choose a grid of $\lambda$ values, and compute the Cross-Validation error for each value of $\lambda$, as we will describe later.We then select the tuning parameter value for which the Cross-Validation error is smallest.Finally, the model is re-fit using all of the available observations and the selected value of the tuning parameter.
	\end{description}
	\begin{center}
		\subsubsection{Cross-validation to find the best value of $\lambda$}
	\end{center}
	There are various methods to select the "best" value for $\lambda$. One is to split the data into \textbf{K} chunks. We then use \textbf{K-1} of this as a training set, and the remaining 1 chunk as the test set.We can repeat this until we've rotated through all \textbf{K} chunks, giving us a good estimate of how well each of the lambda values work in our data.This is called \emph{cross-validation}, and doing this repeated \emph{test/train} split gives us a better estimate of how generalisable our model is.
	
	We can use this new idea to choose a lambda value, by finding the lambda that minimises the error across each of the test and training splits.
	
	Let $(X_{k},y_{k})$ denote the subset of $X$ and $y$ for the $k-th$ fold, with $k=1,\dots, K$.The optimal $\lambda$ is obtained by minimizing the total \textit{Cross-validation} error: 
	\begin{equation}
	\hat{\lambda}=\underset{\lambda}{argmin}\bigg\{\underbrace{\frac{1}{K} \sum_{k=1}^{K}\frac{1}{n_{k}}\parallel y_{k}-X_{k}\hat{\beta}_{k}(\lambda)\parallel_{2}^{2}}_{CV_{(K)}} \bigg\},
	\label{f42}
	\end{equation}
Where $n_{k}$ is the number of observations in the $k-th$ fold and $\hat{\beta}_{k}(\lambda)$ is the Lasso (resp. Ridge) estimates based on $(X_{-k},y_{-k})$ ( the data after removing the $k-th$ fold) and the tuning parameter $\lambda$.
	
	A particular case of this method is the so called \textbf{Leave-one-out cross-validation (LOOCV)} where \textbf{K}=n. In this case, the Cross-validated estimate $\lambda$ is given by:
	\begin{equation}
			\hat{\lambda}_{n}=\underset{\lambda}{argmin}\bigg\{\underbrace{\frac{1}{n} \sum_{i=1}^{n} \big(y_{i}-X_{i}^{t}\hat{\beta}_{i}(\lambda)\big)^{2}}_{CV_{(n)} } \bigg\}
			\label{f43}
	\end{equation}

	\section{Numerical Implementation}
	We present here two illustrative numerical applications. The first one is based on simulated data and the last one on real data.The purpose of the numerical experiment is to show the behaviour and to investigate if there was an difference in predictive power between the previous three regularization methods; ridge, Lasso and Elastic-net regression when they were applied on high-dimensional data.The statistical analysis was implement using \textbf{R} statistical software \cite{nref27}.
	\subsection{Simulated Data}
	For the simulation study, we use generalized linear model (GLMs) for penalized logistic regression.The "\textit{glmnet}" \cite{nref28} package for \textbf{R} fits a GLM via penalized maximum likelihood. We will not provide a theory about GLMs in this study ; for specific information regarding GLMs we refer to \cite{nref29} .The measures that are used to assess how good a logistic regression model is for prediction are : \textit{misclassification error rate }(ME) which denotes the fraction of incorrect classifications over all observations and the \textit{Area Under Curve} (AUC) which is a measure of discrimination tanking values between 0 and 1 (visit \cite{nref29} for more details).The simulation study was inspired by the paper by \textit{Krona } \cite{nref30}. However, adjustments were made to the simulated datasets.
	\paragraph{Process description :}
	The simulated data consisted of four independent high-dimensional datasets.Each dataset was divided into a training and a test set.The three methods were used to fit a corresponding model to each of the training sets.The fitted models were used to make predictions for each of the corresponding test sets.Finally, we computed the AUC , the ME and extracted the number of non-zero $\hat{\beta}$-coefficients.The procedure was repeated 100 times per example.
	\paragraph{Simulation design :}
	We simulated p=1000 predictor and n=200 observation such that $p>>n$ and the data qualified as high-dimensional. All predictor variables $X$ were continuous multivariate normal distributed except for the binary response variable $Y$. A multiple group of predictors with varying strength of correlation were simulated for each data set. The predictors were generated by sampling from a multivariate normal distribution with the following probability density function :
	$$ f_{X}(x)=\frac{1}{(2\pi)^{\frac{n}{2}} \sqrt{\det\big(\Sigma\big)}}\exp\bigg\{-\frac{1}{2}(x-\mu)^{t}\Sigma^{-1}(x-\mu)\bigg\}$$  
	were $\mu$ is the mean vector and $\Sigma=(\rho_{ij})_{i,j}$  is the covariance matrix.For all $x$, we set $\ \mu=0\ $ and $\ Var[x]=1\ $.Thus, $\Sigma\ $ equal the correlation matrix of X. Each predictor variable was assigned a predetermined $\ \beta-value\ $.The response variable were simulated by running the simulated data through the inverse logit function ( see \cite{nref29}) ,
	$$\pi(x)=\frac{1}{1+e^{-X^{t}\beta}}.$$
	Given the threshold $\pi_{0}=0.5$, the observed value was categorized into one of the two classes $\ Y=1\ $ if $\ \pi(x)>0.5\ $ and $\ Y=0\ $ if $\ \pi(x)\leq0.5$ .Consequently, we obtained a vector Y and a matrix X consisting of 200 observations of the binary response variable and the predictor variables respectively.
	\paragraph{Details information about the four examples :}
	\begin{description}
		\item[Example 1 :] we set the pairwise correlation between $\ X_{i}\ and\ X_{j}\ $ predictors to $\ \rho_{ij}=0.5^{|i-j|}\ $. We assigned the first 122 $\beta$-coefficients a specified vector that consisted of random values within [2,5].The remaining coefficients were set to 0.
		\item[Example 2 :] we set $\ \rho_{ij}=0.5^{|i-j|}\ $. we set all coefficients to be $\beta=0.8$.
		\item[Example 3 :] we set $\ \rho_{ij}=0.9^{|i-j|}\ $. The coefficients were split in 8 groups, where the coefficients were set to pairwise be 0 and 2, $\beta=(\underbrace{2,2,\dots,2}_{125},\underbrace{0,0,\dots,0}_{125},\underbrace{2,2,\dots,2}_{125},\dots)^{t}$.
		\item[Example 4] The pairwise correlation between the first 500 predictors $\ X_{i}\ and\ X_{j}\ $ ( $\ 1\leq i,j \leq 500\ $) were set to $\ \rho_{ij}=0.5^{|i-j|}\ $ and the pairwise correlation for the remaining predictors were set to 0.We set the first 500 coefficients to $ \beta=3 $ and the remaining coefficients to 0, $\beta=(\underbrace{3,3,\dots,3}_{500},\underbrace{0,0,\dots,0}_{500})^{t}$.
	\end{description}
\paragraph{Results :}
	\begin{table}[h]
	\begin{center}
		\begin{tabular}{|c|c|c|c|c|c|c|}
			\hline
			models & \multicolumn{3}{c|}{\textbf{Example 1}}&\multicolumn{3}{c|}{\textbf{Example 2}}\\
			\hline
			&AUC&ME&Nb. of $\hat{\beta}\neq 0$& AUC & ME & Nb. of $\hat{\beta}\neq 0$ \\
			\hline
			Ridge& 0.76 (0.042) & 0.32 (0.053)  & 1000   & 0.76 (0.050) & 0.31 (0.052)  &1000  \\
			
			Lasso&0.65 (0.106)  & 0.41 (0.094)  & 29  & 0.55 (0.058) & 0.46 (0.056)  & 20\\
			
			Elastic Net& 0.75 (0.062) & 0.37 (0.074)  & 316  & 0.70 (0.076) & 0.37 (0.068)  &329 \\
			\hline
			& \multicolumn{5}{c }{ }&  \\
			\hline
					 & \multicolumn{3}{c|}{\textbf{Example 3}}&\multicolumn{3}{c|}{\textbf{Example 4}}\\
						\hline
						models&AUC&ME&Nb. of $\hat{\beta}\neq 0$ & AUC & ME & Nb. of $\hat{\beta}\neq 0$ \\
						\hline
						Ridge& 0.92 (0.028) &0.16 (0.041)   &1000   & 0.76 (0.047) & 0.31 (0.041)  & 1000 \\
						
						Lasso&0.84 (0.037)  & 0.24 (0.048)  & 54  &0.58 (0.073)  & 0.46 (0.070)  & 21\\
						
						Elastic Net&0.90 (0.033)  &  0.17 (0.045) & 415  & 0.70 (0.059)  &0.36 (0.054)   & 361 \\
						\hline
		\end{tabular}
	\end{center}
	\caption[Simulation results]{Simulation results.\textit{The table reports the AUC, ME-values and number of non-zero $ \hat{\beta}-coefficients\ $.The simulation was repeated 100 times for each example and all results are reported as median values and (standard deviation sd.)}}
	\label{tab4}
\end{table}
The simulation of Example 1-4 was repeated 100 times: for every simulation, we calculated AUC, ME and their standard deviations (sd).In addition, the average number of selected variables by Lasso and  the Elastic net was calculated.The results are summarized in table \ref{tab4}.

In example 1, a small subset of predictors were assigned non-zero $\ \beta$-coefficients.On average, the Lasso and the elastic net  selected 28 and 316 variables respectively. We  see that the Ridge regression has the highest AUC and the lowest ME.

In example 2, the predictors were assigned coefficients $\ \beta$=0.8 with relatively high correlation amount predictors.As demonstrate in table \ref{tab4}, ridge regression improve over other methods considering AUC and ME. As mention in \textit{subsection 1.3.1}, ridge regression tend to perform well under the circumstances in example 2.Moreover, the average number of coefficients for Lasso and elastic net was 20 and 328 respectively.In this setting, the elastic-net identify a larger number of coefficients that were correlated and non-zero.The Lasso, on the other hand results in a sparse final model but identify less of the non-zero coefficients.instead, the chosen model resulted in a high ME ( table \ref{tab4}).

In example 3, the predictors were divided into 8 groups and pairwise assigned coefficients of 0 and 2.We see that ridge regression outperform the Lasso and elastic net in view of the AUC.Since the elastic net and ridge regression perform considerably similar, they seem to perform equally as good in this setting.As discussed in earlier, ( \textbf{subsection 1.3.2}) , ridge regression included all predictors in the final model and resulted in a less interpretable model.However, the elastic net identified on average 415 non-zero coefficients.supposedly, the elastic net adopted the grouping effect and correctly identified almost all non-zero coefficients simultaneously as it achieved high prediction accuracy.

In example 4, the predictors were divided into two groups of equal size that were assigned with $\ \beta$=3 and $\ \beta$=0 respectively.The first 500 were correlated while the remaining 500 predictors were uncorrelated.As seen in table \ref{tab4} , ridge regression achieved the highest AUC  while elastic net succeeded to identified approximately all non-zero coefficients as a result of the grouping effect.
\paragraph{Summary}
The results show that the three methods perform well in the sense that AUC$\geq$0.5 in examples 1-4.We observe that despite the fact that ridge regression tend to spread the coefficients shrinkage over a large number of coefficients, it achieve high predictive power throughout example 1-4.especially, the results in example 3 demonstrated the capacity of ridge regression.We identify that when the number of predictor are very large and a larger fraction of them most be included in the model, ridge regression dominates the Lasso and the elastic net.Consequently, it confirm that ridge regression is satisfactory method for prediction on correlated datasets.The results from example 2 determine that the Lasso is outperformed by the elastic net.Furthermore we observed that the elastic net benefits from the ability to put a larger weight to the quadratic penalty, while it simultaneously shrinks some coefficients to zero by the absolute penalty.

Moreover, we observe that ridge regression and the elastic net generally improve over the Lasso.We can see that elastic net approximately identified all-non zero coefficients in the simulations.In example 4, elastic-net performed grouped selection and showed to be a better variable selection method than Lasso.Even though ridge regression did not incorporate variable selection, it achieved high prediction accuracy through-out example 1-4.Therefore, we observe that if the interpretability is not fundamental, ridge regression manage to accomplish high predictive power.Ultimately, the elastic net has the advantage of incorporating variable selection.Consequently , its final model is more interpretable than that of ridge regression.\\
\textbf{Note: full R code are provided in Appendix \ref{a1}}
\subsection{Real data example}
\paragraph{Data description:}
For real data example, we will be working with \textit{"human DNA methylation data"} from \textit{"flow-sorted blood samples"} \cite{nref33}. DNA methylation assays measure for each of many sites in the genome, the proportion of DNA that carries a methyl mark ( a chemical modification that does not alter the DNA sequence).In this case, the methylation data come in the form of normalised methylation levels (M-values) where negative values correspond to unmethylated DNA and positive values correspond to methylated DNA.Along with this, we have a number of sample phenotypes (e.g BMI, Sex, Age in year).This methylation object is a \textit{"GenomicRatioset"}, a \textit{Bioconductor}\cite{nref31} data object derived from the \textit{"SummarizedExperiment"}\cite{nref32}.These \textit{"SummarizedExperiment"} objects contain  \textit{"assays"}, in this case normalised methylation levels, and optional sample level \textit{"ColData"} and feature-level \textit{"metadata"}.These objects are very convenient to contain all of the information about a dataset in a high-throughput context.For more details on these objects, one could consult the \textit{vignettes on Bioconductor}....url....

After reading in the data we can see in the provided R output that this object has $dim() \ of\ 5000\times37\ $, meaning it has 5000 features and 37 samples ( observations).to extract the matrix of methylation M-values, we use "\textit{assay()" function}.Note that in the matrix of methylation data, samples or observations are stored as rows.

In this episode, we will focus on the association between \textbf{Age} and \textbf{methylation}.
\paragraph{Experiment steps :} Let's denote by X the methylation matrix,
\begin{description}
\item[1) Singularity:] we investigete singularity of the matrix $\ X^{t}X\ $ and check out what happen if we try to fit linear model to the data.
\item[2) Ordinary least square versus Ridge regression :] here, we work with a set of feature known to be associated with \textbf{Age} from a paper by \textit{Horvath et al.}.Horvath et al. used methylation markers alone to predict the biological \textbf{Age} of an individual.
\begin{itemize}
	\item we extra the first 20 features of the features identified by Horvath, investigate correlations and we split the methylation data matrix and the age vector into training an test sets.
	\item we fit both linear regression and ridge regression on the training data matrix and training \textbf{Age} vector using the previous features and record the MSE between our predictions and the true \textbf{Age}s for the test data.
\end{itemize}
\item[3) Apply egularization methods :] we perform the Lasso, Ridge and Elastic-net on the whole DNA methylation data using cross validation to select the tuning parameter, examine the coefficients paths for each method and load Horvath signature to compare features selected by Lasso and the elastic-net methods.
\end{description}
\paragraph{Results.}
\begin{itemize}
	\item[1)] We can see  that we are able to get some effect size estimates, but they seem vert high.The \textit{"Summary"} also says that we were unable to estimate effects sizes for 4964 features because of  singularities.What this mean is that R couldn't find a way to perform the calculations necessary due to the fact that we have more features than observations.
	\item[2)] Preditors are correlated each other.Since we split the data into test and training data, we can see that ridge regression gives us a better prediction on unseen data despite being worse on train data : $MSE_{lm}=45.14 \ \geq\ MSE_{ridge}=25.30 $. \textit{see also comments of Figures 1.4 and 1.5}
	
			\begin{figure}[b]
				\begin{minipage}[c]{0.49\textwidth}
			\centering
				\includegraphics[scale=0.4]{fig1.png}
			
			\caption[plot showing how estimated coefficients for each methylated site change]{\textit{plot showing how estimated coefficients for each methylated site change as we increase the penalty $\ \lambda$. We can see that initially, some parameter estimates are really large, and these tend to shrink fairly rapidly.} }
		\end{minipage}
	\hfill
	\begin{minipage}[c]{0.49\textwidth}
		\centering
		\includegraphics[scale=0.4]{fig2.png}
			\caption[Predicted Ages for each methods against the true Ages]{\textit{Predicted Ages for each methods against the true Ages. The ridge ones are much less spread out with far fewer extreme predictions.}}
	\end{minipage}
		\end{figure}
	\item[3)] Comparing the feature selected by Lasso ( 41 features) and the elastic net ( 60 features ) with Horvath signature, we can see that we selected some of the same feature ( 8 features for Lasso and 11 features for elastic net).\textit{see also the comments of the remaining 6 figures.}
	\item \textbf{Full R codes are provided in Appendix \ref{a2}}
	
	\begin{figure}
		\begin{minipage}[c]{0.49\textwidth}
			\centering
			\includegraphics[scale=0.4]{fig3.png}
			
			\caption{\textit{Cross-validation performance for Lasso.}}
		\end{minipage}
		\hfill
		\begin{minipage}[c]{0.49\textwidth}
			\centering
			\includegraphics[scale=0.4]{fig4.png}
			\caption{\textit{Cross-validation performance for Ridge.}}
		\end{minipage}
	\end{figure}
	\begin{figure}
	\begin{minipage}[c]{0.49\textwidth}
		\centering
		\includegraphics[scale=0.4]{fig5.png}
		
		\caption[Ridge versus Lasso coefficients paths]{\textit{The paths tend to go exactly to zero much more when sparsity increases when we use lasso model.In ridge case, the paths tends toward zero but less commonly reach exactly zero.}}
	\end{minipage}
	\hfill
	\begin{minipage}[c]{0.49\textwidth}
		\centering
		\includegraphics[scale=0.4]{fig6.png}
		\caption[Coefficients paths elastic net]{\textit{Coefficients paths elastic net.We can see that coefficients tend to go exactly to zero, but the paths are a bit less extreme than with pure Lasso; similar to ridge.}}
	\end{minipage}
\end{figure}
	\begin{figure}
	\begin{minipage}[c]{0.49\textwidth}
		\centering
		\includegraphics[scale=0.4]{fig7.png}
		
		\caption[Cross-validation for elastic net regression]{\textit{Cross-validation to find the optimal pair of ($\alpha,\lambda\ $) for elastic net (mixing percentage).}}
	\end{minipage}
	\hfill
	\begin{minipage}[c]{0.49\textwidth}
		\centering
		\includegraphics[scale=0.4]{fig8.png}
		\caption[Lasso coefficients against elastic net coefficients]{\textit{Lasso coefficients against elastic net coefficients.We can see that the coefficients from these two methods are broadly similar, but the elastic net coefficients are a bit more conservative.}}
	\end{minipage}
\end{figure}
\end{itemize}

	\chapter{Measurement Error In Regression theory }
	\section{Introduction}
	\subsection{Definition an motivating examples}
	This section is about measurement error in statistical analyses.In some sense, all statistical problems involve measurement error.
	
	Measurement error occur whenever we cannot exactly observe one or more of the variables that enter into a model of interest.There are many reason such errors occur, the most common ones being "sampling error and instrument error".Where any notation is used here, the true value is denoted "$X$" and the variable observed in place of "$X$" by "$W$" (error-prone measurement). When the true and observed values are both categorical, then measurement error is more specifically referred to as \textbf{misclassification}.
	
	Measurement error occur in nearly every discipline; Here is a collection of examples in bio-medical field:
	\begin{description}
		\item[Genomic:] In recent decades, genetic and epigenetic studies have become increasingly more important in medical research, but the process of sequencing DNA typically involves some errors.
		\item[Disease statut:] I n epidemiology , the outcome variable is often presence or absence of a disease such as breast cancer, hepatitis, AIDS\dots .This is often assessed through an imperfect diagnostic procedure such as an imaging technique or a blood text which can lead to either false positives or false negatives (misclassification).
	\end{description}
\subsection{Objective and some terminology}
\label{spc}

\begin{itemize}
	\item how to model measurement error ?
	\item what the effects of ignoring it are ?
	\item How, if at all can we correct for measurement error ?
\end{itemize}
These are three general objective in measurement error problem we will try to address in this parts of our work.

There are typically three main ingredients in  measurement error problem:
\begin{description}
\item[1) A model for true values:] This can be essentially any statistical model.
\item[2) A measurement error model:] That is specification of the relationship between the true and observed values.
\item[3) Extra information:] Data or assumption that may be needed to correct for measurement error which may not be always available.This extra information is among others:\textit{validation data} in which both true an mismeasured values are obtained on a set of unit; \textit{Replicate values}; \textit{Knowledge about some of the measurement error parameters or fonctions of them};\dots .
\end{description}

In the sequel, we will focus on the classical additive non-differential measurement error model in the structural case with the assumption of homoskedasticity which refers to  the case where the variance of "$W$" given $X=x$" is constant.
\section{The Model Description}  
One of the fundamental assumption in the linear regression analysis is that all observations are correctly observed.When this assumption is violated the measurement error creep into the data.The the usual statistical tools tend to loose their validity( \textit{see \cite{nref2} and \cite{nref3} for more details.}). And important issue in the area of measurement errors is to find the consistent estimators of the parameters which can be accomplished by using some additional information from outside the sample.

In section \ref{f44} and \ref{f45} we consider a linear regression model defined in \eqref{lrm} with additive error,

\begin{equation}
	y=X\beta + \epsilon\ ,\ W=X+U
	\label{f46}
\end{equation}
$X_{i}=(X_{i1},\dots,X_{ip})^{t},\quad W_{i}=(W_{i1},\dots,W_{ip})^{t},\quad U_{i}=(U_{i1},\dots,U_{ip})^{t};$
$$ X=
\begin{bmatrix}
	X_{1}^{t} \\
	\vdots \\
	X_{n}^{t}
\end{bmatrix}\ n\times p\ matrix;\  U=
\begin{bmatrix}
U_{1}^{t} \\
\vdots \\
U_{n}^{t}
\end{bmatrix}\ n\times p\ matrix;\  W=
\begin{bmatrix}
W_{1}^{t} \\
\vdots \\
W_{n}^{t}
\end{bmatrix}\ n\times p\ matrix$$
For the sake of notation simplicity, we assume that $\beta_{0}=0$. The true covariate $X$ are not observed, and instead we have noisy measurements $W=X+U$ where $U$ is and $n\times p$ random noise matrix with covariance matrix $\Sigma_{U}$.If the $k-th$ variable has been measured correctly, the corresponding column of U will be set equal to zero, as will the variance of the measurement error of the $k-th$ variables, $\Sigma_{U(k,k)}=0$.

\textbf{Assumption}
\begin{itemize}
	\item the matrix of measurement error $U \in \mathbb{R}^{n\times p}$ is assumed to have normally distributed rows , with mean zero and covariance $\Sigma_{U}$. 
	\item furthermore, assume that $\epsilon$ and $U$ are independent and  $\Sigma_{U}$ is a $p\times p$ matrix of Known values with non-negative diagonal elements.
\end{itemize}
\begin{remarque}
	It follow from the structural model
	\begin{equation}
		y_{i}=\beta^{t}X_{i}+\epsilon_{i},\ W_{i}=X_{i}+U_{i}
		\label{f47}
	\end{equation} that the vector $\big(y_{i},W_{i}^{t}\big)^{t}$ follows a p+1-variate normal distribution with mean $\mu= \big(\beta^{t}\mu_{X},\mu_{X}^{t}\big)^{t}$ and the covariance matrix,
\begin{equation}
	\Gamma=\begin{bmatrix}
		\sigma^{2}_{Y}& \Sigma_{YW} \\
		\Sigma_{WY} &\Sigma_{W} \\
	\end{bmatrix} =\begin{bmatrix}
		\sigma^{2}+\beta^{t}\Sigma_{X}\beta& \beta^{t}\Sigma_{X} \\
		\Sigma_{X}\beta &\Sigma_{X}+ \Sigma_{U}\\
	\end{bmatrix}.
\label{f48}
\end{equation}

(easy to verify ) 
\end{remarque}
This lead to:

\begin{equation}
	y_{i}|W_{i}=w_{i}=\gamma^{t}w_{i}+\delta_{i}
	\label{f49}
\end{equation}
where$\delta=(\delta_{1},\dots,\delta_{n})^{t}$ are i.i.d normally with mean zero and variance $\sigma^{2}_{delta}$.
\begin{theoreme}
	Under the given assumptions, $\gamma$ and $\sigma_{\delta}^{2}$ are given by,
	\begin{equation}
				\gamma=\big(\Sigma_{W}\big)^{-1}\Sigma_{X}\beta=\big(\Sigma_{X}+\Sigma_{U}\big)^{-1}\Sigma_{X}\beta
				\label{gam}
	\end{equation}
	\begin{equation}
	\sigma_{\delta}^{2}= \sigma^{2}+\beta^{t}\Sigma_{X}\beta-\gamma^{t}\big(\Sigma_{X}+\Sigma_{U}\big)\gamma
\end{equation}
\end{theoreme}

\textbf{Proof.} mimicking what was done in the simple linear case in \textit{Gleser \cite{nref4}}.

Thus
\begin{equation}
	\beta=\mathcal{K}_{X}^{-1}\gamma.
	\label{f50}
\end{equation}
where $\mathcal{K}_{X}=\big(\Sigma_{X}+\Sigma_{U}\big)^{-1}\Sigma_{X}$ is a $p\times p$ matrix  referred to as the \textit{reliability matrix} ,\textit{see Gleser (1992) \cite{nref4}  and Aickin and Ritenbaugh (1992)} for example,discussion and illustrations of the role of reliability matrix.
\begin{center}
	\subsection{Estimated Coefficients and Behaviour of naive analyses}
\end{center}

	Statistical analysis that is carried out by ignoring the presence of the measurement error is called a naive approach.
	Without measurement error, we saw that the estimated coefficients and the unbiased estimator of $\sigma^{2}$ are given by $\hat{\beta}=(X^{t}X)^{-1}X^{t}y$ \eqref{f3}  and $\hat{\sigma}^{2}=\frac{1}{n-p}\sum_{i}(y-\hat{y}_{i})^{2},\ with\ \hat{y}_{i}=\hat{\beta}^{t}x_{i}$.
	\begin{proposition}
		The maximum likelihood estimators of $\gamma$ and $\sigma_{\delta}^{2}$ are just the naive least squares estimators,
		\begin{equation}
			\hat{\beta}_{naive}=\hat{\gamma}=(W^{t}W)^{-1}W^{t}y=S_{WW}^{-1}S_{Wy}\ ,\ \hat{\sigma}^{2}_{naive}=\hat{\sigma}_{delta}=\frac{1}{n-p}\sum_{i}(y-\hat{y}_{i})^{2},\ with\ \hat{y}_{i}=\hat{\beta}^{t}_{naive}w_{i}
			\label{f51}
		\end{equation}
	\end{proposition}
	where, $S_{WW}=\frac{W^{t}W}{n}$ is the unbiased estimator of $\Sigma_{W}$ and $S_{Wy}=\frac{W^{t}y}{n}$
	\begin{proposition}
		The exact bias expression for the naive estimators under the given assumptions are given by:
		\begin{equation}
			\mathbb{E}\big[\hat{\beta}_{naive}\big]=\gamma=\mathcal{K}_{X}\beta \ , \ \mathbb{E}\big[\hat{\sigma}_{naive}^{2}\big]=\sigma^{2}_{\delta }
			\label{f52}
		\end{equation}
	\begin{remarque}
		This result lead to an important conclusion: The measurement error in one of the variables may induce bias in the estimation of all coefficients including those measured without error.If more covariates are affected by measurement error, the resulting bias may become rather complex and the effect of measurement error may become difficult to describe.
	\end{remarque}
	\end{proposition}
\begin{center}
	\subsection{Correcting for Measurement Error in Multilinear regression}
\end{center}

With some exceptions (\textit{see \cite{nref1}, chap11 and 12}), correcting for measurement error requires informations or data as laid out in item \textbf{3)} section \ref{spc}.

Myriad approaches to carrying out corrections for measurement error have emerged,A number of which are described in \cite{nref1}.These include \textit{direct bias correction, moment based approach, likelihood based techniques,SIMEX and techniques based on modifying equations.} 

\begin{proposition}
	When  $\Sigma_{U}$ is known and $\mathcal{K}_{X}$ is unknown, then $\mathcal{K}_{X}$ is estimated consistently by replacing $\Sigma_{X}$ and $\Sigma_{W}$by their respective consistent estimators as:
	\begin{equation}
		\hat{\Sigma}_{X}=\hat{\Sigma}_{W}-\Sigma_{U}\ ,\ \hat{\Sigma}_{W}=S_{WW}=\frac{W^{t}W}{n};\quad and\ we\ have\ \hat{\mathcal{K}}_{X}=S_{WW}^{-1}\big(S_{WW}-\Sigma_{U}\big).
		\label{est}
	\end{equation}
\end{proposition}

\begin{corollaire}
The maximum likelihood estimates of $\beta$ and $\sigma^{2}$ are given by :
\begin{equation}
	\hat{\beta}=\hat{\mathcal{K}}_{X}^{-1}\hat{\gamma}=\big(S_{WW}-\Sigma_{U}\big)^{-1}S_{Wy},\ ,\ \hat{\sigma}^{2}=\hat{\sigma}_{\delta}^{2}-\hat{\beta}^{t}\Sigma_{U}\hat{\mathcal{K}}_{X}\hat{\beta}
	\label{f53}
\end{equation}
\end{corollaire}
$\hat{\beta}$ is and unbiased estimator and its covariance is given by:
	\begin{align*}
		Cov(\hat{\beta})=&Cov(\hat{\mathcal{K}}_{X}^{-1}\hat{\gamma})=\mathcal{K}_{X}^{-1}Cov(\hat{\gamma})\big(\mathcal{K}_{X}^{-1}\big)^{t}\\
		=& \mathcal{K}_{X}^{-1}\big(\underbrace{W^{t}W}_{n\Sigma_{W}}\big)^{-1}\sigma_{\delta}\big(\mathcal{K}_{X}^{-1}\big)^{t}=\sigma_{\delta}\big(\underbrace{n\Sigma_{X}\Sigma_{W}^{-1}\Sigma_{X}}_{C}\big)^{-1} =\sigma_{\delta}C^{-1}
		\label{f54}
	\end{align*}

When measurement error is present and $\Sigma_{U}$ is not known, it can be estimated through replicated measurements of $W$.
\begin{proposition}
	Suppose on unit $i$ there are $m_{i}>1$ replicated values $W_{i1},\dots,W_{im_{i}}$ of the error-prone measure of $x$ and $\overset{\_}{W}_{i.}=\sum_{k=1}^{m_{i}}\frac{W_{ik}}{m_{i}}$ their mean. replication allows us to estimate $\Sigma_{U}$ as:
	\begin{equation}
		\hat{\Sigma}_{U}=\frac{1}{n}\sum_{i=1}^{n}\frac{\sum_{k=1}^{m_{i}}\big(W_{ik}-\overset{\_}{W}_{i.}\big)\big(W_{ik}-\overset{\_}{W}_{i.}\big)^{t}}{m_{i}-1}
		\label{f55}
	\end{equation}
\end{proposition}
In that case; 
\begin{equation}
	\hat{\Sigma}_{X}=S_{WW}-\hat{\Sigma}_{U},\ \hat{\mathcal{K}}_{X}=S_{WW}^{-1}\big(S_{WW}-\hat{\Sigma}_{U}\big),\ and\ \hat{\beta}=\big(S_{WW}-\hat{\Sigma}_{U}\big)^{-1}S_{Wy}
\end{equation}
\begin{remarque}
	With sufficiently large measurement error, it is possible that $S_{WW}-\hat{\Sigma}_{U}$ can be negative.In that case, some adjustment must be made; \textit{see Block and Peterson (1975)}.
\end{remarque}

Our discussion of the linear model is intended only to set the stage for our main topic, \textbf{measurement error in high-dimensional context} and is far from complete;A vast literature exists on measurement error.There is a number of excellent books, starting with one by \textit{Fuller \cite{nref3}} who wrote the first influential book focusing on linear regression models, and on by \textit{Caroll et al. \cite{nref5}} who treated measurement error in a much broader application context.Another book that give wide treatment to the topic is by \textit{Buanaccorsi \cite{nref2}} who focuses on different topics from those in the aforementioned two books and places emphasis on more applied approach.

\chapter{Measurement Error in High-Dimensional Context :Behaviour and Correction Methods}
\section{ Ridge Regression Estimation Over Measurement Error Ridden Data.}
The standard assumption in the linear regression analysis is that explanatory variables are uncorrelated.When this assumption is violated, the explanatory variables are nearly dependent, which refers as \textbf{multicollinearity problem} (very common in high dimensional data ) and yields poor estimators of interest parameters as described in section \ref{mcp} item \textit{iii)}.In order to resolve this problem, several approaches have been considered among them, the "Ridge regression" introduced by \textit{Horel and Kennard \cite{nref8}} was discuss in section \ref{rre} and considers a shrinkage method to overcome the problem of multicollinearity for the estimation of regression parameters.

When the problem of multicollinearity is present in the measurement error ridden data , then and important issues is how to obtain the consistent estimators of regression coefficients.One simple idea is to use the ridge regression estimation over the error ridden data.An obvious question that crops up is what happen then?.

In this section, we attempt to answer such questions.
\subsection{Ridge Regression Estimator of $\beta$ and its Asymptotic Properties.}
Here we introduce the ridge regression estimators of $\beta$.For this, we first consider the conditional setup of the least squares method \ref{f46} with known \textit{reliability matrix} $\mathcal{K}_{X}$.Remember in this case that the corrected moment estimator or corrected score estimated of $\beta$  and $\gamma$ are respectively given by :
$$ \hat{\beta}^{^{LS}}_{_{ME}}=\mathcal{K}_{X}^{-1}\hat{\gamma}=\big(S_{WW}-\Sigma_{U}\big)^{-1}S_{Wy}\ \eqref{f53} , \ and\ \gamma=\mathcal{K}_{X}\beta\ \eqref{gam}$$
where "\textbf{$ME$}" stands for measurement error.
The suggested estimator of $\beta$ based on a shrinkage strategy is obtain by minimizing ,
\begin{equation}
\underset{\beta \in \mathbb{R}^{p}}{minimize} \bigg\{\parallel y- W\gamma \parallel^{2}_{2}\bigg\} \quad subject\ to\ \parallel \beta \parallel^{2}\leq s\ for\ some\ constant\ s
	\label{f56}
\end{equation}
the Lagrangian problem become
\begin{equation}
	\underset{\beta \in \mathbb{R}^{p}}{minimize} \bigg\{ \parallel y- W\mathcal{K}_{X}\beta \parallel^{2}_{2}+k\parallel \beta \parallel^{2}\bigg\}
	\label{f57}
\end{equation} 
\begin{proposition}
	The numerical solution of this problem corresponding to the ridge regression estimator of $\beta$ in measurement error model \ref{f46} is given by:
	\begin{equation}
		\hat{\beta}^{^{R}}_{_{ME}}=\bigg[\mathbb{I}_{p}+k\big(n\mathcal{K}_{X}^{t}S_{WW}\mathcal{K}_{X}\big)^{-1}\bigg]^{-1}\hat{\beta}^{^{LS}}_{_{ME}}.
		\label{f58}
	\end{equation}
\end{proposition}
\textbf{Proof.}  set $f(\beta)=\parallel y- W\mathcal{K}_{X}\beta \parallel^{2}_{2}+k\parallel \beta \parallel^{2}=\big( y- W\mathcal{K}_{X}\beta\big)^{t}\big( y- W\mathcal{K}_{X}\beta\big)+k\beta^{t}\beta$.Analogously to proof of proposition \textbf{1.3.2}, by taking partial derivative with respect to each component of $\beta$, we obtain $\frac{\partial f(\beta)}{\partial \beta}=2k\beta +2\mathcal{K}_{X}^{t}W^{t}\big(y-W\mathcal{K}_{X}\beta\big)$.

\begin{align*}
	\frac{\partial f(\beta)}{\partial \beta}=0\Rightarrow & k\beta +\mathcal{K}_{X}^{t}W^{t}W\mathcal{K}_{X}\beta=\mathcal{K}_{X}^{t}W^{t}y\ i.e\ \big[k\mathbb{I}_{p}+n\mathcal{K}_{X}^{t}S_{WW}\mathcal{K}_{X}\big]\beta=n\mathcal{K}_{X}^{t}S_{Wy}\\
	\Rightarrow & \beta=\bigg[\mathbb{I}_{p}+k\big(n\mathcal{K}_{X}^{t}S_{WW}\mathcal{K}_{X}\big)^{-1}\bigg]^{-1}\big(n\mathcal{K}_{X}^{t}S_{WW}\mathcal{K}_{X}\big)^{-1}n\mathcal{K}_{X}^{t}S_{Wy}\\
	&=\bigg[\mathbb{I}_{p}+k\big(n\mathcal{K}_{X}^{t}S_{WW}\mathcal{K}_{X}\big)^{-1}\bigg]^{-1}\mathcal{K}_{X}^{-1} \underbrace{S_{WW}^{-1}S_{Wy}}_{\hat{\gamma}}=\bigg[\mathbb{I}_{p}+k\big(n\mathcal{K}_{X}^{t}S_{WW}\mathcal{K}_{X}\big)^{-1}\bigg]^{-1} \underbrace{\mathcal{K}_{X}^{-1}\hat{\gamma}}_{\hat{\beta}^{^{LS}}_{_{ME}}}.
\end{align*}

\begin{corollaire}
	Substituting the consistent estimator of $\mathcal{K}_{X}$ given in \eqref{est} we get,
	\begin{equation}
		\hat{\beta}^{^{R}}_{_{ME}}=\bigg[\mathbb{I}_{p}+k\big(n\hat{\mathcal{K}}_{X}^{t}S_{WW}\hat{\mathcal{K}}_{X}\big)^{-1}\bigg]^{-1}\hat{\beta}^{^{LS}}_{_{ME}}.
		\label{f59}
	\end{equation}
\end{corollaire}
Denote the ridge factor of ridge estimation by: $Z^{^{ME}}_{n}=\big[\mathbb{I}_{p}+kC_{n}^{-1}\big]^{-1}$ with   $C_{n}=n\hat{\mathcal{K}}_{X}^{t}S_{WW}\hat{\mathcal{K}}_{X}$.
\begin{remarque}
	$Z^{^{ME}}_{n}$ is a consistent estimator of $Z^{^{ME}}=\big[\mathbb{I}_{p}+kC^{-1}\big]^{-1}$, with  $C=n\mathcal{K}_{X}^{t}\Sigma_{W}\mathcal{K}$ and $ 	\hat{\beta}^{^{R}}_{_{ME}}=Z^{^{ME}}_{n}\hat{\beta}^{^{LS}}_{_{ME}}$.
\end{remarque}

The expectation,covariance and sum of their square bias of $\hat{\beta}^{^{R}}_{_{ME}}$ are given by:
\begin{itemize}
	\item $\mathbb{E}\big[\hat{\beta}^{^{R}}_{_{ME}}\big]=\mathbb{E}\big[Z^{^{ME}}_{n}\hat{\beta}^{^{LS}}_{_{ME}}\big]=Z^{^{ME}}_{n}\mathbb{E}\big[\hat{\beta}^{^{LS}}_{_{ME}}\big]=Z^{^{ME}}_{n}\beta$ .
	\item $Cov\big(\hat{\beta}^{^{R}}_{_{ME}}\big)=Cov\big(Z^{^{ME}}_{n}\hat{\beta}^{^{LS}}_{_{ME}}\big)=Z^{^{ME}}_{n}Cov\big(\hat{\beta}^{^{LS}}_{_{ME}}\big)Z^{^{ME^{t}}}_{n}=\sigma_{\delta}^{2}Z^{^{ME}}_{n}C_{n}^{-1}\big(Z^{^{ME}}_{n}\big)^{t}$.
	\item $\sum_{j=1}^{p}\big(\mathbb{E}\big[\hat{\beta}^{^{R}}_{_{ME_{j}}}-\beta_{j}\big]\big)^{2}=\beta^{t}\big(\mathbb{I}_{p}-Z^{^{ME}}_{n}\big)^{t}\big(\mathbb{I}_{p}-Z^{^{ME}}_{n}\big)\beta=k^{2}\beta^{t}\big[C_{n}+k\mathbb{I}_{p}\big]^{-2}\beta$; the proof is similar to the one of \textbf{Proposition 1.3.3} replacing $X^{t}X\ $ by $\ C_{n}$.
\end{itemize}
\begin{corollaire}
	The mean square error of $\hat{\beta}^{^{R}}_{_{ME}}$ is given by:
	\begin{equation}
	MSE\big(\hat{\beta}^{^{R}}_{_{ME}},k\big)=k^{2}\beta^{t}\big[C_{n}+k\mathbb{I}_{p}\big]^{-2}\beta+\sigma_{\delta}^{2} tr\big(Z^{^{ME}}_{n}C_{n}^{-1}\big(Z^{^{ME}}_{n}\big)^{t}\big)
	\end{equation}
\end{corollaire}
\textbf{Proof.}
\begin{align*}
	MSE\big(\hat{\beta}^{^{R}}_{_{ME}},k\big)=&\sum_{j=1}^{p}\big(\mathbb{E}\big[\hat{\beta}^{^{R}}_{_{ME_{j}}}-\beta_{j}\big]\big)^{2}+\sum_{j=1}^{p}Var\big(\hat{\beta}^{^{R}}_{_{ME}}\big)\quad   by\ \textbf{Proposition 1.3.1} \\
	=& k^{2}\beta^{t}\big[C_{n}+k\mathbb{I}_{p}\big]^{-2}\beta+ \underbrace{tr\big(Cov(\hat{\beta}^{^{R}}_{_{ME}})\big)}_{\sigma_{\delta}^{2}tr\big(Z^{^{ME}}_{n}C_{n}^{-1}\big(Z^{^{ME}}_{n}\big)^{t} \big)}
\end{align*}
\begin{remarque}
\begin{itemize}
\item	When $n\rightarrow \infty\ $ then $\ C_{n}\rightarrow C\ , \ Z^{^{ME}}_{n}\rightarrow Z^{^ME}\  $ and  $$	MSE\big(\hat{\beta}^{^{R}}_{_{ME}},k\big)=k^{2}\beta^{t}\big[C+k\mathbb{I}_{p}\big]^{-2}\beta+\sigma_{\delta}^{2} tr\big(Z^{^{ME}}C^{-1}\big(Z^{^{ME}}\big)^{t}\big)$$
\item if $k=0\ $ then $\ Z^{^{ME}}=\mathbb{I}_{p}\ $ and $\ MSE\big(\hat{\beta}^{^{R}}_{_{ME}},k\big)=\sigma_{\delta}^{2}tr\big(C^{-1}\big)=MSE\big(\hat{\beta}^{^{LS}}_{_{ME}}\big)$.
\end{itemize}
\end{remarque}
 \subsubsection{Comparison of $\hat{\beta}^{^{R}}_{_{ME}}$ and $\beta^{^{LS}}_{_{ME}}$}
Let $\lambda_{max}=\lambda_{1}\geq \dots \geq \lambda_{p}=\lambda_{min}>0$ denote the eigenvalues of the positive definite matrix $\ C=n\mathcal{K}_{X}^{t}\Sigma_{W}\mathcal{K}$.we can find and orthogonal matrix $P$ such that,$\ P^{t}CP=D=diag(\lambda_{1},\dots,\lambda_{p})\ $ (see \textbf{Remark 1.2.1}); The corresponding eigenvalues of $Z^{^{ME}}\ $ and $\ \big[C+k\mathbb{I}_{p}\big]^{-1}\ $ are respectively, $\ \frac{\lambda_{j}}{\lambda_{j}+k}\ ,\ \frac{1}{\lambda_{j}+k}\ j=1,\dots,p$ so that.
	$$ k^{2}\beta^{t}\big[C_{n}+k\mathbb{I}_{p}\big]^{-2}\beta=k^{2}\beta^{t}P^{t}\big[D+k\mathbb{I}_{p}\big]^{-2}P\beta=k^{2}\sum_{j=1}^{p}\frac{\alpha_{j}^{2}}{(\lambda_{j}+k)^{2}}\ ,\ where\ \alpha=P\beta\ ,\ (p\times1 vector)$$
	and
	$$ \sigma_{\delta}^{2} tr\big(Z^{^{ME}}C^{-1}\big(Z^{^{ME}}\big)^{t}\big)=\sigma_{\delta}^{2} \sum_{j=1}^{p}\frac{\lambda_{j}}{(\lambda_{j}+k)^{2}}\ ,\ see\ \textbf{Remark 1.2.1 and (1.19)}$$.
	Now the MSE of $\hat{\beta}^{^{R}}_{_{ME}}$ may be written as:
	\begin{equation}
		MSE\big(\hat{\beta}^{^{R}}_{_{ME}},k\big)=k^{2}\sum_{j=1}^{p}\frac{\alpha_{j}^{2}}{(\lambda_{j}+k)^{2}}+\sigma_{\delta}^{2} \sum_{j=1}^{p}\frac{\lambda_{j}}{(\lambda_{j}+k)^{2}}=\psi_{b}(k)+\psi_{v}(k).
		\label{f60}
	\end{equation}
\begin{theoreme}
Under the given assumption,
\begin{description}
	\item[i)] The quadratic bias, $\psi_{b}(k)$ is a continuous, monotonically increasing function of k.
	\item[ii)] $\psi_{b}(k)$ approach $\beta^{t}\beta$ as an upper limit
	\item[iii)] As $k\rightarrow 0^{+} \ , \  \frac{d\psi_{b}(k)}{dk}\rightarrow 0$
	\item[iv)] The total variance $\psi_{v}(k)$ is a continuous monotonically decreasing function of $k$.
\end{description}
\end{theoreme}
\textbf{Proof.}
\begin{itemize}
	\item[(i)] clearly, $\psi_{0}(k)=0$, thus $\psi_{b}(k)$ is a continuous function of $k$ and $\frac{\psi_{b}(k)}{dk}=2k\sum_{j=1}^{p}\sum_{j=1}^{p}\frac{\lambda_{j}\alpha_{j}^{2}}{(\lambda_{j}+k)^{2}}>0\ $, is non negative hence the result.
	\item[(ii)] $\underset{k \rightarrow +\infty}{lim}\psi_{b}(k)=\underset{k \rightarrow +\infty}{lim}\sum_{j=1}^{p}\frac{\alpha_{j}^{2}}{(1+\frac{\lambda_{j}}{k})^{2}}=\sum_{j}\alpha_{j}^{2}=\alpha^{t}\alpha=\beta^{t}P^{t}P\beta=\beta^{t}\beta\ $.
	\item (iii) and (iv) are also easy to verify.
\end{itemize}
	\begin{theoreme}[ \textit{from \cite{nref6}}]
		There always exist a $k>0$ such that ,
		\begin{equation}
				MSE\big(\hat{\beta}^{^{R}}_{_{ME}},k\big)< MSE\big(\hat{\beta}^{^{LS}}_{_{ME}}\big)\ .
		\end{equation}
	\end{theoreme}
\textbf{Proof.} consider the derivative of $MSE\big(\hat{\beta}^{^{R}}_{_{ME}},k\big)$ with respect to $k$, $\frac{MSE\big(\hat{\beta}^{^{R}}_{_{ME}},k\big)}{dk}=\frac{\psi_{b}(k)}{dk}+\frac{\psi_{v}(k)}{dk}=2 \sum_{j=1}^{p}\frac{\lambda_{j}}{(\lambda_{j}+k)^{3}}(k\alpha_{j}^{2}-\sigma^{2})\ (\ast)$. A sufficient condition for $(\ast)$ to be negative is that, $\ (k\alpha_{max}^{2}-\sigma^{2})<0\Rightarrow k<\frac{\sigma^{2}}{\alpha^{2}_{max}}\ $ with $\ \alpha_{max}=\max(\alpha_{j})\ $. Thus, for $0<k<\frac{\sigma^{2}}{\alpha^{2}_{max}},\ 	MSE\big(\hat{\beta}^{^{R}}_{_{ME}},k\big)\ $ decrease i.e
$$	MSE\big(\hat{\beta}^{^{R}}_{_{ME}},k\big)<MSE\big(\hat{\beta}^{^{R}}_{_{ME}},0\big)= MSE\big(\hat{\beta}^{^{LS}}_{_{ME}}\big)\ \ as\ required.$$
\section{Measurement Error In Lasso}
Modern  statistics is facing problems due to the increase of dimensionality of the data in field such as genomics,finance,network analysis,\dots .It is quite canonical in high-dimensional regression, where the number of variables $p$  largely exceeds the sample size $n$ to assume that the number of covariates $s$ that has an effect on the response variable $y$ is much less than $n$ (\textit{sparsity assumption}).Hence, the vector of regression parameters is assumed to be $s-sparse$.A plethora of high-dimensional regression methods is available, among which the "Lasso regression \cite{nref9} we presented in \textbf{ subsection 1.3.2}, "Dantzig selector (DS) \cite{nref2} and Smoothly Clipped Absolute Deviation (SCAD) \cite{nref3}.These methods all allow model selection and parameter estimation through a penalization of the parameters as seen for the Lasso case (\textbf{ subsection 1.3.2}).These methods are developed for the case in which the covariates are fully observed and without errors; However, in many applications, our data are subject to at least some measurement error.In classical regression context, when $p<n$ and standard methods can be applied, it is well known that measurement error in the covariates will lead to bias in the estimation of the parameters \eqref{f52} and to loss of power \cite{nref5}.

Since the standard Lasso is widely used despite the present of measurement error, it is of interest to study the effects measurement error has on the analysis and describes some of the statistical methods used to correct for those effects.
\subsection{Impact Of Ignoring Measurement Error}
The notation in \textbf{\ref{tpl} P.21} (used to study proprieties of lasso) is used for $W$ and $U$.We partition the variance matrix in the form:
\begin{equation}
S_{WW}=\begin{bmatrix}
	S_{WW}(S,S)& S_{WW}(S,S^{c}) \\
	 S_{WW}(S^{c},S) & S_{WW}(S^{c},S^{c}) \\
\end{bmatrix}
\end{equation}
We saw that in the absence of measurement error, the Lasso is consistent for prediction and estimation (\textbf{Theorem 1.3.3} \eqref{f35}).
$y=X\beta +\epsilon=(W+U)\beta+\epsilon=W\beta+ \underbrace{\epsilon-U\beta}_{\delta}$.
\begin{proposition}
	Assume the compatibility condition  \eqref{f34} holds with constant $\Phi$ , and that there exist a constant $\lambda_{0}$ such that  $\frac{2}{n}\parallel\delta^{t}W\parallel_{\infty}\leq \lambda_{0}$; Then, with a regularization parameter $\lambda \geq 2\lambda_{0}$,
	\begin{equation}
		\frac{1}{n}\parallel W(\hat{\beta}^{Lasso}-\beta )\parallel^{2}_{2}+  \lambda\parallel \hat{\beta}^{Lasso}-\beta \parallel_{1}\leq \frac{4 \lambda^{2}s}{\Phi_{o}^{2}}.
		\label{f61}
	\end{equation}
\end{proposition}
\textbf{Proof.} This is from \textbf{Theorem 1.3.3}\\

This shows that in the presence of measurement error, the estimation error of Lasso can be bounded.Using the triangle inequality, we have 
\begin{align*}
	\parallel\delta^{t}W\parallel_{\infty}=&	\parallel(\epsilon-U\beta)^{t}W\parallel_{\infty}=\parallel \epsilon^{t}W-\beta^{t}U^{t}(X+U)\parallel_{\infty}\\
	\leq&\parallel \epsilon^{t}W\parallel_{\infty}+\parallel\beta^{t}U^{t}X\parallel_{\infty}+\parallel\beta^{t}U^{t}U\parallel_{\infty}\leq\parallel \epsilon^{t}W\parallel_{\infty}+\parallel\beta^{t}U^{t}X\parallel_{\infty}+\parallel\beta\parallel_{1}\parallel U^{t}U\parallel_{\infty}
\end{align*}
Hence the bound \eqref{f61} is implied by ,
\begin{equation}
	\frac{2}{n}\parallel \epsilon^{t}W\parallel_{\infty}+\frac{2}{n}\parallel\beta^{t}U^{t}X\parallel_{\infty}+2\parallel\beta\parallel_{1}\parallel \frac{U^{t}U}{n}\parallel_{\infty}\leq\lambda_{0}\ ;
	\label{f62}
\end{equation}
and the Lasso with measurement error is consistent if all the three terms in the above expression \eqref{f62} converge to $0$.However, 
$$\frac{U^{t}U}{n}\underset{n \rightarrow +\infty}{\longrightarrow}\Sigma_{U}\ \ and\ \ \parallel\Sigma_{U}\parallel_{\infty} \neq0$$,
consequently, we do not obtain consistency.

We have just see that standard results for consistency of estimation no longer hold when the covariates are affected by measurement error.Now let's see how measurement error affect covariate selection with Lasso.By \textbf{Definition 1.3.5 \eqref{f40}}, the "irrepresentable condition with measurement error (\textbf{IC-ME}) hold if there exists a constant $\theta \in [0,1[$ such that ,
\begin{equation}
	\parallel S_{WW}(S^{c},S)S_{WW}(S,S)^{-1}sign\big(\beta_{S}\big)\parallel_{\infty}\leq \theta.
	\label{f63}.
\end{equation}
In presence of measurement error, \textit{Sorensen, Frigessi and Thoren (2015) \cite{nref17}} shown that to achieve covariate selection consistency, we need the following additional condition called "Measurement Error Condition" (\textbf{MEC}):
\begin{definition}[MEC]
	The measurement error condition (MEC) is satisfied if
	\begin{equation}
		\Sigma_{W}(S^{c},S)\Sigma_{W}(S,S)^{-1}\Sigma_{U}(S,S)-\Sigma_{U}(S^{c},S)=0.\ ,\ (visit\ \cite{nref17}\ for\ more\ details).
	\end{equation}
\end{definition}
\begin{remarque}
	The MEC applies to population covariance matrix,whereas the IC-ME applies to sample covariance matrix.
\end{remarque}
\subsection{Correction for Measurement Error in Lasso}
The purpose of this section es to describe some penalized regressions correction methods that may be used to correct both the variable selection and the model estimation at the same time assuming measurement error is adequately modelled (in our case " additive measurement error" ).

To show the bias in the estimation caused by measurement error, consider the naive Lasso approach, plugging in $W$ for $X$ in the Lasso estimator defined in \eqref{f18}
	\begin{equation}
	\hat{\beta}^{^{LS}}(\lambda_{n}) = argmin_{\beta \in \mathbb{R}^{p} } \bigg\{ \parallel y- W\beta \parallel^{2}_{2}+ \lambda_{n}\parallel \beta \parallel_{1}\bigg\}.
	\label{f64}
\end{equation}
It is possible to  demonstrate that this yield the bias loss function:
\begin{equation}
	\mathbb{E}\big[\parallel y- W\beta \parallel^{2}_{2}|X,y\big]=\parallel y- X\beta \parallel^{2}_{2}+n\beta^{t}\Sigma_{U}\beta.
	\label{f65}
\end{equation}
Indeed, using the properties of the conditional expectation , we have:
\begin{align*}
	\mathbb{E}\big[\parallel y- W\beta \parallel^{2}_{2}|X,y\big]=&\mathbb{E}\big[(y- W\beta)^{t}( y- W\beta)\big]=\mathbb{E}\bigg[\big(( y- X\beta)^{t}-(U\beta)^{t}\big)\big( (y- X\beta)-(U\beta)\big)\bigg]\\
	=&\mathbb{E}\big[( y- X\beta)^{t}( y- X\beta)|X,y\big]-( y- X\beta)^{t}\beta\underbrace{\mathbb{E}[U]}_{0}-\beta^{t}( y- X\beta)\underbrace{\mathbb{E}[U^{t}]}_{0}+\mathbb{E}\big[\beta^{t}U^{t}U\beta\big]\\
	=&	\parallel y- X\beta \parallel^{2}_{2}+\beta^{t}\mathbb{E}\big[U^{t}U\big]\beta=\parallel y- X\beta \parallel^{2}_{2}+n\beta^{t}\Sigma_{U}\beta\, \ (remember\ \mathbb{E}[\frac{U^{t}U}{n}]=\Sigma_{U}).
\end{align*}
\subsection{Corrected Lasso (Non Convex Lasso)}
The must natural way for correcting for the bias in \eqref{f65} leads to the constrained correct Lasso ($CCL$):
\begin{equation}
	\hat{\beta}_{_{CCL}}\in \underset{\beta:\parallel\beta\parallel_{1}\leq R}{argmin}\bigg\{\frac{1}{n}\parallel y- W\beta \parallel^{2}_{2}-\beta^{t}\Sigma_{U}\beta\bigg\}.
	\label{f66}
\end{equation}
or alternatively , the regularized version (regularize corrected Lasso),
\begin{equation}
		\hat{\beta}_{_{RCL}}\in \underset{\beta \in \mathbb{R}^{p}}{argmin}\bigg\{\frac{1}{n}\parallel y- W\beta \parallel^{2}_{2}-\beta^{t}\Sigma_{U}\beta+\lambda_{_{RCL}}\parallel\beta\parallel_{1}\bigg\}.
	\label{f67}
\end{equation}
both introduced by \textit{Loh and Wainright (2012) \cite{nref17}}.

Since in practice we may not know the covariance matrix $\Sigma_{X}$, given the set of samples, it is natural to form the estimates of the quantities $\Sigma_{X}$ and $\Sigma_{X}\beta$ as:$$\hat{\Sigma_{X}}=\frac{W^{t}W}{n}-\Sigma_{U}\ ,\ and\ \ \hat{\gamma}=\frac{1}{n}W^{t}y$$.
Notice that $\Sigma_{U}$ is in practice unknown and must be estimated from data, (see \textbf{Proposition 2.1.4}).
\begin{proposition}
	The estimator \eqref{f66} and \eqref{f67} can be reformulated as:
	\begin{equation}
			\hat{\beta}_{_{CCL}}\in \underset{\beta:\parallel\beta\parallel_{1}\leq R}{argmin}\bigg\{\frac{1}{2}\beta^{t}\hat{\Sigma}_{X}\beta-\hat{\gamma}^{t}\beta\bigg\}.\ ,\ and
		\label{f68}
	\end{equation}
	\begin{equation}
	\hat{\beta}_{_{RCL}}\in \underset{\beta:\parallel\beta\parallel_{1}\leq b_{0}\sqrt{s}}{argmin}\bigg\{\frac{1}{2}\beta^{t}\hat{\Sigma}_{X}\beta-\hat{\gamma}^{t}\beta+\lambda_{_{RCL}}\parallel\beta\parallel_{1}\bigg\}.\ ,\ for\ some\ constant\ b_{0} .
	\label{f69}
\end{equation}
\end{proposition}
\textbf{Proof.} set $f(\beta)=\frac{1}{n}\parallel y- W\beta \parallel^{2}_{2}-\beta^{t}\Sigma_{U}\beta$. The result follow from the fact that,
\begin{align*}
	f(\beta)&=\frac{1}{n}(y-W\beta)^{t}(y-W\beta)-\beta^{t}\Sigma_{U}\beta\\
&	=\frac{1}{n}y^{t}y-\frac{1}{n}y^{t}W\beta-\frac{1}{n}\beta^{t}W^{t}y+\beta^{t}\frac{W^{t}W}{n}\beta-\beta^{t}\Sigma_{U}\beta\\
&	=\frac{1}{n}\parallel y\parallel_{2}^{2}-2\frac{1}{n}y^{t}W\beta +\beta^{t}\big(\hat{\Sigma}_{X}+\Sigma_{U}\big)-\beta^{t}\Sigma_{U}\beta
	=\frac{1}{n}\parallel y\parallel_{2}^{2}-2\hat{\gamma}^{t}\beta+\beta^{t}\hat{\Sigma}_{X}\beta \\
&\underset{\beta}{argmin}\big\{f(\beta)\big\}=\underset{\beta}{argmin}\bigg\{\frac{1}{2}\big(f(\beta)-\frac{1}{n}\parallel y\parallel_{2}^{2}\big)\bigg\}=\underset{\beta}{argmin}\bigg\{\frac{1}{2}\beta^{t}\hat{\Sigma}_{X}\beta-\hat{\gamma}^{t}\beta\bigg\}
\end{align*}
\begin{remarque}
	When $\Sigma_{U}=0_{\mathbb{R}^{p\times p}}$ (corresponding to the noiseless case), the estimators reduce to the standard Lasso.However, when $\Sigma_{U} \neq0_{\mathbb{R}^{p\times p}}$, the matrix $\hat{\Sigma}_{X}$ is not positive semidefinite in high-dimensional regime ($p>>n$).Indeed, since the matrix $\frac{1}{n}W^{t}W$ has rank at must $n$, the subtracted matrix $\Sigma_{U}$ may cause $\hat{\Sigma}_{X}$ to have a large number of negative eigenvalues.Consequently the quadratic losses appearing in the problems \eqref{f66} and \eqref{f67} are \textbf{non convex}.
\end{remarque}
\begin{remarque}
	When, $\hat{\Sigma}_{X}\ $  has negative eigenvalues (which happen very often under high-dimensionality), the objective function in equation \eqref{f67} is unbounded from below, hence we make use of the regularized estimator \eqref{f69} to overcome these technical  difficulties.
\end{remarque}

\begin{remarque}
Note that,"$\in$" and not "$=$" has been used because in the presence of non-convexity, it is generally impossible to provide a polynomial-time algorithm that converges to a (near) global optimum due to the presence of local minima.

\textit{Loh and Wainwright \cite{nref13}} demonstrated that a simple "\textbf{project gradient descent algorithm}" applied to the problems \eqref{f66} or \eqref{f69} (if $b_{0}$ is properly chosen) converge with high probability to a small neighbourhood of the set of all global minimizers.
\end{remarque}
\begin{definition}
	Project gradient descent is a standard way to solve constrained optimization problem.
\end{definition}

The correct Lasso has been shown to yield good estimation bounds,see [\textit{ \cite{nref17}, Theorem 1 and Theorem 2} ]. \textit{Sorensen et al. \cite{nref17}} derived its asymptotic selection consistency properties.

\subsection{Convex Conditional Lasso}
A clear drawback of the previous method is that it leads to a non-convex optimization problem.The ideal behind CoCoLasso is to intervene directly on $\hat{\Sigma}_{X}\ $, the estimated covariance matrix of $X$, with a transformation that will provide a "positive semi-definite" matrix.

We first introduce some necessary notations and model setup:
\begin{itemize}
	\item For any square matrix $G=(g_{ij})_{i,j}$, we write $G>0\ $ ($\geq$0) when it is positive (semi-) definite.
	\item Let $\parallel G\parallel_{\max}=\underset{i,j}{\max}|g_{ij}|$ denote the element-wise maximum norm.
	\item We assume that all variables are centred so the the intercept term is not included in the model.
\end{itemize}
We now define a nearest positive semi-definite matrix operator as follows:

For any square matrix $G$, 
\begin{equation}
	\big(G\big)_{+}=\underset{G_{1}\geq0}{argmin}\parallel G-G_{1}\parallel_{\max}
	\label{f70}
\end{equation}
This operator will project the matrix  $\hat{\Sigma}_{X}\ $ into a space of semi-definite matrix selecting the nearest one. Then, by denoting $\overset{_{\thicksim}}{\Sigma}_{X}=\big(\hat{\Sigma}_{X}\big)_{+}\ $ , the convex conditional Lasso is define as:
\begin{equation}
\hat{\beta}_{_{CoCo}}= \underset{\beta \in \mathbb{R}^{p}}{argmin}\bigg\{\frac{1}{2}\beta^{t}\overset{_{\thicksim}}{\Sigma}_{X}\beta-\hat{\gamma}^{t}\beta+\lambda_{_{CoCo}}\parallel\beta\parallel_{1}\bigg\}
\label{f71}
\end{equation}
\begin{remarque}
	The matrix $\overset{_{\thicksim}}{\Sigma}_{X}$ is always positive semi-definite by construction while  $\hat{\Sigma}_{X}\ $ is guaranteed to be positive semi-definite only for $p<n$.Consequently, the optimization problem in \eqref{f71} is guaranteed to be convex.
\end{remarque}
\begin{theoreme}[Cholesky decomposition]
	Let $A$ be a real-valued symmetric (semi-) positive-definite matrix; There exist a lower triangular matrix $L$ with real and positive diagonal entries, such that,
	\begin{equation}
		A=L^{T}L
		\label{f72}
	\end{equation}
\end{theoreme}
Defining $ \frac{1}{\sqrt{n}}\overset{_{\thicksim}}{X}$ the Cholesky factor of  $\overset{_{\thicksim}}{\Sigma}_{X}$ (i.e $\ \frac{1}{n}\overset{_{\thicksim}}{X}^{t}\overset{_{\thicksim}}{X}=\overset{_{\thicksim}}{\Sigma}_{X}\ $) and $\overset{_{\thicksim}}{y}$ such that $\frac{1}{n}\overset{_{\thicksim}}{X}^{t}\overset{_{\thicksim}}{y}=\hat{\gamma}=\frac{1}{n}W^{t}y$, the estimator \eqref{f71} can be reformulates as:
\begin{equation}
\hat{\beta}_{_{CoCo}}= \underset{\beta \in \mathbb{R}^{p}}{argmin}\bigg\{\frac{1}{n}\parallel \overset{_{\thicksim}}{y}- \overset{_{\thicksim}}{X}\beta \parallel^{2}_{2}+\lambda_{_{CoCo}}\parallel\beta\parallel_{1}\bigg\}
\label{f73}
\end{equation}

\begin{remarque}
	This is a regular Lasso regression of $\ \overset{_{\thicksim}}{y}\ $ and $\ \overset{_{\thicksim}}{X}\ $ with penalization parameter $\lambda_{_{CoCo}}\ $ (\textbf{Section 1.3.2}).It is of great advantage for the practical implementation.We can apply any standard Lasso algorithm as the \textit{ coordinate descent algorithm \cite{nref20} or Least angle regression \cite{nref21} } to obtain solution.
\end{remarque}
\begin{center}
	\subsubsection{Theoretical Properties (Consistency assessment)}
\end{center}

Theoretically, \eqref{f71} can be analysed by the tools for analysing the clean Lasso. By definition of $\overset{_{\thicksim}}{X}$, we have:
\begin{equation}
	\parallel\overset{_{\thicksim}}{\Sigma}_{X}-\Sigma_{X} \parallel_{\max}\leq \parallel\overset{_{\thicksim}}{\Sigma}_{X}-\hat{\Sigma}_{X} \parallel_{\max} + \parallel\hat{\Sigma}_{X}-\Sigma_{X} \parallel_{\max} \leq2 \parallel \hat{\Sigma}_{X}-\Sigma_{X} \parallel_{\max}
	\label{f74}
\end{equation}

\begin{remarque}
Equation \eqref{f74} ensure that $\overset{_{\thicksim}}{\Sigma}_{X}$ approximates the true Gram matrix $\Sigma_{X}$ as well as the initial surrogate $\hat{\Sigma}_{X}$ chosen by  \textit{Loh and Wanwright} which is often an unbiased estimate of $\Sigma_{X}$ achieving a desired rate of convergence under the $max\ norm\ \parallel . \parallel_{\max}$.
\end{remarque}
\begin{definition}[Closeness Condition]
	Let us assume that the distribution of $\ \hat{\Sigma}_{X}\ $ and $\ \hat{\gamma}\ $ are identified by a set of parameters $\theta$.Then, there exist universal constants $C$ and $c$ and positive functions $\zeta$ and $\epsilon_{0}$ depending on $\beta_{S},\ \theta\ and\ \sigma^{2} $, such that for every $\epsilon\leq\ \epsilon_{0}\ $, $\ \hat{\Sigma}_{X}\ $ and $\ \hat{\gamma}\ $ satisfy the following probability statements:
	\begin{equation}
		\mathbb{P}\bigg(\bigg\{|\hat{\Sigma}_{X_{ij}}-\Sigma_{X_{ij}}|\geq \epsilon \bigg\}\bigg)\leq C\exp\big(-nc\epsilon^{2}\zeta^{-1}\big)
		\label{f75}
	\end{equation}
\begin{equation}
		\mathbb{P}\bigg(\bigg\{|\hat{\gamma}_{j}-\gamma_{j}|\geq \epsilon \bigg\}\bigg)\leq C\exp\big(-nc s^{-2}\epsilon^{2}\zeta^{-1}\big)\ ,\quad \forall i,j=1,\dots,p.
		\label{f76}
\end{equation}
\end{definition}
\begin{remarque}
	This condition required that the surrogates $\hat{\Sigma}_{X}$ (and hence $\overset{_{\thicksim}}{\Sigma}_{X}$ ) and $\hat{\gamma}$ are close to $\Sigma_{X}$ and $\gamma$ respectively in term of the element-wise maximum norm.
\end{remarque}
\begin{definition}[ Restricted eigenvalue Condition]
	\begin{equation}
		\underset{v\neq0_{\mathbb{R}^{p}},\parallel v_{S^{c}}\parallel_{1}\leq3\parallel v_{S}\parallel_{1}}{min}\bigg\{\frac{v^{t}\Sigma_{X}v}{\parallel v \parallel_{2}^{2}}\bigg\}=\phi>0
		\label{f77}
	\end{equation}
\end{definition}
refer to \cite{nref22} for more details about this condition. We have the following result on the $L_{1}$ and $L_{2}$ statistical error of the CoCoLasso estimate.
\begin{theoreme}[\textit{from \cite{nref18}}]
	Under the assumptions stated in \eqref{f75},\eqref{f76},\eqref{f77} ; for $\lambda \leq min\big(\epsilon_{0},12\epsilon_{0}\parallel \beta_{S}\parallel_{\infty}\big)\ $ and $\epsilon\leq min\big(\epsilon_{0},\frac{\phi}{64s}\big)\ $ the following results holds true with probability at least $1-p^{2}C\exp(-ncs^{-2}\lambda^{2}\zeta^{-1})-p^{2}C\exp(-nc\epsilon^{2}\zeta^{-1})\ $:
	\begin{equation}
		\parallel \hat{\beta}_{_{CoCo}}-\beta \parallel_{2}\leq C\lambda\frac{\sqrt{s}}{\phi}\ ;\ \parallel \hat{\beta}_{_{CoCo}}-\beta \parallel_{1}\leq C\lambda\frac{s}{\phi}
	\end{equation}
\end{theoreme}
\textbf{Proof.} proof are provided in [\textit{\cite{nref18}, Section 8, P.14}].\\
In order to establish the sign consistency of CoCoLasso, In addition to the \textit{Closeness Condition}\eqref{f75} and \eqref{f76}, let's assume the \textit{ "minimum eigenvalue condition"} and the \textit{irrepresentable condition} which are sufficient and nearly necessary for sign consistency of the standard Lasso(\textbf{Theorem 1.3.4}):

\begin{equation}
	\parallel \Sigma_{X}(S^{c},S)\Sigma_{X}^{-1}(S,S)sign(\beta_{S})\parallel_{\infty}<1\ ,\ \lambda_{min}\big(\Sigma_{X}(S,S)\big)=C_{min}>0
	\label{f78}
\end{equation}
where $\ \lambda_{min}\big(\Sigma_{X}(S,S)\big)\ $ denote the minimum eigenvalue of $\Sigma_{X}(S,S)$.
\begin{theoreme}
	under the assumptions given in equations \eqref{f75},\eqref{f76} and \eqref{f78}, for $lambda\leq min\big(\epsilon_{0},\frac{4\epsilon}{\gamma}\big)\ $ and $\epsilon\leq min\big(\epsilon_{1},\frac{\lambda}{(\lambda \epsilon_{2}+\epsilon_{3})}\big)$ where $\epsilon_{i}'s$ are bounded positive constants depending of $\Sigma_{X}(S,S),\ \beta_{S},\ \theta\ and\ \sigma^{2}\ $; The following occur with probability at least $1-\psi\ $, with\\
	$\psi=p^{2}C\exp\big(-ncs^{-2}\lambda^{2}\zeta^{-1}\big)-p^{2}C\exp\big(-ncs^{-2}\epsilon^{2}\zeta^{-1}\big)$ :
	\begin{itemize}
		\item There exist a unique solution $\hat{\beta}^{^{CoCo}}$ minimizing \eqref{f71} whose support is a subset of the true support.
		\item $\parallel \hat{\beta}^{^{CoCo}}_{_{S}} -\beta_{_{S}}\parallel_{\infty}\leq \kappa\lambda\ $ where $\ \kappa=\big( 4\parallel\Sigma_{X}^{-1}(S,S)\parallel_{\infty}+C_{min}^{-\frac{1}{2}} \big)$
		\item If $|\beta_{min}|\geq \kappa\lambda,\ $ then $Sign\big(hat{\beta}^{^{CoCo}}_{_{S}}\big)=Sign\big(\beta_{_{S}}\ $.
	\end{itemize}
\end{theoreme}
\textbf{Proof.} visit [\cite{nref18}, \textit{Theorem 2, Section 8, P.14}] for more details.
\begin{remarque}
	If we assume for simplicity that $\ \kappa\ $ is $o(1)$ and the triplet $\{n,p,s\}$ and $\ \beta\ $ satisfy the scaling:
	\begin{equation}
		\frac{s^{2}\log(p)}{n}\longrightarrow 0\ \ as\ n,p\rightarrow+\infty\ ,\ |\beta_{min}|>>s\bigg(\zeta \frac{\log(p)}{n}\bigg)^{\frac{1}{2}}\ .
		\label{f79}
	\end{equation}
	Then from the expression of $psi$ in \textbf{Theorem 2.3.3} above, we can choose $\lambda$ so that $1-\psi$ goes to one, which implies the sign-consistency of the CoCoLasso estimate.
\end{remarque}
\begin{corollaire}[Sign-Consistency]
	If $\ \Sigma_{X}\ $ , $\overset{_{\thicksim}}{\Sigma}_{X}\ $ and $\ \hat{\gamma}\ $ satisfy the regularity conditions given in \textbf{Theorem 2.3.3}, Then under the scaling in equation \eqref{f79}, the CoCoLasso estimate $\ \hat{\beta}^{^{CoCo}}\ $ defined in \eqref{f71} is sign-consistent if $\ |\beta_{min}|>>\lambda>>s\big(\zeta \frac{\log(p)}{n}\big)^{\frac{1}{2}}\ $ and we have bound 
	\begin{equation}
		\mathbb{P}\bigg(\parallel \hat{\beta}^{^{CoCo}}_{_{S}} -\beta_{_{S}}\parallel_{\infty}\leq \kappa\lambda\bigg) \underset{n\rightarrow+\infty}{\longrightarrow}1
	\end{equation}

\end{corollaire}
\begin{remarque}
	Unlike asymptotic selection consistency properties for non convex Lasso (NCL) , the which was derived only for restrictive case of additive measurement error, the result provided in this subsection those note requires any specification of the type of measurement error.
\end{remarque}
\subsection{Selecting The Tuning Parameter Under Measurement Error}
The choose of the tuning parameter in penalized methods  relies on \textit{Cross-Validation} (\textbf{Subsection 1.3.5}). In presence of measurement error, naive application of Cross-validation might lead to bias results.To elucidate, consider the usual K-folds Cross-validation  for selecting optimal $\ \lambda\ $ in the clean Lasso \eqref{f42}.

If we naively use the observed data $(W,y)$ , then the cross-validated choice of $\lambda$ is defined by minimizing ,
\begin{equation}
CV_{(K)}	=\frac{1}{K} \sum_{k=1}^{K}\frac{1}{n_{k}}\parallel y_{k}-W_{k}\hat{\beta}_{k}(\lambda)\parallel_{2}^{2}.
\label{f80}
\end{equation}
Even if we use CoCoLasso or NCL to compute $\hat{\beta}_{k}(\lambda)$ based on $W_{-k} and\ y_{-k}\ $, the above criterion is biased compared to \eqref{f42} in the same way we shown that the loss function in \eqref{f64} is a biased version of the one in \eqref{f18}.Observing  that \eqref{f42} is equivalent to:
\begin{equation}
	\hat{\lambda}= \underset{\lambda}{argmin}\bigg\{\frac{1}{K}\sum_{k=1}^{K}\frac{1}{2}\hat{\beta}^{t}_{_{k}}(\lambda)\Sigma_{k}\hat{\beta}_{_{k}}(\lambda)-\gamma^{t}_{_{k}}\hat{\beta}_{_{k}}(\lambda)\bigg\}.
	\label{f81}
\end{equation}
where $\Sigma_{k}=\frac{1}{n_{k}}X_{k}^{t}X_{k}\ $ and $\ \gamma_{k}=\frac{1}{n_{k}}X_{k}^{t}y_{k}\ $;see proof of \textbf{Proposition 2.3.2 }(setting $W=X_{k}$ and $\ \beta^{t}\Sigma_{U}\beta=0$ ).


Since unbiased the unbiased surrogate $\hat{\Sigma}_{k}\ $ possibly has negative eigenvalues, using it will lead to a cross validation function unbounded from below.\textit{Datta and Zou \cite{nref18}} substituted $\Sigma_{k}$ and $\ \gamma_{k}\ $ with their projected and estimated counterparts $\ \overset{_{\thicksim}}{\Sigma}_{k}=\big(\hat{\Sigma}_{k}\big)_{+}\ $ and  $\ \hat{\gamma}_{k}\ $.With this correction, the cross-validated $\lambda$ is defined as:
\begin{equation}
		\overset{_{\thicksim}}{\lambda}= \underset{\lambda}{argmin}\bigg\{\frac{1}{K}\sum_{k=1}^{K}\frac{1}{2}\hat{\beta}^{t}_{_{k}}(\lambda)\overset{_{\thicksim}}{\Sigma}_{k}\hat{\beta}_{_{k}}(\lambda)-\hat{\gamma}^{t}_{_{k}}\hat{\beta}_{_{k}}(\lambda)\bigg\}.
\end{equation}
$\overset{_{\thicksim}}{\lambda}$ is an unbiased estimator  of $\lambda$.More theoretical result about cross-validation under measurement error could be found in \cite{nref25}.
\section{Matrix uncertainty selector (MU-Selector)}
So far, we saw that corrected Lasso (NCL) \eqref{f69} and CoCoLassso correct for measurement error, by including in the model the covariance of the measurement error $\Sigma_{U}$, and yielding estimators with good theoretical properties.However, this quantity is assumed to be known and in practice it is usually not known.The estimation of the covariance matrix of the measurement error requires additional data as replicated  measurement of the covariates, and can be computationally expensive or even unfeasible when the number of variables $p$ increases.

An interesting alternative is the so-called \textit{Matrix Uncertainty Selector} proposed by \textit{Rosenbaum and Tsybakov \cite{nref23}}.

We consider the model in \eqref{f46}.We typically assume that $\beta$ is "s-sparse" where $\ 1\leq s\leq p\ $ is some integer. In what follows, we assume that $\epsilon$ and $U$ satisfy the assumptions:
\begin{equation}
	\frac{1}{n}\parallel W^{t}\epsilon\parallel_{\infty}\leq\lambda\ \ and\ \ \parallel U \parallel_{\infty}\leq \delta .\quad (\ with\ high\ probability\ ).
	\label{f82}
\end{equation}
The "Matrix Uncertainty Selector" $\ \hat{\beta}_{_{MUS}}\ $ is define as the solution of the minimization problem:
\begin{equation}
	\min\bigg\{\parallel \beta \parallel_{1}\ :\beta\in \Theta, \frac{1}{n}\parallel W^{t}(y-W\beta)\parallel_{\infty}\leq (1+\delta)\delta\parallel\beta\parallel_{1}+\lambda\bigg\},
	\label{f83}
\end{equation}
where $\ \Theta \subseteq \mathbb{R}^{p}\ $ is a given set characterizing the prior knowledge about $\ \beta$.

The problem \eqref{f83} is a convex minimization problem and it reduces to linear programming if $\ \Theta=\mathbb{R}^{p}\ $.Throughout this section, we assume for simplicity that all diagonal elements of the Gram matrix $\ \frac{1}{n}X^{t}X\ $ are equal to 1.\
\begin{proposition}[solution existence]
	Under assumptions \eqref{f82} , the feasible set of the convex problem  \eqref{f83} is non empty,
	\begin{equation}
		\Psi=\bigg\{\beta\in \Theta, \frac{1}{n}\parallel W^{t}(y-W\beta)\parallel_{\infty}\leq (1+\delta)\delta\parallel\beta\parallel_{1}+\lambda\bigg\}\neq \emptyset
		\label{f84}
	\end{equation}	
\end{proposition}
\textbf{Proof.} let's show that the true $\ \beta^{\ast}\ $belong to $\Psi$. $\beta^{\ast}\in \Theta\ $ and,
\begin{align*}
\frac{1}{n}\parallel W^{t}(y-W\beta^{\ast})\parallel_{\infty}&=\parallel W^{t}(X\beta^{\ast}+\epsilon -W\beta^{\ast})\parallel_{\infty}=\parallel W^{t}\big(\epsilon -(\underbrace{W-X}_{U})\beta^{\ast}\big)\parallel_{\infty}\\
&\leq \frac{1}{n}\parallel W^{t}\epsilon\parallel_{\infty}+\frac{1}{n}\parallel W^{t}U\beta^{\ast}\parallel_{\infty}\leq \lambda + \frac{1}{n}\parallel W^{t}U\beta^{\ast}\parallel_{\infty}\leq ,\ by\ \eqref{f82}
\end{align*}
next by second inequality of \eqref{f82} and by the fact that all the diagonal elements of $\frac{1}{n}X^{t}X\ $  are equal to 1, the columns matrix $\ W_{(j)}\ $ of $\ W\ $ satisfy $\parallel W_{(j)}\parallel_{2}\leq\sqrt{n}(1+\delta)\ $, therefore, we obtain
\begin{align*}
\frac{1}{n}\parallel W^{t}U\beta^{\ast}\parallel_{\infty}&\leq\frac{1+\delta}{\sqrt{n}}\parallel U\beta^{\ast}\parallel_{2}	\leq (1+\delta)\parallel U\beta^{\ast}\parallel_{\infty}\\
&\leq (1+\delta)\parallel U \parallel_{\infty}\parallel \beta^{\ast}\parallel_{1}\leq  (1+\delta)\delta\parallel \beta^{\ast}\parallel_{1}.
\end{align*}
 Hence $\frac{1}{n}\parallel W^{t}(y-W\beta)\parallel_{\infty}\leq\lambda+ (1+\delta)\delta\parallel\beta\parallel_{1}\ $, thus there always exists a solution $\ \hat{beta}_{MUS}\ $ of \eqref{f83}.
 
 \begin{remarque}
 	If $\ \delta=0\ $ and $\Theta=\mathbb{R}^{p}\ $, the MU-Selector becomes the Dantzig selector \eqref{f41}.
 	
 	The MU-Selector can be seen as an evolution of the Dantzig selector that can also take into account the measurement error in the model without needing any information about the measurement error variance, but rather by using a supplementary tuning parameter ($"\delta"$).
 \end{remarque}
\begin{center}
	\subsubsection{Theoretical Properties}
\end{center}
For a vector $\beta\in \mathbb{R}^{p}\ $ and a subset $S$ of $\ \{1,\dots,p\}\ $, we denote by $\beta_{S}\ $ the vector in $\mathbb{R}^{p}\ $ that has same coordinates as $\beta\ $ on the set indices $S\ $ and zero coordinates on its complement $\ S^{c}\ $ ($\ \beta=\beta_{S}+\beta_{S^{c}})\ $.

Assume that the matrix $X\ $ satisfy one of the following conditions (assumptions):
\begin{description}
	\item[Restricted eigenvalue assumption RE(s):] there exist $\phi>0\ $ such that :
	\begin{equation}
		\underset{V\neq0_{\mathbb{R}^{p}},\parallel V_{S^{c}}\parallel_{1}\leq\parallel V_{S}\parallel_{1}}{min}\bigg\{\frac{\parallel XV\parallel_{2}}{\sqrt{n}\parallel V \parallel_{2}}\bigg\}\geq\phi
		\label{f85}
	\end{equation}
for all subsets $\ S\ $ of $\ \{1,\dots,p\}\ $ of cardinality $|S|\leq s$. Detailed discussion of this assumption can be found in \cite{nref24}.
\item[Coherence Condition:] all the diagonal elements of the matrix $C=\frac{1}{n}X^{t}X\ $ are equal to 1 and its off-diagonal elements $\ c_{ij}\ i\neq j\  $ , satisfy the coherence condition:
\begin{equation}
	\underset{i\neq j}{\max}|c_{ij}|\leq\rho\quad with\ some \ \rho<1.
	\label{f86}
\end{equation}
\end{description}
\begin{theoreme}[\textit{from \cite{nref23}}]
	Assume that model \eqref{f46} holds with an unknown \textit{s-sparse} parameter vector $\ \beta\ $ and that all the diagonal elements of $\frac{1}{n}X^{t}X\ $ are equal to 1.Let \eqref{f82} holds, set $\nu=2(2+\delta)\delta\parallel\beta\parallel_{1}+2\lambda$. Then for any solution $\hat{\beta}_{_{MUS}}\ $ of \eqref{f83}, we have the following inequalities:
	\begin{description}
		\item[i)] under restricted eigenvalue assumption RE(s) \eqref{f85},
		\begin{equation}
			\parallel\hat{\beta}_{_{MUS}}-\beta \parallel_{1}\leq\frac{4\nu s}{\phi^{2}}\quad ,\ 	\parallel X(\hat{\beta}_{_{MUS}}-\beta )\parallel_{2}^{2}\leq\frac{4\nu^{2} s}{\phi^{2}}
			\label{f87}
			\end{equation}
	\item[ii)] Under Coherence condition assumption \eqref{f86} with $\ \rho<\frac{1}{3\alpha s},\ \alpha>1$:
	\begin{equation}
			\parallel\hat{\beta}_{_{MUS}}-\beta \parallel_{\infty}\leq\frac{3\alpha +1}{3(\alpha-1)}\nu
			\label{f88}
	\end{equation}
		\end{description}
\end{theoreme}
\textbf{Sketch of proof.} set $\ V=\hat{\beta}_{_{MUS}}-\beta\ $ and $\ S\ $ the set of non zero coordinates of $\ \beta$.we have,\\
$\parallel U^{t}X\parallel_{\infty}=\underset{1\leq j,k \leq p}{\max}|U_{(j)}^{t}X_{(k)}\leq \underset{1\leq j,k \leq p}{\max}\parallel U_{(j)}\parallel_{2}\parallel X_{(j)}\parallel_{2}\leq \delta n\ \ (\ast)$\\
where $U_{(j)},\ X_{(j)}\ $ are the column of $\ U\ $ and $\ X\ $ respectively and we used that, $\parallel X_{(k)}\parallel_{2}=\sqrt{n}\ $ by assumption on $\frac{1}{n}X^{t}X\ $ , and $\parallel U_{(j)}\parallel_{2}\leq \delta \sqrt{n}\ $ by \eqref{f82}.Now, note that \eqref{f82}, $(\ast)\ $ and the fact that $\  \hat{\beta}_{_{MUS}}\ $ belong to the feasible set $\ \Psi\ $\eqref{f84} of \eqref{f83} leads to $$ \parallel\frac{1}{n}X^{t}XV \parallel_{\infty}\ (\ast \ast)$$
Taking into account $\ (\ast \ast)\ $, the proof of \eqref{f87} follows the same lines ass the proof of \textit{[ \cite{nref23}, Theorem 7.1]} setting $\ r=\frac{\nu}{2}\ $ and $\ m=s\ $.\\
To prove \eqref{f88}, one could refer to \textit{[ \cite{nref23}, Theorem 1]}.

\begin{remarque}
	The bounds of \textbf{2.4.1} do not depend on $\parallel \hat{\beta}_{_{MUS}}\parallel_{1}\ $ but on the unknown $\ \beta \parallel_{1}\ $. This drawback is corrected for small values of $\ delta\ $, as shown in the next result.
\end{remarque}
\begin{theoreme}[estimation consistency]
	Let the assumptions of \textbf{Theorem 2.4.1} hold and $\ \delta<\frac{\phi^{2}}{4s}\ $.Set $\ \nu_{1}=2(2+\delta)\delta\parallel  \hat{\beta}_{_{MUS}}\parallel_{1}+2\lambda\ $. then for any solution $\  \hat{\beta}_{_{MUS}}\ $ of \eqref{f83}, we have:
		\begin{description}
		\item[i)] under restricted eigenvalue assumption RE(s) \eqref{f85},
		\begin{equation}
			\parallel\hat{\beta}_{_{MUS}}-\beta \parallel_{1}\leq\frac{4\nu_{1} s}{\phi^{2}}\bigg(1-\frac{4\delta s}{\phi^{2}}\bigg)^{_{-1}}\quad ,\ 	\parallel X(\hat{\beta}_{_{MUS}}-\beta )\parallel_{2}^{2}\leq\frac{4\nu^{2}_{1} s}{\phi^{2}}\bigg(1-\frac{4\delta s}{\phi^{2}}\bigg)^{_{-1}}
		\end{equation}
		\item[ii)] Under Coherence condition assumption \eqref{f86} with $\ \rho<\frac{1}{3\alpha s},\ \alpha>1\ $ and $\ \delta\leq \frac{\phi^{2}}{8s}$:
		\begin{equation}
			\parallel\hat{\beta}_{_{MUS}}-\beta \parallel_{\infty}\leq\frac{2(3\alpha +1)}{3(\alpha-1)}\nu_{1}
		\end{equation}
	\end{description}
\end{theoreme}
\textbf{Proof.} It goes along the same lines as the proof of  \textit{Theorem 2.4.1} , \textit{cf. [ \cite{nref23}, Theorem 4,P.16]}.\\
\begin{definition}
	the threshold estimator $\ \overset{_{\thicksim}}{\beta}=(\overset{_{\thicksim}}{\beta}_{1},...,\overset{_{\thicksim}}{\beta}_{p})^{t}\ $ is define by:
	\begin{equation}
		\overset{_{\thicksim}}{\beta}_{j}=\hat{\beta}_{_{j}}^{^{MUS}}\mathbb{1}_{\big\{|\hat{\beta}_{_{j}}^{^{MUS}}|>\tau\big\}}\ ,\ \ j=1,\dots,p.
		\label{f89}
	\end{equation}
where the threshold is given either by 
\begin{equation}
	\tau=\frac{2(3\alpha +1)}{3(\alpha-1)}\big(2\lambda+2(2+\delta)\delta a\big)\ , \ for\ \alpha>0,\ a>0.
	\label{f90}
\end{equation}
or by
\begin{equation}
	\tau=\frac{2(3\alpha +1)}{3(\alpha-1)}\big(2\lambda+2(2+\delta)\delta \parallel\hat{\beta}_{_{MUS}}\parallel_{1}\big)\ , \ for\ \alpha>0,\ .
	\label{f91}
\end{equation}
\end{definition}
\begin{theoreme}[ Selection Consistency]
	Let the assumptions of \textbf{Theorem 2.4.1} hold.Let either $\ \Theta\subseteq \big\{\beta \in \mathbb{R}^{p}: \parallel \beta \parallel_{1}\leq a\big\}\ $ for some $\ a>0\ $ and the threshold $\ \tau\ $ is given by \eqref{f90} or $\ \frac{\phi^{2}}{8s}\ $ and the threshold $\tau\ $ is given by \eqref{f91}.If $\ \underset{j\in S}{\min}|\beta_{j}|>\tau\ $, then,
	\begin{equation}
		Sign(\overset{_{\thicksim}}{\beta}_{j})=Sign(\beta_{j})\ ,\ for\ all\ \overset{_{\thicksim}}{\beta}_{j}\in \eqref{f89}\ ,\ j=1,...,p
	\end{equation}
\end{theoreme}
\textbf{Proof.} \textit{Cf. \cite{nref23}, Theorem 5,P.17}\\
In summary, under some assumptions, $ \hat{\beta}_{_{MUS}}\ $ recover $\ \beta\ $ with high accuracy in different norm  as well as under prediction risk ( \textbf{Theorem 2.4.2}); And under somewhat stronger assumptions these estimators recover correctly the sparsity pattern (\textbf{Theorem 2.4.3}).
\section{Numerical Studies}
\subsection{Ridge under measurement error (simulation) }
As discuss earlier, ridge regression \eqref{f12} provide better estimators when facing problem of multicollinearity in our data.The purpose of this simulation is to evaluate the performance of the modified ridge estimation in \eqref{f57} when problem of multicollinearity is present in the measurement error ridden data.To this end, we will restrict particularly to the case where p<n ( low-dimensional data) with high correlations between covariates measured with error.
\paragraph{Simulation design:}
We simulate data from the true model ,
$$y=X\beta +\epsilon \quad ,\ \epsilon \rightsquigarrow \mathcal{N}(0,1)\ ,\ p=100\ and\ n=500$$
where $\ X\ $ has been generated as $\ X\rightsquigarrow \mathcal{N}\big(0,\Sigma_{X} \big)\ $ with $\ \Sigma_{X}=(\rho_{ij})\  (\rho_{ij}=0.9^{|i-j|})$. All coefficients are set to 3, $\beta=(3,3,\dots,3)^{t}$.The observed data were generated as ,
$$W=X+U,\quad where\ U\rightsquigarrow \mathcal{N}\big(0,\Sigma_{U} \big)\ with\ \Sigma_{U}=0.75\mathbb{I}_{p}$$.
The simulated data was divided into a training and a test set.The four methods ;\textit{True OLS\footnote{Ordinary least square} ($\ y\sim X$)}\eqref{f3} , \textit{corrected OLS}\eqref{f53} , \textit{naive ridge} and \textit{modified ridge regression} \eqref{f57} were used to fit a corresponding model to the  training set. The fitted models were used to make predictions to the test set and we computed the MSE and the PE (prediction error on the test set).The procedure was repeated 100 times.
\paragraph{Simulation results:}
We can see in table \ref{fig9} that both the MSE and PE (on average) of the estimates $\hat{\beta}$ provided by the modified  (corrected) ridge are lower than those of the three others.Meaning that the provided $\hat{\beta}$ is much more reliable considering MSE (as mentioned in \textit{theorem 3.1.2}) and PE .We also find out in passing that using the corrected version of OLS \eqref{f53} in this setting ( \textit{"high-correlation with measurement error"}) would result to a pretty poor estimator given the MSE and PE ( table \ref{fig9}). \textbf{R codes are provided in appendix \ref{a3}}

	\begin{table}[h]
	\begin{center}
		\begin{tabular}{|c|c|c|c|c|}
			\hline
			 & \textbf{true OLS }&\textbf{corrected OLS}&\textbf{naive Ridge}  & \textbf{corrected Ridge}\\
			\hline
			MSE&6.85 (0.328) & 477.94 (4700)& 6.84 (0.327) & 6.83 (0.327)\\
	
			PE & 2.01(0.146) & 50058. (499396)  & 0.05 (0.008) & 0.04 (0.006)  \\
			\hline
\end{tabular}
\end{center}
\caption[Simulation results for ridge under measurement error]{\textit{Simulation results for ridge under measurement error. The table reports the PE and the estimation error as $l_{2}\ $ norm (MSE).results are reported as median values and (standard deviation sd.) }}
\label{fig9}
\end{table}
\subsection{Measurement error with sparsity assumption (simulations): NCL, CoCoLasso and MUS implementation.}
\appendix
\chapter{R codes}
\section{R code for numerical experiment of Lasso, Ridge and Elastic net.}
\label{a1}
\begin{lstlisting}
# loading required packages

library(glmnet); library(pROC); library(caret)
# setup
N <- 200 # number of observations
n.sim <- 100 # number of simulations
n.cov <- 1000 # number of covariates

############ four examples of high dimensional data sets ##########
# container matrix  (for misclassifications errors)
lasso.me <- matrix(NA,nrow = n.sim , ncol = 4)
ridge.me <- matrix(NA,nrow = n.sim , ncol = 4)
elnet.me <- matrix(NA,nrow = n.sim , ncol = 4)
#container matrix (for AUC values)
lasso.auc <- matrix(NA,nrow = n.sim , ncol = 4)
ridge.auc <- matrix(NA,nrow = n.sim , ncol = 4)
elnet.auc <- matrix(NA,nrow = n.sim , ncol = 4)
#container matrix for the number of non-zero beta-coeff estimated
lasso.nb <- matrix(NA,nrow = n.sim , ncol = 4)
ridge.nb <- matrix(NA,nrow = n.sim , ncol = 4)
elnet.nb <- matrix(NA,nrow = n.sim , ncol = 4)
# 100 lambda values for ridge and lasso
grid <- 10^seq(2,-2,length=100) 
# set up (alpha,lambda)grid to search for pair that minimizes CV error
alp.grid <-seq(0.05,0.9,length=10) ; lam.grid <- 10^seq(2,-2,length=20)
set.seed(123)
#### we loop over simulations and record the ME and AUC value each time #####

###############################################################################
############################### EXAMPLE 1 #####################################
for(i in 1:n.sim) {
	# variance-covariance matrix fill with correlation
	Sigma <- matrix(NA,n.cov,n.cov) 
	for(j in 1:n.cov){
		for(k in 1:n.cov) Sigma[j,k] <- 0.5^abs(j-k)
	}
	diag(Sigma) <-1  # set diagonal to 1
	# N (200) random draws of 1000 covariates with mean 0 and variance Sigma
	X <- MASS::mvrnorm(N,rep(0,n.cov),Sigma)
	dim(X) # p >> n 
	# beta-coefficients
	beta <- c(runif(122,2,5),rep(0,878))
	# response variable Y simulation
	y <- apply(X,MARGIN = 1,FUN = function(x) rbinom(1,1,1/(1+exp(-t(x)%*%beta))))
	# split into training and test data
	train <- sample(c(1:N),size = 120)
	
	################### Ridge
	# 10-fold cross-validation on ridge to find best of 100 lambda value
	cv.ridge <-cv.glmnet(X[train,],y[train],alpha=0,lambda=grid,
	nfolds = 10,family="binomial")
	ridge.model <- glmnet(X[train,],y[train],alpha=0,
	lambda=cv.ridge$lambda.min,family="binomial")
	# predict outcome using the model with the best lambda
	r.pred.prob <- predict(ridge.model,newx=X[-train,],type = "response" )
	r.pred.classes <- ifelse(r.pred.prob > 0.5, 1,0)
	## Model accuracy :
	#Misclassification error rate (ME)
	obs.classes <- y[-train]
	ridge.me[i,1] <- mean(r.pred.classes != obs.classes)
	#AUC value
	ridge.auc[i,1] <-auc(y[-train],r.pred.prob )
	# number of non-zero beta-coefficients for ridge
	ridge.nb[i,1]<- length(as.matrix(coef(ridge.model))[which(coef(ridge.model)[-1]!=0),1])
	
	######################Lasso
	# 10-fold cross-validation on Lasso to find best of 100 lambda values
	cv.lasso <-cv.glmnet(X[train,],y[train],alpha=1,lambda=grid,
	nfolds = 10,family="binomial")
	lasso.model <- glmnet(X[train,],y[train],alpha=1,
	lambda=cv.lasso$lambda.min,family="binomial")
	# predict outcome using the model with the best lambda
	l.pred.prob <- predict(lasso.model,newx=X[-train,],type = "response")
	l.pred.classes <- ifelse(l.pred.prob > 0.5, 1,0)
	## Model accuracy :
	# Misclassification error rate (ME)
	obs.classes <- y[-train]
	lasso.me[i,1]<- mean(l.pred.classes!= obs.classes)
	#AUC value
	lasso.auc[i,1]<- auc(y[-train],l.pred.prob )
	# number of non-zero  beta-coefficients for Lasso regression
	lasso.nb[i,1]<- length(as.matrix(coef(lasso.model))[which(coef(lasso.model)[-1]!=0),1])
	
	####################### Elastic-Net
	y1<-as.factor(y)
	data<-as.data.frame(cbind(y1,X))
	test.data <- data[-train,]
	train.data<-data[train,]
	# set up cross validation method for train function
	control<-trainControl(method = "cv",number = 10)
	#set up search grid for alpha and lambda parameters
	srchgrid<-expand.grid(alpha=alp.grid,lambda=lam.grid)
	#Training Elastic Net regression:perform CV forecasting y level based on all features
	cv.elnet<-train(y1~.,data=train.data,method="glmnet",trControl=control,
	tuneGrid=srchgrid)
	# Elastic net regression  model 
	op.alp<-cv.elnet$bestTune$alpha
	op.lam<-cv.elnet$bestTune$lambda
	elnet.model<-glmnet(X[train,],y[train],alpha=op.alp,lambda=op.lam,family="binomial")
	# predict outcome using the model
	eln.pred.prob<-predict(elnet.model,newx=X[-train,],type = "response")
	eln.pred.classes<- ifelse(eln.pred.prob > 0.5, 1,0)	
	## Model accuracy :
	# Misclassification error rate (ME)
	elnet.me[i,1]<- mean(eln.pred.classes!=obs.classes)
	# AUC value
	elnet.auc[i,1]<-auc(y[-train],eln.pred.prob)
	# number of non-zero beta coefficients for Elastic-net
	elnet.nb[i,1]<- length(as.matrix(coef(elnet.model))[which(coef(elnet.model)[-1]!=0),1])
}
###############################################################################
################################ EXAMPLE 2 ####################################
for(i in 1:n.sim) {
	# variance-covariance matrix fill with correlation
	Sigma <- matrix(NA,n.cov,n.cov) 
	for(j in 1:n.cov){
		for(k in 1:n.cov) Sigma[j,k] <- 0.5^abs(j-k)
	}
	diag(Sigma) <-1  # set diagonal to 1
	# N (200) random draws of 1000 covariates with mean 0 and variance Sigma
	X <- MASS::mvrnorm(N,rep(0,n.cov),Sigma)
	dim(X) # p >> n 
	# beta-coefficients
	beta <- c(rep(0.8,n.cov))
	# response variable Y simulation
	y <- apply(X,MARGIN = 1,FUN = function(x) rbinom(1,1,1/(1+exp(-t(x)%*%beta))))
	# slit into training and test data
	train <- sample(c(1:N),size = 120)
	
	################### Ridge
	# 10-fold cross-validation on ridge to find best of 100 lambda value
	cv.ridge <-cv.glmnet(X[train,],y[train],alpha=0,lambda=grid,
	nfolds = 10,family="binomial")
	ridge.model <- glmnet(X[train,],y[train],alpha=0,
	lambda=cv.ridge$lambda.min,family="binomial")
	# predict outcome using the model with the best lambda
	r.pred.prob <- predict(ridge.model,newx=X[-train,],type = "response" )
	r.pred.classes <- ifelse(r.pred.prob > 0.5, 1,0)
	## Model accuracy :
	#Misclassification error rate (ME)
	obs.classes <- y[-train]
	ridge.me[i,2] <- mean(r.pred.classes != obs.classes)
	#AUC value
	ridge.auc[i,2] <-auc(y[-train],r.pred.prob )
	# number of non-zero beta-coefficients for ridge
	ridge.nb[i,2]<- length(as.matrix(coef(ridge.model))[which(coef(ridge.model)[-1]!=0),1])
	
	######################Lasso
	# 10-fold cross-validation on Lasso to find best of 100 lambda values
	cv.lasso <-cv.glmnet(X[train,],y[train],alpha=1,lambda=grid,
	nfolds = 10,family="binomial")
	lasso.model <- glmnet(X[train,],y[train],alpha=1,
	lambda=cv.lasso$lambda.min,family="binomial")
	# predict outcome using the model with the best lambda
	l.pred.prob <- predict(lasso.model,newx=X[-train,],type = "response")
	l.pred.classes <- ifelse(l.pred.prob > 0.5, 1,0)
	## Model accuracy :
	# Misclassification error rate (ME)
	obs.classes <- y[-train]
	lasso.me[i,2]<- mean(l.pred.classes!= obs.classes)
	#AUC value
	lasso.auc[i,2]<- auc(y[-train],l.pred.prob )
	#number  of non-zero beta-coefficients for lasso
	lasso.nb[i,2]<- length(as.matrix(coef(lasso.model))[which(coef(lasso.model)[-1]!=0),1])
	####################### Elastic-Net
	y1<-as.factor(y)
	data<-as.data.frame(cbind(y1,X))
	test.data <- data[-train,]
	train.data<-data[train,]
	# set up cross validation method for train function
	control<-trainControl(method = "cv",number = 10)
	#set up search grid for alpha and lambda parameters
	srchgrid<-expand.grid(alpha=alp.grid,lambda=lam.grid)
	#Training Elastic Net regression:perform CV forecasting y level based on all features
	cv.elnet<-train(y1~.,data=train.data,method="glmnet",trControl=control,
	tuneGrid=srchgrid)
	# Elastic net regression  model 
	op.alp<-cv.elnet$bestTune$alpha
	op.lam<-cv.elnet$bestTune$lambda
	elnet.model<-glmnet(X[train,],y[train],alpha=op.alp,lambda=op.lam,family="binomial")
	# predict outcome using the model
	eln.pred.prob<-predict(elnet.model,newx=X[-train,],type = "response")
	eln.pred.classes<- ifelse(eln.pred.prob > 0.5, 1,0)
	## Model accuracy :
	# Misclassification error rate (ME)
	elnet.me[i,2]<- mean(eln.pred.classes!=obs.classes)
	# AUC value
	elnet.auc[i,2]<-auc(y[-train],eln.pred.prob)
	#number of non-zero beta coefficients for Elastic-net
	elnet.nb[i,2]<- length(as.matrix(coef(elnet.model))[which(coef(elnet.model)[-1]!=0),1])	
}
###############################################################################
################################ EXAMPLE 3 ####################################
for(i in 1:n.sim) {
	# variance-covariance matrix fill with correlation
	Sigma <- matrix(NA,n.cov,n.cov) 
	for(j in 1:n.cov){
		for(k in 1:n.cov) Sigma[j,k] <- 0.9^abs(j-k)
	}
	diag(Sigma) <-1  # set diagonal to 1
	# N (200) random draws of 1000 covariates with mean 0 and variance Sigma
	X <- MASS::mvrnorm(N,rep(0,n.cov),Sigma)
	dim(X) # p >> n 
	# beta-coefficients
	beta <-rep(c(rep(2,125),rep(0,125)),4)
	# response variable Y simulation
	y <- apply(X,MARGIN = 1,FUN = function(x) rbinom(1,1,1/(1+exp(-t(x)%*%beta))))
	# slit into training and test data
	train <- sample(c(1:N),size = 120)
	
	################### Ridge
	# 10-fold cross-validation on ridge to find best of 100 lambda value
	cv.ridge <-cv.glmnet(X[train,],y[train],alpha=0,lambda=grid,
	nfolds = 10,family="binomial")
	ridge.model <- glmnet(X[train,],y[train],alpha=0,
	lambda=cv.ridge$lambda.min,family="binomial")
	# predict outcome using the model with the best lambda
	r.pred.prob <- predict(ridge.model,newx=X[-train,],type = "response" )
	r.pred.classes <- ifelse(r.pred.prob > 0.5, 1,0)
	## Model accuracy :
	#Misclassification error rate (ME)
	obs.classes <- y[-train]
	ridge.me[i,3] <- mean(r.pred.classes != obs.classes)
	#AUC value
	ridge.auc[i,3] <-auc(y[-train],r.pred.prob )
	# number of non-zero beta-coefficients for ridge
	ridge.nb[i,3]<- length(as.matrix(coef(ridge.model))[which(coef(ridge.model)[-1]!=0),1])
	######################Lasso
	# 10-fold cross-validation on Lasso to find best of 100 lambda values
	cv.lasso <-cv.glmnet(X[train,],y[train],alpha=1,lambda=grid,
	nfolds = 10,family="binomial")
	lasso.model <- glmnet(X[train,],y[train],alpha=1,
	lambda=cv.lasso$lambda.min,family="binomial")
	# predict outcome using the model with the best lambda
	l.pred.prob <- predict(lasso.model,newx=X[-train,],type = "response")
	l.pred.classes <- ifelse(l.pred.prob > 0.5, 1,0)
	## Model accuracy :
	# Misclassification error rate (ME)
	obs.classes <- y[-train]
	lasso.me[i,3]<- mean(l.pred.classes!= obs.classes)
	#AUC value
	lasso.auc[i,3]<- auc(y[-train],l.pred.prob )
	#number  of non-zero beta-coefficients for lasso
	lasso.nb[i,3]<- length(as.matrix(coef(lasso.model))[which(coef(lasso.model)[-1]!=0),1])
	
	####################### Elastic-Net
	y1<-as.factor(y)
	data<-as.data.frame(cbind(y1,X))
	test.data <- data[-train,]
	train.data<-data[train,]
	# set up cross validation method for train function
	control<-trainControl(method = "cv",number = 10)
	#set up search grid for alpha and lambda parameters
	srchgrid<-expand.grid(alpha=alp.grid,lambda=lam.grid)
	#Training Elastic Net regression:perform CV forecasting y level based on all features
	cv.elnet<-train(y1~.,data=train.data,method="glmnet",trControl=control,
	tuneGrid=srchgrid)
	# Elastic net regression  model 
	op.alp<-cv.elnet$bestTune$alpha
	op.lam<-cv.elnet$bestTune$lambda
	elnet.model<-glmnet(X[train,],y[train],alpha=op.alp,lambda=op.lam,family="binomial")
	# predict outcome using the model
	eln.pred.prob<-predict(elnet.model,newx=X[-train,],type = "response")
	eln.pred.classes<- ifelse(eln.pred.prob > 0.5, 1,0)
	## Model accuracy :
	# Misclassification error rate (ME)
	elnet.me[i,3]<- mean(eln.pred.classes!=obs.classes)
	# AUC value
	elnet.auc[i,3]<-auc(y[-train],eln.pred.prob)
	#number of non-zero beta coefficients for Elastic-net
	elnet.nb[i,3]<- length(as.matrix(coef(elnet.model))[which(coef(elnet.model)[-1]!=0),1])
}
###############################################################################
################################ EXAMPLE 4 ####################################
for(i in 1:n.sim) {
	# variance-covariance matrix fill with correlation
	Sigma <- matrix(0,n.cov,n.cov) 
	for(j in 1:n.cov/2){
		for(k in 1:n.cov/2) Sigma[j,k] <- 0.5^abs(j-k)
	}
	diag(Sigma) <-1  # set diagonal to 1
	# N (200) random draws of 1000 covariates with mean 0 and variance Sigma
	X <- MASS::mvrnorm(N,rep(0,n.cov),Sigma)
	dim(X) # p >> n 
	# beta-coefficients
	beta <-c(rep(3,500),rep(0,500))
	# response variable Y simulation
	y <- apply(X,MARGIN = 1,FUN = function(x) rbinom(1,1,1/(1+exp(-t(x)%*%beta))))
	# slit into training and test data
	train <- sample(c(1:N),size = 120)
	################### Ridge
	
	# 10-fold cross-validation on ridge to find best of 100 lambda value
	cv.ridge <-cv.glmnet(X[train,],y[train],alpha=0,lambda=grid,
	nfolds = 10,family="binomial")
	ridge.model <- glmnet(X[train,],y[train],alpha=0,
	lambda=cv.ridge$lambda.min,family="binomial")
	# predict outcome using the model with the best lambda
	r.pred.prob <- predict(ridge.model,newx=X[-train,],type = "response" )
	r.pred.classes <- ifelse(r.pred.prob > 0.5, 1,0)
	## Model accuracy :
	#Misclassification error rate (ME)
	obs.classes <- y[-train]
	ridge.me[i,4] <- mean(r.pred.classes != obs.classes)
	#AUC value
	ridge.auc[i,4] <-auc(y[-train],r.pred.prob )
	# number of non-zero beta-coefficients for ridge
	ridge.nb[i,4]<- length(as.matrix(coef(ridge.model))[which(coef(ridge.model)[-1]!=0),1])
	
	######################Lasso
	# 10-fold cross-validation on Lasso to find best of 100 lambda values
	cv.lasso <-cv.glmnet(X[train,],y[train],alpha=1,lambda=grid,
	nfolds = 10,family="binomial")
	lasso.model <- glmnet(X[train,],y[train],alpha=1,
	lambda=cv.lasso$lambda.min,family="binomial")
	# predict outcome using the model with the best lambda
	l.pred.prob <- predict(lasso.model,newx=X[-train,],type = "response")
	l.pred.classes <- ifelse(l.pred.prob > 0.5, 1,0)
	## Model accuracy :
	# Misclassification error rate (ME)
	obs.classes <- y[-train]
	lasso.me[i,4]<- mean(l.pred.classes!= obs.classes)
	#AUC value
	lasso.auc[i,4]<- auc(y[-train],l.pred.prob )
	#number  of non-zero beta-coefficients for lasso
	lasso.nb[i,4]<- length(as.matrix(coef(lasso.model))[which(coef(lasso.model)[-1]!=0),1])
	
	####################### Elastic-Net
	y1<-as.factor(y)
	data<-as.data.frame(cbind(y1,X))
	test.data <- data[-train,]
	train.data<-data[train,]
	# set up cross validation method for train function
	control<-trainControl(method = "cv",number = 10)
	#set up search grid for alpha and lambda parameters
	srchgrid<-expand.grid(alpha=alp.grid,lambda=lam.grid)
	#Training Elastic Net regression:perform CV forecasting y level based on all features
	cv.elnet<-train(y1~.,data=train.data,method="glmnet",trControl=control,
	tuneGrid=srchgrid)
	# Elastic net regression  model 
	op.alp<-cv.elnet$bestTune$alpha
	op.lam<-cv.elnet$bestTune$lambda
	elnet.model<-glmnet(X[train,],y[train],alpha=op.alp,lambda=op.lam,family="binomial")
	# predict outcome using the model
	eln.pred.prob<-predict(elnet.model,newx=X[-train,],type = "response")
	eln.pred.classes<- ifelse(eln.pred.prob > 0.5, 1,0)
	## Model accuracy :
	# Misclassification error rate (ME)
	elnet.me[i,4]<- mean(eln.pred.classes!=obs.classes)
	# AUC value
	elnet.auc[i,4]<-auc(y[-train],eln.pred.prob)
	#number of non-zero beta coefficients for Elastic-net
	elnet.nb[i,4]<- length(as.matrix(coef(elnet.model))[which(coef(elnet.model)[-1]!=0),1])
}

################################################################################
######## result Matrix of ME and AUC values for each simulations ##########

## we take column mean to get the Average ME over n.sim simulations and
#create an outcome object where the rows contain ridge,lasso,el-net
#and full logistic results respectively
Ave.ME.results<-rbind(apply(ridge.me ,2,mean),apply(lasso.me ,2,mean),
apply(elnet.me ,2,mean), apply(full.model.me ,2,mean)  )
rownames(Ave.ME.results)<-c("Ridge","Lasso","Elastic Net"," full logistic model")
colnames(Ave.ME.results)<-c("ME.ave_Exp1","ME.ave_Exp2","ME.ave_Exp3","ME.ave_Exp4")
### Now proceed the same way and store Standard deviation ME over n.sim
Sd.ME.results<-rbind(apply(ridge.me ,2,sd),apply(lasso.me ,2,sd),
apply(elnet.me ,2,sd), apply(full.model.me ,2,sd))
rownames(Sd.ME.results)<-c("Ridge","Lasso","Elastic Net"," full logistic model")
colnames(Sd.ME.results)<-c("ME.sd_Exp1","ME.sd_Exp2","ME.sd_Exp3","ME.sd_Exp4")
### outcome object containing results Average AUC over n.sim simulations
Ave.AUC.results<-rbind(apply(ridge.auc ,2,mean),apply(lasso.auc ,2,mean),
apply(elnet.auc ,2,mean), apply(full.model.auc ,2,mean)  )
rownames(Ave.AUC.results)<-c("Ridge","Lasso","Elastic Net"," full logistic model")
colnames(Ave.AUC.results)<-c("AUC.ave_Exp1","AUC.ave_Exp2","AUC.ave_Exp3","AUC.ave_Exp4")
### outcome object containing results standard deviation AUC over n.sim simulations
Sd.AUC.results<-rbind(apply(ridge.auc ,2,sd),apply(lasso.auc ,2,sd),
apply(elnet.auc ,2,sd), apply(full.model.auc ,2,sd))
rownames(Sd.AUC.results)<-c("Ridge","Lasso","Elastic Net"," full logistic model")
colnames(Sd.AUC.results)<-c("AUC.sd_Exp1","AUC.sd_Exp2","AUC.sd_Exp3","AUC.sd_Exp4")
### Outcome object containing results Average of non-zero beta coefficients estimated 
# over n.sim simulations
Ave.nbc.results<-rbind(apply(ridge.nb ,2,mean),apply(lasso.nb ,2,mean),
apply(elnet.nb,2,mean), apply(full.model.nb,2,mean) )
rownames(Ave.nbc.results)<-c("Ridge","Lasso","Elastic Net"," full logistic model")
colnames(Ave.nbc.results)<-c("nbc.ave_Exp1","nbc.ave_Exp2","nbc.ave_Exp3","nbc.ave_Exp4")

############################# RESUME ###########################
Simulation.Results<-list(Ave.ME=Ave.ME.results,Sd.ME=Sd.ME.results,
Ave.AUC=Ave.AUC.results,Sd.AUC= Sd.AUC.results,Ave.Num.coef=Ave.nbc.results)
Simulation.Results
\end{lstlisting}

\section{R codes for real data example}
\label{a2}
\begin{lstlisting}
	########################## REAL DATA EXAMPLE ##########################
	#Loading required library
	library(minfi);library(here);library(readr);library(SummarizedExperiment)
	library(caret);library(glmnet)
	################## let's read in the data
	DNA.methylation.data<-readRDS(here("datasets/methylation.rds"))
	DNA.methylation.data # we see that this object has "dim()=5000*37" p>>n
	# Extract the matrix of methylation M-values
	methyl.matrix<-assay(DNA.methylation.data)
	# transpose to have features as column and samples as rows
	methyl.matrix<-t(assay(DNA.methylation.data))
	# view dimension of methylation matrix
	dim(methyl.matrix)
	# examine the metadata,phenotypes and grouping relating to this data
	head(pData(DNA.methylation.data)) # for the first 6 samples
	######### We will focus on the association between age and methylation ########
	Age<-DNA.methylation.data$Age
	#### let us check out what happens if we try fit a linear model to the data ###
	# R will run a multivariate regression model in which each of the column in 
	# methyl.matrix is used as predictor.
	linear_model_fit <- lm(Age~methyl.matrix)
	summary(linear_model_fit)
	# singularities
	XtX <- t(methyl.matrix)%*%methyl.matrix
	det(XtX)  # we can't fit standard linear model to this high-dimensional data.
	###### Now we'll work with set of features known to be associated with Age from
	#                        a paper by "Horvath et al."                         #
	# read in the data
	coefhorvath<-readRDS(here("datasets/coefHorvath.rds"))
	dim(coefhorvath); class(coefhorvath)
	features<-coefhorvath[1:20,]$CpGmarker
	horv.matrix<-methyl.matrix[,features]
	dim(horv.matrix)   # not technically high-dimensional data 
	# Generate an index to split the data into train and test set
	set.seed(50)
	train<-sample(nrow(methyl.matrix),27)
	train.horv.matrix<-horv.matrix[train,] ; train.Age<-Age[train]
	test.horv.matrix<-horv.matrix[-train,] ; test.Age<-Age[-train]
	#############################################################################
	########### OLS regression Vs Ridge regression on "horv.matrix" data ########
	## investigate correlations
	corr.matrix<-cor(train.horv.matrix)
	write.csv2(corr.matrix,file="corr.matrix.horv.csv")
	# using heat map
	heatmap(train.horv.matrix,scale = "column")
	## multilinear regression fit
	horv.lm.fit<-lm(train.Age~.,data = as.data.frame(train.horv.matrix))
	summary(horv.lm.fit)
	# Check mean squared error on the model.
	horv.lm.mse<-mean(residuals(horv.lm.fit)^2)
	horv.lm.mse
	# examine the MSE on the test Data
	pred.lm<-predict(horv.lm.fit,newdata = as.data.frame(test.horv.matrix))
	MSE.lm<-mean((test.Age-pred.lm)^2)
	MSE.lm
	## Ridge regression fit
	# 100 lambda values for ridge and lasso
	grid <- 10^seq(2,-2,length=100) 
	# performing Leave One Out CV to search for the best lambda
	cv.ridge<- cv.glmnet(x=train.horv.matrix,y=train.Age,nfolds =27,alpha=0,lambda =grid  )
	ridge.fit<-glmnet(x=train.horv.matrix,y=train.Age,lambda = cv.ridge$lambda.min,alpha=0)
	# plot of test MSE's vs lambda values
	#plot showing how estimated coefficients change as we increase the penalty, "lambda"
	ridg.fit<-glmnet(x=train.horv.matrix,y=train.Age,alpha=0)
	dev.new() ;plot(cv.ridge);dev.new() ;plot(ridg.fit,xvar="lambda")
	abline(v=log(cv.ridge$lambda.min),lty="dashed");abline(h=0,lty="dashed")
	# examine MSE on test data
	pred.ridge<-predict(ridge.fit,newx=test.horv.matrix)
	MSE.ridge<-mean((test.Age-pred.ridge)^2);MSE.ridge
	####### Which performs better, Ridge or OLS ?
	min(c(MSE.ridge,MSE.lm))
	# plot predicted Ages for both method against the true Ages
	lim<-range(c(pred.lm,test.Age,pred.ridge));dev.new();par(mfrow=1:2)
	plot(test.Age,pred.lm,xlim=lim,ylim=lim,pch=19) ;abline(0:1,lty="dashed")
	plot(test.Age,pred.ridge,xlim=lim,ylim=lim,pch=19) ;abline(0:1,lty="dashed")
	## display coefficients estimated
	horv.coefs<-cbind(coef(horv.lm.fit),coef(ridge.fit))
	colnames(horv.coefs)<-c("lm_coefs","ridge_coefs");horv.coefs
	###########################################################################
	###########   LASSO regression on DNA.methylation.data ####################
	
	## examine correlations using the pearson heatmap
	heatmap(methyl.matrix,scale = "column",col=cm.colors(256))
	# perform 10-folds CV to find the best lambda value
	cv.lasso<-cv.glmnet(methyl.matrix,Age,alpha=1,lambda = grid ,nfolds=10)
	lasso.fit<-glmnet(methyl.matrix,Age,alpha=1,lambda = cv.lasso$lambda.min)
	# plot of test MSE's vs lambda values
	#plot showing how estimated coefficients change as we increase the penalty, "lambda"
	lass.fit<-glmnet(methyl.matrix,Age,alpha=1);dev.new() ;plot(cv.lasso)
	dev.new() ;plot(lass.fit,xvar="lambda");abline(v=log(cv.lasso$lambda.min),lty="dashed")
	# view coefficients of the model
	lasso_coefficients <- coef(lasso.fit);lasso_coefficients
	# view selected variables performed by Lasso regression
	selected_coefs <- as.matrix(lasso_coefficients)[which(lasso_coefficients !=0),1]
	selected_features<-names(selected_coefs);selected_features;length(selected_features)
	## compare features selected with Horvath signature
	intersect(selected_features,coefhorvath$CpGmarker) # we selected some of the same feature
	length(intersect(selected_features,coefhorvath$CpGmarker))
	## Lasso Vs Ridge coefficients paths
	dev.new() ;par(mfrow=c(2,1));plot(ridg.fit,xvar="lambda",main="ridge case")
	plot(lass.fit,xvar="lambda",main="lasso case")
	#############################################################################
	################ Ridge regression on DNA.methylation.data ###################
	
	# perform 10-folds CV to find the best lambda value
	cv.r<-cv.glmnet(methyl.matrix,Age,alpha=0,lambda = grid ,nfolds=10)
	Ridge.fit<-glmnet(methyl.matrix,Age,alpha=0,lambda = cv.r$lambda.min)
	# plot of test MSE's vs lambda values
	#plot showing how estimated coefficients change as we increase the penalty, "lambda"
	Ridg.fit<-glmnet(methyl.matrix,Age,alpha=0);dev.new() ;plot(cv.r);dev.new() 
	plot(Ridg.fit,xvar="lambda");abline(v=log(cv.r$lambda.min),lty="dashed")
	# view coefficients of the model
	ridge_coefficients <- coef(Ridge.fit);ridge_coefficients
	##############################################################################
	######### Blending Ridge regression and the LASSO : Elastic nets #############
	
	# set up (alpha,lambda)grid to search for pair that minimizes CV error 
	# using "caret package
	alp.grid <-seq(0.05,0.9,length=10) ; lam.grid <- 10^seq(2,-2,length=20)
	data<-as.data.frame(cbind(Age=Age,methyl.matrix))
	# set up cross validation method for train function
	control<-trainControl(method = "cv",number = 10)
	#set up search grid for alpha and lambda parameters
	srchgrid<-expand.grid(alpha=alp.grid,lambda=lam.grid)
	#Training Elastic Net regression:perform CV forecasting y level based on all features
	cv.elnet<-train(Age~.,data=data,method="glmnet",trControl=control,tuneGrid=srchgrid)
	cv.elnet
	# plot CV performance
	dev.new();plot(cv.elnet)
	# Elastic net regression  model 
	op.alp<-cv.elnet$bestTune$alpha; op.lam<-cv.elnet$bestTune$lambda
	elnet.model<-glmnet(methyl.matrix,Age,alpha=op.alp,lambda=op.lam)
	### Lasso Vs Elastic coefficients paths (setting "alpha=op.alp") 
	eln.model<-glmnet(methyl.matrix,Age,alpha=op.alp);dev.new()
	par(mfrow=c(2,1));plot(eln.model,main="Elastic Net case"); plot(lass.fit,main="lasso case")
	### compare the coefficients with the LASSO model
	elnet_coefs<-coef(elnet.model)
	sum(elnet_coefs[,1]==0)     #number of coefficients set to zero for "elnet"
	sum(lasso_coefficients[,1]==0) # number of coefficients set to zero for LASSO
	# plot Lasso coefficients against Elastic Net coefficients
	dev.new()
	plot(lasso_coefficients[,1],elnet_coefs[,1],pch=19,xlab="Lasso coefficients",
	ylab="Elastic net coefficients"); abline(0:1,lty="dashed",col="blue")
	# compare features remaining in the model with Horvath signature
	elnet.rm.features<-names(as.matrix(elnet_coefs)[which(elnet_coefs!=0),1])
	elnet.rm.features; length(elnet.rm.features)
	intersect(elnet.rm.features,coefhorvath$CpGmarker)
	length(intersect(elnet.rm.features,coefhorvath$CpGmarker))
	# export the estimated coeffitients provide by the tree method
	result_coefs<-cbind(lasso_coefficients,ridge_coefficients,elnet_coefs)
	colnames(result_coefs)<-c("lasso_coefs","ridge_coefs","eln_coefs")
	result_coefs
	write.csv2(result_coefs,file = "coefs_DNA.methyl.data.csv")
\end{lstlisting}
\section{R code for ridge regression over measurement error ridden data}
\label{a3}
\begin{lstlisting}
	# loading require package.
	library(glmnet)
	n=500 ; p=200 #number of samples and number of covariates
	n.sim=100 # number of Monte Carlo simulation
	grid <- 10^seq(2,-2,length=100) # 100 lambda values for Ridge
	### container matrix ###
	# container matrix for mean square error(MSE) and prediction error for 
	# "true linear model (lm), naive linear model, corrected lm, true ridge
	#, naive ridge and corrected ridge regression.
	lm.res= matrix(NA,nrow = n.sim , ncol = 3)
	ridge.res=matrix(NA,nrow = n.sim , ncol = 3)
	colnames(lm.res)<-c("t.lm.mse","n.lm.mse","cor.lm.mse")
	colnames(ridge.res)<-c("t.ridge.mse","n.ridge.mse","cor.ridge.mse")
	lm.res2= matrix(NA,nrow = n.sim , ncol = 3)
	ridge.res2=matrix(NA,nrow = n.sim , ncol = 3)
	colnames(lm.res2)<-c("t.lm.pe","n.lm.pe","cor.lm.pe")
	colnames(ridge.res2)<-c("t.ridge.pe","n.ridge.pe","cor.ridge.pe")
	######### Loop over simulation and record MSE and PE value each time #########
	for(i in 1:n.sim){
		###### model setup
		# variance-covariance matrix of true unobserved values with high correlation
		Sigma_X <- matrix(NA,p,p) 
		for(j in 1:p){
			for(k in 1:p) Sigma_X[j,k] <- 0.9^abs(j-k)}
		X <- MASS::mvrnorm(n,rep(0,p),Sigma_X) #true variables
		Sigma_U<-diag(x=0.75 ,nrow = p, ncol = p) #measurement error covariance matrix (assume here to be known)
		U<- MASS::mvrnorm(n,rep(0,p),Sigma_U)
		W <- X + U  #measurement matrix ( observed values)
		beta<-runif(p,1,4) #coefficient
		y <- X %*% beta + rnorm(n, sd = 1);y<-scale(y);X<-scale(X);W<-scale(W) #Response 
		train <- sample(c(1:n),size = 400) # split into training and test data
		###### fit true Linear model on training data
		t.lm.fit<-lm(y[train]~X[train,])
		hat.beta<-coef(t.lm.fit)[-1] # estimated coefficient
		lm.res[i,1]<-mean((hat.beta-beta)^2) # MSE 
		pred.t.lm<-predict(t.lm.fit,newdata = as.data.frame(X[-train,])) #prediction error
		lm.res2[i,1]<-mean((y[-train]-pred.t.lm)^2)
		###### fit naive linear model
		n.lm.fit<-lm(y[train]~W[train,])
		hat.beta.n<-coef(n.lm.fit)[-1] # estimated coefficient
		lm.res[i,2]<-mean((hat.beta.n-beta)^2) # MSE 
		pred.n.lm<-predict(n.lm.fit,newdata = as.data.frame(W[-train,])) # prediction error
		lm.res2[i,2]<-mean((y[-train]-pred.n.lm)^2)
		###### correct for measurement error in the model
		n1<-n-100 #  reliability matrix "K" estimate
		hat.K<-solve(t(W[train,])%*%W[train,])%*%(t(W[train,])%*%W[train,]-n1*Sigma_U)
		# estimate coefficient under measurement error
		hat.beta.me<-solve(hat.K)%*%hat.beta.n
		lm.res[i,3]<-mean((hat.beta.me-beta)^2) ## MSE
		lm.res2[i,3]<-mean((y[-train]-W[-train,]%*%hat.beta.me)^2) ## prediction error 
		####### fit true Ridge regression model
		
		# 10-folds cross validation to find the optimal "lambda"
		cv.t.ridge <-cv.glmnet(X[train,],y[train],alpha=0,lambda=grid,nfolds = 10)
		t.ridge.fit<- glmnet(X[train,],y[train],alpha=0,lambda=cv.t.ridge$lambda.min)
		hat.beta_R<-coef(t.ridge.fit)[-1]
		ridge.res[i,1]<-mean((hat.beta_R-beta)^2) ## MSE
		pred.t.ridge<-pred.ridge<-predict(t.ridge.fit,newx=X[-train,]) # prediction error on test data
		ridge.res2[i,1]<-mean((y[-train]-pred.t.ridge)^2)
		###### fit naive Ridge
		# 10-folds cross validation to find the optimal "lambda"
		cv.n.ridge <-cv.glmnet(W[train,],y[train],alpha=0,lambda=grid,nfolds = 10)
		n.ridge.fit<- glmnet(W[train,],y[train],alpha=0,lambda=cv.n.ridge$lambda.min)
		hat.beta_nR<-coef(n.ridge.fit)[-1]
		ridge.res[i,2]<-mean((hat.beta_nR-beta)^2) ## MSE
		pred.n.ridge<-predict(n.ridge.fit,newx=W[-train,])# prediction error on test data
		ridge.res2[i,2]<-mean((y[-train]-pred.n.ridge)^2)
		##### correct for measurement error in ridge regression
		# use estimated reliability matrix "hat.K" 
		# perform regular ridge regression of "y" on "W%*%hat.K "
		# 10-folds cross validation to find the optimal "lambda"
		cv.cor.ridge <-cv.glmnet(W[train,]%*%hat.K,y[train],alpha=0,lambda=grid,nfolds = 10)
		cor.ridge.fit<- glmnet(W[train,]%*%hat.K,y[train],alpha=0,lambda=cv.cor.ridge$lambda.min)
		hat.beta_corR<-coef(cor.ridge.fit)[-1]
		ridge.res[i,3]<-mean((hat.beta_corR-beta)^2)  ## MSE
		ridge.res2[i,3]<-mean((y[-train]-W[-train,]%*%hat.beta_corR)^2)  ## prediction error
	}
	######################## Result Matrix of MSE #############################
	lm.res ; ridge.res ;lm.res2 ; ridge.res2
	lm.MSE.res<-rbind(apply(lm.res ,2,mean),apply(lm.res ,2,sd) )
	R.MSE.res<-rbind(apply(ridge.res ,2,mean),apply(ridge.res ,2,sd) )
	lm.PE.res<-rbind(apply(lm.res2 ,2,mean),apply(lm.res2 ,2,sd) )
	R.PE.res<-rbind(apply(ridge.res2 ,2,mean),apply(ridge.res2 ,2,sd) )
	#outcome object containing result average and standard deviation for each method
	simulation.res1<-cbind(lm.MSE.res,R.MSE.res);simulation.res2<-cbind(lm.PE.res,R.PE.res)
	rownames(simulation.res1)<-c("Ave","Sd");rownames(simulation.res2)<-c("Ave","Sd")
	simulation.res1 ; simulation.res2 #display
\end{lstlisting}
\bibliography{biblio_data_base}
\bibliographystyle{abbrv}

\end{document}